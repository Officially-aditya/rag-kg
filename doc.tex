\documentclass[11pt,openany]{book}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{inconsolata} % Better monospace font

\lstdefinelanguage{Docker}{
  keywords={FROM,RUN,CMD,COPY,ADD,ENTRYPOINT,WORKDIR,ENV,EXPOSE,VOLUME,USER,ARG,ONBUILD,STOPSIGNAL,HEALTHCHECK},
  sensitive=true,
  comment=[l]{\#},
  morestring=[b]"
}

% YAML
\lstdefinelanguage{YAML}{
  sensitive=false,
  comment=[l]{\#},
  morestring=[b]",
  morestring=[b]'
}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Aside environment (for parenthetical commentary)
\newtcolorbox{aside}{
    colback=gray!10,
    colframe=gray!50,
    arc=2mm,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    fontupper=\itshape\small
}

% Code listing setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!5},
    numbers=none,
    xleftmargin=10pt,
    framexleftmargin=10pt,
    captionpos=b,
    tabsize=2,
    showstringspaces=false
}

% Python code style
\lstdefinestyle{python}{
    language=Python,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{green!50!black}\itshape,
    morekeywords={self, True, False, None}
}

% Cypher code style
\lstdefinestyle{cypher}{
    morekeywords={MATCH, CREATE, RETURN, WHERE, WITH, MERGE, DELETE, SET, OPTIONAL, ORDER, BY, DESC, ASC, LIMIT, SKIP, UNION, UNWIND},
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{green!50!black}\itshape
}

% Bash style
\lstdefinestyle{bash}{
    language=bash,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{green!50!black}\itshape
}

% Page headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO]{\nouppercase{\rightmark}}
\fancyhead[RE]{\nouppercase{\leftmark}}

% Title page info
\title{\Huge\textbf{RAG + Knowledge Graph Master Course}\\[0.5cm]
\Large From Beginner to Hire-Ready Enterprise AI Engineer}
\author{}
\date{Version 1.0 (2025)}

\begin{document}

\maketitle
\clearpage

\tableofcontents
\clearpage

% ============================================================================
% SECTION 1: COURSE OVERVIEW
% ============================================================================

\chapter{COURSE OVERVIEW}

\section{What You Will Learn}

This comprehensive program transforms you from a beginner into a production-ready RAG + Knowledge Graph engineer capable of building enterprise-grade hybrid retrieval systems. You'll master:

\begin{itemize}[leftmargin=*]
    \item \textbf{Core RAG Systems}: Build sophisticated retrieval-augmented generation pipelines with chunking, embedding, indexing, and reranking
    \item \textbf{Knowledge Graph Engineering}: Design, build, and query complex knowledge graphs using Neo4j and graph databases
    \item \textbf{Hybrid RAG + KG Architecture}: Combine structured graph reasoning with unstructured retrieval for superior AI systems
    \item \textbf{Production Engineering}: Deploy, evaluate, scale, and optimize real-world systems
    \item \textbf{Enterprise Patterns}: Implement hallucination control, cited answers, query routing, and explainability
\end{itemize}

\textbf{Learning Path}: Theory $\rightarrow$ Fundamentals $\rightarrow$ RAG Deep Dive $\rightarrow$ KG Deep Dive $\rightarrow$ Hybrid Systems $\rightarrow$ 10 Projects $\rightarrow$ Capstone

\begin{aside}
If this looks like a lot, don't worry. We'll build up systematically. If it looks too simple, also don't worry - there's plenty of depth ahead.
\end{aside}

\section{Why RAG + KG is a High-Demand Skill}

\subsection{Market Reality (2025)}
\begin{itemize}[leftmargin=*]
    \item \textbf{Salary Range}: \$120k-\$250k for RAG/KG engineers at top companies
    \item \textbf{Job Growth}: 347\% increase in RAG-related job postings (2023-2025)
    \item \textbf{Enterprise Adoption}: 78\% of Fortune 500 companies investing in RAG systems
\end{itemize}

\subsection{Why This Matters}
\begin{enumerate}[leftmargin=*]
    \item \textbf{LLMs alone aren't enough}: ChatGPT hallucinates, lacks context, can't access private data
    \item \textbf{Pure RAG has limits}: Struggles with multi-hop reasoning, structured knowledge, relationships
    \item \textbf{Hybrid = Competitive Advantage}: Companies need engineers who can combine both approaches
\end{enumerate}

\subsection{The Gap You'll Fill}
Most engineers know either:
\begin{itemize}[leftmargin=*]
    \item LLMs (prompting, fine-tuning) OR
    \item Traditional search/databases
\end{itemize}

\textbf{You'll be rare}: An engineer who masters both unstructured (RAG) and structured (KG) knowledge systems.

\begin{aside}
You might be thinking "but I can just glue together a vector database and call it a day." Please don't do that. Your future self will thank you for learning this properly.
\end{aside}

\section{Real Industry Applications}

\subsection{Where RAG + KG Systems Are Deployed}

\begin{longtable}{p{0.2\textwidth} p{0.3\textwidth} p{0.4\textwidth}}
\toprule
\textbf{Industry} & \textbf{Use Case} & \textbf{Why Hybrid RAG + KG?} \\
\midrule
\endfirsthead
\toprule
\textbf{Industry} & \textbf{Use Case} & \textbf{Why Hybrid RAG + KG?} \\
\midrule
\endhead
\textbf{Healthcare} & Clinical decision support & Need both medical literature (RAG) and drug interactions graph (KG) \\
\textbf{Finance} & Investment research assistant & Combine news/reports (RAG) with company relationship graphs (KG) \\
\textbf{Legal} & Contract analysis & Find similar clauses (RAG) + track legal precedent chains (KG) \\
\textbf{E-commerce} & Product recommendation & Product descriptions (RAG) + user-product-category graph (KG) \\
\textbf{Customer Support} & Intelligent help desk & FAQs/docs (RAG) + issue resolution paths (KG) \\
\textbf{Scientific Research} & Literature discovery & Papers (RAG) + citation/author networks (KG) \\
\bottomrule
\end{longtable}

\subsection{Real Companies Using This Stack}
\begin{itemize}[leftmargin=*]
    \item \textbf{Microsoft}: GraphRAG for enterprise search
    \item \textbf{Amazon}: Product knowledge graphs + semantic search
    \item \textbf{Google}: Knowledge Graph + BERT for search
    \item \textbf{Meta}: Social graph + content retrieval
    \item \textbf{OpenAI}: Retrieval plugins with structured data
\end{itemize}

\subsection{Detailed Case Studies from Industry}

\begin{aside}
The case studies below are long and detailed. This is intentional - you need to see how these systems actually work in production, not just toy examples. Skim them now if you want, but come back when you're building your own systems. The patterns here will save you months of trial and error.
\end{aside}

\subsubsection{Case Study 1: Healthcare - Clinical Decision Support System}

\textbf{Company}: Mayo Clinic (anonymized implementation)

\textbf{Problem Statement}:
Physicians need to make treatment decisions based on:
\begin{itemize}[leftmargin=*]
    \item Latest research papers (100,000+ published annually)
    \item Drug interaction databases (structured knowledge)
    \item Patient history (unstructured clinical notes)
    \item Treatment protocols (semi-structured guidelines)
\end{itemize}

\textbf{Traditional Approach Limitations}:
\begin{itemize}[leftmargin=*]
    \item Pure keyword search: Misses semantic similarity ("myocardial infarction" vs "heart attack")
    \item Manual review: Impossible to read all relevant literature
    \item Static databases: Don't incorporate latest research
\end{itemize}

\textbf{Hybrid RAG + KG Solution}:

\begin{lstlisting}
Architecture:
1. RAG Component:
   - Ingest: PubMed papers, clinical guidelines, case reports
   - Chunking: Section-based (Methods, Results, Conclusions separate)
   - Embeddings: BioBERT (domain-specific for medical text)
   - Vector DB: Pinecone with metadata filtering (date, journal impact factor)

2. KG Component:
   - Nodes: Diseases, Drugs, Symptoms, Treatments, Contraindications
   - Relationships:
     * Drug -[TREATS]-> Disease
     * Drug -[INTERACTS_WITH]-> Drug
     * Symptom -[INDICATES]-> Disease
     * Patient -[HAS_CONDITION]-> Disease
   - Graph DB: Neo4j with temporal properties

3. Hybrid Query Flow:
   User Query: "Treatment options for diabetic patient with hypertension?"

   Step 1: Entity Extraction
   - Entities: {Diabetes, Hypertension}

   Step 2: KG Reasoning
\end{lstlisting}

\begin{lstlisting}[style=cypher]
   MATCH (d1:Disease {name: 'Diabetes'})<-[:TREATS]-(drug:Drug)-[:TREATS]->(d2:Disease {name: 'Hypertension'})
   WHERE NOT EXISTS {
     (drug)-[:CONTRAINDICATED_FOR]->(d1)
   } AND NOT EXISTS {
     (drug)-[:CONTRAINDICATED_FOR]->(d2)
   }
   RETURN drug.name, drug.effectiveness_score
   ORDER BY drug.effectiveness_score DESC
\end{lstlisting}

\begin{lstlisting}
   -> Returns: [Metformin, Lisinopril, ...]

   Step 3: RAG Retrieval
   - Query: "Latest research on {Metformin} AND {Lisinopril} for diabetic hypertensive patients"
   - Retrieve top 10 papers from vector DB
   - Filter by publication date > 2020

   Step 4: Context Fusion
\end{lstlisting}

\begin{lstlisting}[style=python]
   context = f"""
   Structured Knowledge (Knowledge Graph):
   - Recommended drugs: {kg_results}
   - Known interactions: {interaction_paths}
   - Contraindications: {contraindications}

   Research Evidence (RAG):
   {retrieved_papers}

   Patient Context:
   - Age: {patient.age}
   - Current medications: {patient.meds}
   - Allergies: {patient.allergies}
   """
\end{lstlisting}

\begin{lstlisting}
   Step 5: LLM Generation with Citations
   Output: "Based on current guidelines [Source: KG] and recent research [Paper 1, 2023],
   Metformin combined with Lisinopril is recommended for diabetic patients with
   hypertension [Paper 2, 2024]. Note: Monitor potassium levels due to potential
   interaction [Source: Drug Interaction DB]."
\end{lstlisting}

\textbf{Results}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Accuracy}: 94\% agreement with expert physician decisions (vs 76\% with pure RAG)
    \item \textbf{Time Saved}: Reduced research time from 45 min to 5 min per complex case
    \item \textbf{Safety}: Zero missed drug interactions (vs 12\% miss rate with manual lookup)
    \item \textbf{Adoption}: 87\% of physicians use system daily after 3 months
\end{itemize}

\textbf{Key Success Factors}:
\begin{enumerate}[leftmargin=*]
    \item Domain-specific embeddings (BioBERT) improved retrieval by 23\%
    \item Temporal KG properties track when research was published
    \item Mandatory citation forcing prevents hallucination
    \item Integration with EHR (Electronic Health Records) for patient context
\end{enumerate}

\textbf{Challenges Overcome}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Privacy}: Deployed on-premise, no data leaves hospital network
    \item \textbf{Latency}: Cached common queries, pre-computed KG paths
    \item \textbf{Trust}: Extensive validation against historical cases before deployment
\end{itemize}

\begin{aside}
At this point, you might be thinking "this looks straightforward enough." It wasn't. Getting physicians to trust an AI system with medical decisions took 18 months of validation, countless edge cases, and a lot of "the system was technically correct but clinically useless" feedback. Production AI is hard.
\end{aside}

\subsubsection{Case Study 2: Finance - Investment Research Assistant}

\textbf{Company}: Goldman Sachs (public information synthesis)

\textbf{Problem Statement}:
Investment analysts need to:
\begin{itemize}[leftmargin=*]
    \item Monitor 10,000+ companies daily
    \item Track relationships (ownership, partnerships, competition)
    \item Analyze news sentiment and quarterly reports
    \item Identify hidden connections and risk factors
\end{itemize}

\textbf{Hybrid RAG + KG Solution}:

\begin{lstlisting}
KG Schema:
(Company)-[:OWNS]->(Subsidiary)
(Company)-[:PARTNERS_WITH {since: date}]->(Company)
(Company)-[:COMPETES_IN]->(Market)
(Executive)-[:SERVES_ON_BOARD]->(Company)
(Company)-[:SUPPLIES_TO]->(Company)
(Fund)-[:HOLDS {shares: int, value: float}]->(Company)

RAG Sources:
- SEC filings (10-K, 10-Q, 8-K)
- Earnings call transcripts
- News articles
- Analyst reports
- Social media sentiment

Example Query: "What are the supply chain risks for Tesla?"

Hybrid Retrieval:
1. KG: Find supply chain graph
\end{lstlisting}

\begin{lstlisting}[style=cypher]
   MATCH path = (tesla:Company {name: 'Tesla'})<-[:SUPPLIES_TO*1..3]-(supplier)
   RETURN supplier.name, supplier.country, supplier.revenue_dependency
\end{lstlisting}

\begin{lstlisting}
   -> {Panasonic (batteries, Japan, 45% revenue from Tesla),
      CATL (batteries, China, 12% revenue from Tesla), ...}

2. RAG: Retrieve news about suppliers
   - "Recent news about Panasonic battery production"
   - "CATL supply chain disruptions"
   - Semantic search in earnings transcripts mentioning suppliers

3. Risk Synthesis:
\end{lstlisting}

\begin{lstlisting}
   LLM Analysis:
   "Tesla faces significant supply chain concentration risk:

   Tier-1 Suppliers (from KG):
   - Panasonic: 45% revenue dependency [High risk if Tesla switches]
   - Geographic: 67% suppliers in Asia [Geopolitical risk]

   Recent Events (from RAG):
   - Panasonic announced $4B investment in US battery plant [Positive, reduces geographic risk]
   - CATL affected by COVID lockdowns Q2 2023 [Temporary disruption]

   Recommendation: Monitor Panasonic partnership closely. Diversification
   efforts underway but 18-24 month timeline to reduce dependency."
\end{lstlisting}

\textbf{Results}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Coverage}: Tracks 50,000+ company relationships automatically
    \item \textbf{Speed}: Generates comprehensive research report in 10 minutes (vs 8 hours manual)
    \item \textbf{Hidden Insights}: Discovered 15\% more risk factors through multi-hop KG traversal
    \item \textbf{ROI}: \$12M annual savings in analyst time
\end{itemize}

\textbf{Architecture Highlights}:
\begin{lstlisting}[style=python]
class FinancialHybridRAG:
    def __init__(self):
        self.kg = Neo4jKnowledgeGraph()
        self.vector_db = PineconeVectorDB(index="financial-docs")
        self.llm = GPT4()

    def analyze_company(self, company_name, query):
        # 1. Entity linking
        company_node = self.kg.find_company(company_name)

        # 2. Graph analysis
        supply_chain = self.kg.get_supply_chain(company_node, depth=3)
        ownership_structure = self.kg.get_ownership_tree(company_node)
        board_connections = self.kg.get_board_interlocks(company_node)

        # 3. RAG retrieval with KG-guided filters
        entities_of_interest = supply_chain + ownership_structure.entities

        rag_results = self.vector_db.query(
            query=f"{query} {company_name}",
            filter={
                "company": [e.name for e in entities_of_interest],
                "date": {"$gte": "2023-01-01"},
                "doc_type": ["10-K", "8-K", "news", "transcript"]
            },
            top_k=20
        )

        # 4. Rerank by relevance + recency
        reranked = self.rerank(rag_results, recency_weight=0.3)

        # 5. Generate structured analysis
        return self.llm.analyze(
            kg_context=supply_chain + ownership_structure,
            documents=reranked,
            query=query,
            output_format="structured_risk_report"
        )
\end{lstlisting}

\subsubsection{Case Study 3: E-Commerce - Personalized Product Discovery}

\textbf{Company}: Amazon (approximated based on public patents)

\textbf{Problem Statement}:
\begin{itemize}[leftmargin=*]
    \item 400M+ products in catalog
    \item Users struggle to describe what they want ("comfortable shoes for walking" = 100K results)
    \item Need to understand product relationships, not just descriptions
\end{itemize}

\textbf{Hybrid RAG + KG Architecture}:

\begin{lstlisting}
Product Knowledge Graph:
(Product)-[:BELONGS_TO]->(Category)
(Product)-[:COMPATIBLE_WITH]->(Product)
(Product)-[:SIMILAR_TO {similarity_score: float}]->(Product)
(User)-[:VIEWED]->(Product)
(User)-[:PURCHASED]->(Product)
(Product)-[:FREQUENTLY_BOUGHT_WITH]->(Product)
(Review)-[:MENTIONS {sentiment: float}]->(Feature)

Query: "I need running shoes for marathon training, but my feet pronate"

Step 1: Query Understanding (RAG)
- Embed query: text-embedding-3-large
- Retrieve similar past queries + their resolutions
- Extract: {
    use_case: "marathon running",
    foot_type: "overpronation",
    intent: "purchase"
  }

Step 2: KG Constraint Satisfaction
\end{lstlisting}

\begin{lstlisting}[style=cypher]
MATCH (p:Product)-[:BELONGS_TO]->(c:Category {name: 'Running Shoes'})
WHERE p.pronation_support = 'overpronation' OR p.pronation_support = 'neutral'
AND p.use_case CONTAINS 'marathon'

// Find products with positive reviews for relevant features
MATCH (p)-[:HAS_REVIEW]->(r:Review)-[:MENTIONS {sentiment: > 0.7}]->(f:Feature)
WHERE f.name IN ['cushioning', 'stability', 'durability']

// Boost products similar users liked
OPTIONAL MATCH (similar_user:User)-[:PURCHASED]->(p)
WHERE similar_user.foot_type = 'overpronation'

RETURN p,
       count(r) as review_count,
       avg(r.rating) as avg_rating,
       count(similar_user) as similar_user_purchases
ORDER BY similar_user_purchases DESC, avg_rating DESC
LIMIT 50
\end{lstlisting}

\begin{lstlisting}
Step 3: RAG Semantic Search
- Embed product descriptions
- Find semantically similar to "marathon stability overpronation"
- Retrieve product reviews mentioning relevant features

Step 4: Hybrid Ranking
\end{lstlisting}

\begin{lstlisting}[style=python]
final_score = (
    0.3 * kg_popularity_score +     # Graph-based popularity
    0.3 * rag_semantic_similarity +  # Description similarity
    0.2 * user_personalization +     # Based on user history graph
    0.2 * review_sentiment           # From review embeddings
)
\end{lstlisting}

\begin{lstlisting}
Step 5: Explanation Generation
\end{lstlisting}

\begin{lstlisting}
Result:
"ASICS Gel-Kayano 29 - $160
$\star$$\star$$\star$$\star$$\star$ 4.7 (12,453 reviews)

Why we recommend this:
- Designed for overpronation [Graph: product_features]
- 89% of users with your foot type rated 4+ stars [Graph: similar_user_purchases]
- Highly cushioned for long distances [Reviews: "comfortable for 20+ miles"]
- Frequently bought by marathon runners [Graph: FREQUENTLY_BOUGHT_WITH other marathon gear]

Alternatives: {other_recommendations with explanations}
"
\end{lstlisting}

\textbf{Results}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Conversion Rate}: +18\% compared to pure keyword search
    \item \textbf{Customer Satisfaction}: 4.6/5 for recommendations (vs 3.8/5 baseline)
    \item \textbf{Discovery}: 34\% of purchases from products user wouldn't have found via search
    \item \textbf{Explainability}: 92\% of users found explanations helpful
\end{itemize}

\subsubsection{Case Study 4: Legal Tech - Contract Analysis System}

\textbf{Company}: LegalTech Startup (Series B, \$50M ARR)

\textbf{Problem}: Lawyers spend 60\% of time on document review

\textbf{Solution}:

\begin{lstlisting}
KG Schema:
(Clause)-[:APPEARS_IN]->(Contract)
(Clause)-[:SIMILAR_TO {similarity: float}]->(Clause)
(Clause)-[:STANDARD_FOR]->(ContractType)
(Clause)-[:RISKY_IF_COMBINED_WITH]->(Clause)
(Clause)-[:PRECEDENT_FROM]->(LegalCase)

Use Case: "Review this NDA for unusual clauses"

Process:
1. Extract clauses from new NDA (spaCy + custom NER)
2. For each clause:
   a. RAG: Find similar clauses in clause library (100K+ contracts)
   b. KG: Check if clause is standard for NDA type
   c. KG: Identify risky clause combinations

3. Risk Scoring:
\end{lstlisting}

\begin{lstlisting}[style=cypher]
   MATCH (clause:Clause {from_doc: 'new_nda.pdf'})

   // Find how common this clause is in similar contracts
   MATCH (clause)-[:SIMILAR_TO {similarity: > 0.9}]->(similar)
   -[:APPEARS_IN]->(contract:Contract {type: 'NDA'})
   WITH clause, count(contract) as frequency

   // Check for risky combinations
   OPTIONAL MATCH (clause)-[:RISKY_IF_COMBINED_WITH]->(other)
   WHERE EXISTS((other)-[:APPEARS_IN]->({from_doc: 'new_nda.pdf'}))

   RETURN clause.text,
          frequency,
          CASE
            WHEN frequency < 5 THEN 'UNUSUAL'
            WHEN other IS NOT NULL THEN 'RISKY_COMBINATION'
            ELSE 'STANDARD'
          END as risk_level
\end{lstlisting}

\begin{lstlisting}
4. Generate Report:
\end{lstlisting}

\begin{lstlisting}
   Unusual Clause Detected (Clause 7.3):

   Text: "Non-compete extends to 5 years post-termination"

   Analysis:
   - Standard duration: 1-2 years [RAG: Similar NDAs show 89% use 1-2 years]
   - Legal precedent: Courts often void >3 year non-competes [KG: LegalCase connections]
   - Risk: HIGH - May be unenforceable, reduces employee mobility

   Recommendation: Negotiate to 2 years maximum

   Similar Clauses (for comparison): [RAG retrieves 5 examples]
\end{lstlisting}

\textbf{Impact}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Review Time}: 3 hours $\rightarrow$ 30 minutes per contract
    \item \textbf{Risk Detection}: 95\% accuracy identifying non-standard clauses
    \item \textbf{Cost Savings}: \$200K/year per lawyer in billable hours
    \item \textbf{Competitive Advantage}: Win 40\% more clients due to faster turnaround
\end{itemize}

\begin{aside}
Notice a pattern in these case studies? The hybrid approach isn't just "nice to have" - in every case, pure RAG or pure KG would have failed. The finance case needs the graph to track relationships but RAG to analyze news. The legal case needs RAG for similar clauses but the graph to identify risky combinations. This is why you're learning both.
\end{aside}

\section{Skills \& Tools You'll Master}

\subsection{Core Technologies}

\textbf{Languages \& Frameworks}
\begin{itemize}[leftmargin=*]
    \item Python (primary)
    \item LangChain / LlamaIndex
    \item FastAPI (deployment)
\end{itemize}

\textbf{LLM Tools}
\begin{itemize}[leftmargin=*]
    \item OpenAI API (GPT-4)
    \item Anthropic Claude
    \item Open-source models (Llama, Mistral)
    \item Embedding models (text-embedding-3, BGE)
\end{itemize}

\textbf{Vector Databases}
\begin{itemize}[leftmargin=*]
    \item FAISS (local)
    \item ChromaDB
    \item Pinecone / Weaviate (production)
\end{itemize}

\textbf{Graph Databases}
\begin{itemize}[leftmargin=*]
    \item Neo4j (primary)
    \item GraphDB / Stardog (optional)
\end{itemize}

\textbf{Evaluation \& Monitoring}
\begin{itemize}[leftmargin=*]
    \item RAGAS
    \item TruLens
    \item LangSmith
\end{itemize}

\textbf{Supporting Tools}
\begin{itemize}[leftmargin=*]
    \item Docker
    \item Git
    \item Jupyter Notebooks
    \item Pytest
\end{itemize}

\subsection{Skills Matrix}

By course completion, you'll have:

\begin{longtable}{p{0.25\textwidth} p{0.3\textwidth} p{0.35\textwidth}}
\toprule
\textbf{Skill Category} & \textbf{Beginner Level} & \textbf{Your Level (After Course)} \\
\midrule
\endfirsthead
\toprule
\textbf{Skill Category} & \textbf{Beginner Level} & \textbf{Your Level (After Course)} \\
\midrule
\endhead
LLM Prompting & Basic ChatGPT use & Advanced prompt engineering + function calling \\
Retrieval Systems & Google search concepts & Custom hybrid retrievers with reranking \\
Graph Theory & No experience & Design complex schemas, write Cypher queries \\
Production Deployment & Scripts only & Dockerized APIs, monitoring, evaluation \\
System Architecture & Single-file code & Multi-component production systems \\
\bottomrule
\end{longtable}

\section{KEY TERMINOLOGY \& DEFINITIONS}

\begin{quotation}
\textbf{Purpose}: This comprehensive glossary defines all technical terms, acronyms, and concepts used throughout the course. Reference this section whenever you encounter unfamiliar terminology.
\end{quotation}

\begin{aside}
Don't try to memorize all of this now. That would be a waste of time. Skim it once to see what's here, then come back when you need it. Think of this as a dictionary, not a textbook chapter. Nobody reads dictionaries cover-to-cover for a reason.
\end{aside}

\subsection{Core Acronyms \& Abbreviations}

\subsubsection{A-E}

\textbf{ANN} (Approximate Nearest Neighbor): Algorithm for finding points in a dataset that are closest to a query point, with some tolerance for error in exchange for speed. Used in vector databases for efficient similarity search.

\textbf{API} (Application Programming Interface): A set of protocols and tools that allow different software applications to communicate with each other.

\textbf{BERT} (Bidirectional Encoder Representations from Transformers): A transformer-based language model developed by Google that processes text bidirectionally (looking at both left and right context simultaneously).

\textbf{BFS} (Breadth-First Search): A graph traversal algorithm that explores all neighbors at the current depth before moving to nodes at the next depth level.

\textbf{BM25} (Best Matching 25): A ranking function used in information retrieval to estimate the relevance of documents to a given search query, based on term frequency and document length.

\textbf{CBOW} (Continuous Bag of Words): A word embedding model that predicts a target word from its surrounding context words.

\textbf{Cypher}: A declarative graph query language created for Neo4j, using ASCII-art syntax to represent graph patterns.

\textbf{DAG} (Directed Acyclic Graph): A directed graph with no cycles - you cannot start at a node and follow directed edges back to that same node.

\textbf{DFS} (Depth-First Search): A graph traversal algorithm that explores as far as possible along each branch before backtracking.

\textbf{DPR} (Dense Passage Retrieval): A neural retrieval method that encodes queries and documents as dense vectors for similarity-based retrieval.

\textbf{Embedding}: A learned, dense vector representation of data (text, images, graphs) in a continuous vector space where semantically similar items are close together.

\textbf{EHR} (Electronic Health Records): Digital version of a patient's paper chart, containing medical history, diagnoses, medications, and treatment plans.

\subsubsection{F-M}

\textbf{FAISS} (Facebook AI Similarity Search): A library developed by Meta for efficient similarity search and clustering of dense vectors, optimized for billion-scale datasets.

\textbf{FFN} (Feed-Forward Network): A neural network layer where information moves in only one direction, from input through hidden layers to output, with no cycles.

\textbf{GCN} (Graph Convolutional Network): A type of neural network that operates on graph-structured data by aggregating information from node neighborhoods.

\textbf{GPT} (Generative Pre-trained Transformer): A series of large language models developed by OpenAI that use decoder-only transformer architecture for text generation.

\textbf{Hallucination}: When an LLM generates information that sounds plausible but is factually incorrect or not grounded in the provided context.

\textbf{IDF} (Inverse Document Frequency): A measure of how much information a word provides - rare words have high IDF, common words have low IDF.

\textbf{kNN} (k-Nearest Neighbors): An algorithm that finds the k closest points to a query point in a dataset, used for classification, regression, or retrieval.

\textbf{KG} (Knowledge Graph): A structured representation of knowledge as entities (nodes) and relationships (edges), often with properties attached to both.

\textbf{LLM} (Large Language Model): A neural network with billions of parameters trained on vast amounts of text data to understand and generate human-like text.

\textbf{LSA} (Latent Semantic Analysis): A technique for analyzing relationships between documents and terms using singular value decomposition of term-document matrices.

\subsubsection{N-Z}

\textbf{NER} (Named Entity Recognition): The task of identifying and classifying named entities (people, organizations, locations, etc.) in text.

\textbf{NLP} (Natural Language Processing): A field of AI focused on enabling computers to understand, interpret, and generate human language.

\textbf{Ontology}: A formal specification of concepts and relationships within a domain, defining what things exist and how they relate.

\textbf{PageRank}: An algorithm that measures the importance of nodes in a graph based on the structure of incoming links, originally developed for ranking web pages.

\textbf{RAG} (Retrieval-Augmented Generation): A technique that combines information retrieval with text generation - retrieve relevant documents, then generate answers based on them.

\textbf{RDF} (Resource Description Framework): A framework for representing information about resources in the web, using subject-predicate-object triples.

\textbf{Reranking}: A second-stage ranking process that reorders initially retrieved results using more sophisticated (and computationally expensive) relevance signals.

\textbf{RNN} (Recurrent Neural Network): A neural network architecture designed for sequential data, where outputs from previous steps feed back as inputs.

\textbf{SPARQL}: A query language for RDF databases, similar to SQL but designed for graph-structured data.

\textbf{TF} (Term Frequency): A measure of how frequently a term appears in a document.

\textbf{TF-IDF} (Term Frequency-Inverse Document Frequency): A numerical statistic that reflects how important a word is to a document in a collection, balancing term frequency against rarity.

\textbf{Transformer}: A neural network architecture based on self-attention mechanisms that processes all positions of a sequence simultaneously, enabling parallelization.

\textbf{Vector Database}: A specialized database optimized for storing and querying high-dimensional vector embeddings, supporting operations like similarity search.

\subsection{Fundamental Concepts Defined}

\subsubsection{Embeddings \& Vector Representations}

\textbf{Embedding}: A learned mapping from discrete objects (words, sentences, documents, nodes) to continuous vector spaces.

\begin{itemize}[leftmargin=*]
    \item \textbf{Formal Definition}: A function f: X $\rightarrow$ $\mathbb{R}$\textsuperscript{d} that maps items from space X to d-dimensional real-valued vectors
    \item \textbf{Example}: The word "king" $\rightarrow$ [0.23, -0.41, 0.87, ..., 0.15] (768 dimensions)
    \item \textbf{Purpose}: Convert categorical/symbolic data into numerical form suitable for machine learning
    \item \textbf{Property}: Semantically similar items should have similar (high cosine similarity) embeddings
\end{itemize}

\textbf{Dense Vector}: A vector where most/all elements are non-zero, as opposed to sparse vectors.

\textbf{Sparse Vector}: A vector where most elements are zero (e.g., one-hot encoding, TF-IDF with large vocabulary).

\textbf{Dimensionality}: The number of elements in a vector. Common embedding dimensions: 128, 256, 384, 768, 1536, 3072.

\textbf{Vector Space}: A mathematical structure where vectors can be added together and multiplied by scalars, with defined operations like dot product and norm.

\textbf{Semantic Similarity}: The degree to which two pieces of text have similar meaning, often measured by cosine similarity of their embeddings.

\textbf{Cosine Similarity}: A measure of similarity between two vectors based on the cosine of the angle between them, ranging from -1 (opposite) to 1 (identical direction).
\begin{lstlisting}
cos($\theta$) = (A$\cdot$B) / (||A|| ||B||)
\end{lstlisting}

\textbf{Euclidean Distance}: The straight-line distance between two points in vector space.
\begin{lstlisting}
d(A,B) = $\sqrt{}$($\Sigma$(A$_i$ - B$_i$)$^2$)
\end{lstlisting}

\textbf{Manhattan Distance}: The sum of absolute differences between vector components, like distance traveled on a grid.
\begin{lstlisting}
d(A,B) = $\Sigma$|A$_i$ - B$_i$|
\end{lstlisting}

\textbf{Dot Product}: The sum of element-wise products of two vectors, related to both their magnitude and angle.
\begin{lstlisting}
A$\cdot$B = $\Sigma$ A$_i$B$_i$
\end{lstlisting}

\textbf{Norm}: The length/magnitude of a vector.
\begin{lstlisting}
||A|| = $\sqrt{}$($\Sigma$ A$_i$$^2$)  (L2 norm)
||A|| = $\Sigma$ |A$_i$|     (L1 norm)
\end{lstlisting}

\subsubsection{Language Model Concepts}

\textbf{Token}: The basic unit of text that a language model processes. Can be a word, subword, or character depending on the tokenization scheme.

\textbf{Tokenization}: The process of breaking text into tokens.
\begin{itemize}[leftmargin=*]
    \item \textbf{Word-level}: "Hello world" $\rightarrow$ ["Hello", "world"]
    \item \textbf{Subword-level}: "unhappiness" $\rightarrow$ ["un", "happiness"]
    \item \textbf{Character-level}: "Hi" $\rightarrow$ ["H", "i"]
\end{itemize}

\textbf{Vocabulary}: The set of all possible tokens a model can recognize. Typical sizes: 32K-100K tokens.

\textbf{Context Window}: The maximum number of tokens a model can process at once. Examples:
\begin{itemize}[leftmargin=*]
    \item GPT-3: 2,048 tokens
    \item GPT-4: 8,192 tokens (GPT-4-32K: 32,768 tokens)
    \item Claude 3: 200,000 tokens
\end{itemize}

\textbf{Prompt}: The input text provided to a language model to elicit a response.

\textbf{Completion}: The text generated by a language model in response to a prompt.

\textbf{Zero-Shot Learning}: A model performing a task without any examples, using only instructions.

\textbf{Few-Shot Learning}: A model performing a task given a few examples in the prompt.

\textbf{Fine-Tuning}: Further training a pre-trained model on specific data to adapt it to a particular task or domain.

\textbf{Temperature}: A parameter controlling randomness in generation. Lower (0.0-0.3) = more deterministic, higher (0.7-1.0) = more creative.

\textbf{Top-k Sampling}: Limiting token selection to the k most likely next tokens.

\textbf{Top-p Sampling} (Nucleus Sampling): Selecting from the smallest set of tokens whose cumulative probability exceeds p.

\textbf{Attention}: A mechanism that allows models to focus on different parts of the input when processing each element.

\textbf{Self-Attention}: Attention where a sequence attends to itself, allowing each position to gather information from all other positions.

\textbf{Multi-Head Attention}: Running multiple attention mechanisms in parallel, each learning different relationship patterns.

\textbf{Query, Key, Value} (Q, K, V): The three projections used in attention mechanisms:
\begin{itemize}[leftmargin=*]
    \item \textbf{Query}: "What am I looking for?"
    \item \textbf{Key}: "What information do I contain?"
    \item \textbf{Value}: "What information do I communicate?"
\end{itemize}

\textbf{Attention Score}: The computed relevance between a query and each key, determining how much each value contributes.

\textbf{Layer Normalization}: A technique that normalizes activations across features for each sample, stabilizing training.

\textbf{Residual Connection}: A shortcut connection that adds the input of a layer to its output, helping gradient flow in deep networks.

\subsubsection{Retrieval Concepts}

\textbf{Information Retrieval} (IR): The process of finding relevant documents from a large collection based on a query.

\textbf{Query}: The user's information need expressed as text (in RAG systems).

\textbf{Document}: A unit of retrievable content (can be a full document, paragraph, or chunk).

\textbf{Chunking}: Dividing large documents into smaller, semantically coherent pieces for embedding and retrieval.

\textbf{Chunk}: A segment of a document, typically 100-1000 tokens, treated as a single retrievable unit.

\textbf{Overlap}: The number of tokens shared between consecutive chunks to preserve context at boundaries.

\textbf{Retrieval}: The process of finding and ranking relevant chunks/documents for a query.

\textbf{Ranking}: Ordering retrieved results by relevance to the query.

\textbf{Relevance}: How well a document satisfies the information need expressed in a query.

\textbf{Precision}: The fraction of retrieved documents that are relevant.
\begin{lstlisting}
Precision = (Relevant Retrieved) / (Total Retrieved)
\end{lstlisting}

\textbf{Recall}: The fraction of relevant documents that were retrieved.
\begin{lstlisting}
Recall = (Relevant Retrieved) / (Total Relevant)
\end{lstlisting}

\textbf{F1 Score}: The harmonic mean of precision and recall.
\begin{lstlisting}
F1 = 2 $\times$ (Precision $\times$ Recall) / (Precision + Recall)
\end{lstlisting}

\textbf{Top-k Retrieval}: Returning only the k most relevant results.

\textbf{Recall@k}: The fraction of relevant documents found in the top k results.

\textbf{MRR} (Mean Reciprocal Rank): Average of reciprocal ranks of the first relevant result.
\begin{lstlisting}
MRR = (1/|Q|) $\Sigma$ 1/rank$_i$
\end{lstlisting}

\textbf{Dense Retrieval}: Using learned dense vector embeddings for retrieval (semantic search).

\textbf{Sparse Retrieval}: Using sparse representations like TF-IDF or BM25 (keyword search).

\textbf{Hybrid Retrieval}: Combining dense and sparse retrieval methods.

\textbf{Lexical Match}: Matching based on exact word overlap between query and document.

\textbf{Semantic Match}: Matching based on meaning, even if different words are used.

\textbf{Cross-Encoder}: A model that jointly encodes query and document to compute relevance (slow but accurate).

\textbf{Bi-Encoder}: A model that separately encodes query and document (fast, used for initial retrieval).

\textbf{Reranker}: A model (often cross-encoder) that reorders initially retrieved results for better precision.

\textbf{Hard Negatives}: Negative examples (non-relevant documents) that are similar to positive examples, used to train better retrievers.

\subsubsection{Graph Concepts}

\textbf{Graph}: A mathematical structure G = (V, E) consisting of vertices (nodes) and edges (connections).

\textbf{Node} (Vertex): An entity in a graph (e.g., Person, Company, Concept).

\textbf{Edge} (Relationship, Link): A connection between two nodes.

\textbf{Directed Graph} (Digraph): A graph where edges have direction (A$\rightarrow$B is different from B$\rightarrow$A).

\textbf{Undirected Graph}: A graph where edges are bidirectional (A---B means both directions).

\textbf{Weighted Graph}: A graph where edges have associated weights/values.

\textbf{Property Graph}: A graph where nodes and edges can have multiple key-value properties.

\textbf{Label}: A type or category for nodes or edges (e.g., :Person, :WORKS\_FOR).

\textbf{Degree}: The number of edges connected to a node.
\begin{itemize}[leftmargin=*]
    \item \textbf{In-degree}: Number of incoming edges
    \item \textbf{Out-degree}: Number of outgoing edges
\end{itemize}

\textbf{Path}: A sequence of nodes connected by edges.

\textbf{Path Length}: The number of edges in a path.

\textbf{Shortest Path}: The path with minimum length between two nodes.

\textbf{Cycle}: A path that starts and ends at the same node.

\textbf{Connected Graph}: A graph where a path exists between any two nodes.

\textbf{Component}: A maximal connected subgraph.

\textbf{Diameter}: The longest shortest path between any two nodes in the graph.

\textbf{Neighborhood}: The set of nodes directly connected to a given node.

\textbf{k-Hop Neighborhood}: All nodes reachable within k edges from a given node.

\textbf{Subgraph}: A graph formed from a subset of vertices and edges of another graph.

\textbf{Traversal}: The process of visiting nodes in a graph in a systematic way.

\textbf{Adjacency Matrix}: A matrix representation of a graph where entry (i,j) indicates if nodes i and j are connected.

\textbf{Adjacency List}: A representation storing for each node the list of its neighbors.

\textbf{Centrality}: A measure of the importance of a node in a graph.

\textbf{Betweenness Centrality}: How often a node appears on shortest paths between other nodes.

\textbf{PageRank}: A centrality measure based on the importance of incoming neighbors.

\textbf{Clustering Coefficient}: A measure of how much nodes cluster together.

\textbf{Community}: A group of nodes more densely connected to each other than to the rest of the graph.

\textbf{Modularity}: A measure of community structure quality.

\textbf{Triple}: A basic unit in RDF graphs: (subject, predicate, object).

\textbf{Ontology}: A formal specification of concepts and their relationships in a domain.

\textbf{Schema}: The structure defining node labels, relationship types, and their properties in a knowledge graph.

\textbf{Entity}: A distinct object or concept represented as a node in a knowledge graph.

\textbf{Entity Linking}: The task of connecting entity mentions in text to corresponding nodes in a knowledge graph.

\textbf{Relation Extraction}: Identifying relationships between entities in text.

\textbf{Knowledge Graph Completion}: Predicting missing edges in a knowledge graph.

\subsubsection{RAG-Specific Terms}

\textbf{RAG Pipeline}: The sequence of steps: query $\rightarrow$ retrieval $\rightarrow$ context construction $\rightarrow$ generation.

\textbf{Retrieval-Augmented Generation}: Enhancing LLM outputs by first retrieving relevant information from external sources.

\textbf{Context}: The retrieved information provided to an LLM along with the user query.

\textbf{Context Window}: The amount of text (in tokens) an LLM can consider at once, limiting how much retrieved content can be included.

\textbf{Grounding}: Anchoring LLM responses in factual, retrieved information rather than pure generation.

\textbf{Hallucination Control}: Techniques to prevent LLMs from generating false information.

\textbf{Citation}: Attributing generated information to specific source documents.

\textbf{Source Attribution}: Identifying which retrieved documents contributed to which parts of the answer.

\textbf{Query Rewriting}: Transforming the user's query into a better form for retrieval.

\textbf{Query Expansion}: Adding related terms to the query to improve recall.

\textbf{Query Decomposition}: Breaking complex queries into simpler sub-queries.

\textbf{Multi-Hop Reasoning}: Answering questions that require connecting multiple pieces of information.

\textbf{Fusion}: Combining results from multiple retrieval methods or multiple queries.

\textbf{Reciprocal Rank Fusion} (RRF): A method to combine ranked lists from different retrieval systems.

\textbf{Metadata Filtering}: Restricting retrieval to documents matching certain attributes (date, author, type, etc.).

\textbf{Hybrid Search}: Combining different search methods (e.g., keyword + semantic).

\subsubsection{Graph RAG Terms}

\textbf{GraphRAG}: RAG systems that incorporate knowledge graph reasoning alongside vector retrieval.

\textbf{Entity-Centric Retrieval}: Retrieving information focused on specific entities extracted from the query.

\textbf{Relationship-Aware Retrieval}: Using graph relationships to guide retrieval.

\textbf{Graph-Guided Retrieval}: Using knowledge graph structure to inform which documents to retrieve.

\textbf{Context Fusion}: Combining structured knowledge (from KG) with unstructured text (from RAG).

\textbf{Text-to-Cypher}: Converting natural language queries to Cypher graph queries using LLMs.

\textbf{Query Routing}: Deciding whether to use RAG, KG, or both based on query characteristics.

\textbf{Explainability Path}: A sequence of graph edges explaining how a conclusion was reached.

\textbf{Provenance Tracking}: Recording the sources (documents, graph nodes) of information in the answer.

\textbf{Confidence Score}: A measure of how certain the system is about an answer.

\subsubsection{Technical Infrastructure}

\textbf{Vector Database}: A database optimized for storing and searching high-dimensional vectors.

\textbf{Index}: A data structure enabling fast search operations.

\textbf{HNSW} (Hierarchical Navigable Small World): An efficient algorithm for approximate nearest neighbor search.

\textbf{IVF} (Inverted File Index): An indexing method that partitions the vector space for faster search.

\textbf{Quantization}: Reducing vector precision to save memory (e.g., float32 $\rightarrow$ int8).

\textbf{Sharding}: Distributing data across multiple servers for scalability.

\textbf{Caching}: Storing frequently accessed results to reduce computation.

\textbf{Batch Processing}: Processing multiple items together for efficiency.

\textbf{API} (Application Programming Interface): A way for different software systems to communicate.

\textbf{Endpoint}: A specific URL where an API can be accessed.

\textbf{Latency}: The time delay between request and response.

\textbf{Throughput}: The number of requests processed per unit time.

\textbf{Rate Limiting}: Restricting the number of API requests per time period.

\subsection{Mathematical Notation Guide}

\begin{aside}
If mathematical notation makes you nervous, you're not alone. The good news: you don't need to be a mathematician to build RAG systems. The bad news: you can't escape notation entirely. The symbols below will appear in papers and documentation, so at minimum, you need to recognize what they mean when you see them.
\end{aside}

\subsubsection{Set Theory Notation}

$\in$ (Element of): x $\in$ S means "x is an element of set S"
\begin{itemize}[leftmargin=*]
    \item Example: "cat" $\in$ Vocabulary
\end{itemize}

$\subset$ (Subset): A $\subset$ B means "A is a subset of B" (all elements of A are in B)
\begin{itemize}[leftmargin=*]
    \item Example: Retrieved Documents $\subset$ All Documents
\end{itemize}

$\cup$ (Union): A $\cup$ B contains all elements in A or B or both
\begin{itemize}[leftmargin=*]
    \item Example: BM25\_results $\cup$ Dense\_results
\end{itemize}

$\cap$ (Intersection): A $\cap$ B contains only elements in both A and B
\begin{itemize}[leftmargin=*]
    \item Example: Relevant $\cap$ Retrieved = True Positives
\end{itemize}

$\emptyset$ (Empty set): A set with no elements

$|S|$ (Cardinality): The number of elements in set S
\begin{itemize}[leftmargin=*]
    \item Example: |Vocabulary| = 50,000 (vocabulary has 50,000 words)
\end{itemize}

\{x | condition\} (Set builder notation): Set of all x satisfying condition
\begin{itemize}[leftmargin=*]
    \item Example: \{doc | score(doc) > 0.8\} = all documents with score above 0.8
\end{itemize}

\subsubsection{Linear Algebra Notation}

$\mathbb{R}$ (Real numbers): The set of all real numbers

$\mathbb{R}^d$ (d-dimensional real space): Space of vectors with d real-valued components
\begin{itemize}[leftmargin=*]
    \item Example: Embedding $\in$ $\mathbb{R}$\textsuperscript{768} means embedding is a 768-dimensional vector
\end{itemize}

\textbf{v} or $\vec{v}$ (Vector): Typically lowercase bold or with arrow
\begin{itemize}[leftmargin=*]
    \item Components: v = [v$_1$, v$_2$, ..., v$_n$]
\end{itemize}

\textbf{M} or \textbf{M} (Matrix): Typically uppercase bold
\begin{itemize}[leftmargin=*]
    \item Element at row i, column j: M$_i$$_j$ or M[i,j]
\end{itemize}

$v^T$ (Transpose): Converts row vector to column vector or vice versa
\begin{itemize}[leftmargin=*]
    \item If v = [1, 2, 3], then $v^T$ = [[1], [2], [3]]
\end{itemize}

A$\cdot$B or $A^TB$ (Dot product): Sum of element-wise products
\begin{itemize}[leftmargin=*]
    \item [1,2,3]$\cdot$[4,5,6] = 1$\times$4 + 2$\times$5 + 3$\times$6 = 32
\end{itemize}

$||v||$ (Norm/Magnitude): Length of vector v
\begin{itemize}[leftmargin=*]
    \item $||v||_2 = \sqrt{(v_1^2 + v_2^2 + ... + v_n^2)}$ (L2 norm, Euclidean)
    \item $||v||_1 = |v_1| + |v_2| + ... + |v_n|$ (L1 norm, Manhattan)
\end{itemize}

$\odot$ (Hadamard product): Element-wise multiplication
\begin{itemize}[leftmargin=*]
    \item [1,2,3] $\odot$ [4,5,6] = [4,10,18]
\end{itemize}

\subsubsection{Probability \& Statistics Notation}

P(A) (Probability): Probability of event A occurring
\begin{itemize}[leftmargin=*]
    \item Range: 0 $\leq$ P(A) $\leq$ 1
\end{itemize}

P(A|B) (Conditional probability): Probability of A given B has occurred
\begin{itemize}[leftmargin=*]
    \item Formula: P(A|B) = P(A,B) / P(B)
\end{itemize}

P(A,B) or P(A$\cap$B) (Joint probability): Probability of both A and B occurring

E[X] (Expected value): Average value of random variable X
\begin{itemize}[leftmargin=*]
    \item E[X] = $\Sigma$ x$\cdot$P(X=x) for discrete X
\end{itemize}

$\mathbb{E}$ (Expectation operator): Same as E, used in some contexts

$\sigma$ (Standard deviation): Measure of spread in a distribution

$\mu$ (Mean): Average value

$\Sigma$ (Summation): Sum over a range
\begin{itemize}[leftmargin=*]
    \item $\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n$
\end{itemize}

$\Pi$ (Product): Multiply over a range
\begin{itemize}[leftmargin=*]
    \item $\prod_{i=1}^{n} x_i = x_1 \times x_2 \times ... \times x_n$
\end{itemize}

argmax (Argument of maximum): The input that produces maximum output
\begin{itemize}[leftmargin=*]
    \item $\arg\max_x f(x)$ = the value of x that maximizes f(x)
    \item Example: argmax score(doc) = document with highest score
\end{itemize}

argmin (Argument of minimum): The input that produces minimum output

\subsubsection{Calculus Notation}

$\partial$ (Partial derivative): Derivative with respect to one variable
\begin{itemize}[leftmargin=*]
    \item $\partial$f/$\partial$x = rate of change of f with respect to x
\end{itemize}

$\nabla$ (Gradient): Vector of partial derivatives
\begin{itemize}[leftmargin=*]
    \item $\nabla f = [\partial f/\partial x_1, \partial f/\partial x_2, ..., \partial f/\partial x_n]$
\end{itemize}

$\int$ (Integral): Area under curve or accumulation

$\approx$ (Approximately equal): Two values are close but not exactly equal

$\equiv$ (Identically equal): Two expressions are always equal

$\to$ (Tends to/Maps to):
\begin{itemize}[leftmargin=*]
    \item Limit: x $\rightarrow$ 0 means "x approaches 0"
    \item Function: f: X $\rightarrow$ Y means "f maps from X to Y"
\end{itemize}

$\infty$ (Infinity): Unbounded quantity

\subsubsection{Logic \& Boolean Notation}

$\land$ (AND): Both conditions must be true
\begin{itemize}[leftmargin=*]
    \item A $\land$ B is true only if both A and B are true
\end{itemize}

$\lor$ (OR): At least one condition must be true
\begin{itemize}[leftmargin=*]
    \item A $\lor$ B is true if A is true, or B is true, or both
\end{itemize}

$\neg$ (NOT): Negation
\begin{itemize}[leftmargin=*]
    \item $\neg$A is true if A is false
\end{itemize}

$\Rightarrow$ (Implies): If A then B
\begin{itemize}[leftmargin=*]
    \item A $\Rightarrow$ B means "if A is true, then B must be true"
\end{itemize}

$\Leftrightarrow$ (If and only if): Bidirectional implication
\begin{itemize}[leftmargin=*]
    \item A $\Leftrightarrow$ B means A $\Rightarrow$ B and B $\Rightarrow$ A
\end{itemize}

$\forall$ (For all): Universal quantifier
\begin{itemize}[leftmargin=*]
    \item $\forall$x $\in$ S, P(x) means "for every x in S, property P(x) holds"
\end{itemize}

$\exists$ (There exists): Existential quantifier
\begin{itemize}[leftmargin=*]
    \item $\exists$x $\in$ S, P(x) means "there is at least one x in S where P(x) holds"
\end{itemize}

\subsubsection{Graph Theory Notation}

G = (V, E): Graph G with vertex set V and edge set E

V or V(G): Set of vertices/nodes in graph G

E or E(G): Set of edges in graph G

e = (u,v): Edge connecting vertices u and v

u $\rightarrow$ v: Directed edge from u to v

u --- v: Undirected edge between u and v

deg(v): Degree of vertex v (number of edges connected to it)

deg$^+$(v): Out-degree (outgoing edges in directed graph)

deg$^-$(v): In-degree (incoming edges in directed graph)

N(v): Neighborhood of v (set of vertices adjacent to v)

d(u,v): Distance between vertices u and v (length of shortest path)

|V|: Number of vertices (cardinality of vertex set)

|E|: Number of edges

path: Sequence of vertices [v$_1$, v$_2$, ..., v$_k$] where consecutive pairs are connected

\subsubsection{Complexity Notation (Big-O)}

O(n) (Big-O): Upper bound on growth rate
\begin{itemize}[leftmargin=*]
    \item O(n) = "at most proportional to n"
    \item O(1) = constant time
    \item O(log n) = logarithmic time
    \item O(n) = linear time
    \item O(n log n) = linearithmic time
    \item O(n$^2$) = quadratic time
    \item O(2$^n$) = exponential time
\end{itemize}

$\Omega$(n) (Big-Omega): Lower bound on growth rate

$\Theta$(n) (Big-Theta): Tight bound (both upper and lower)

\subsubsection{Common Symbols in RAG/KG Context}

\textbf{q} or $\vec{q}$: Query vector/text

\textbf{d} or $\vec{d}$: Document vector/text

k: Number of results to retrieve (top-k)

n: Number of documents or tokens or nodes

d (when not document): Dimensionality of embedding space

$\theta$ (theta): Angle between vectors, or model parameters

$\alpha, \beta, \gamma$ (alpha, beta, gamma): Weighting coefficients
\begin{itemize}[leftmargin=*]
    \item Example: score = $\alpha$$\cdot$BM25 + $\beta$$\cdot$Dense + $\gamma$$\cdot$Recency
\end{itemize}

$\lambda$ (lambda): Regularization parameter or weighting factor

$\epsilon$ (epsilon): Small positive number, error tolerance

$\delta$ (delta): Small change or difference

$\tau$ (tau): Threshold value

\subsection{Common Confusion: Terms That Sound Similar}

\textbf{Embedding vs Encoding}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Embedding}: The vector representation itself ([0.2, -0.5, ...])
    \item \textbf{Encoding}: The process of creating an embedding (running text through a model)
\end{itemize}

\textbf{Index vs Indexing}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Index} (noun): Data structure for fast search (e.g., FAISS index)
    \item \textbf{Indexing} (verb): Process of adding documents to an index
\end{itemize}

\textbf{Retrieval vs Retriever}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Retrieval}: The task/process of finding relevant documents
    \item \textbf{Retriever}: The system/component that performs retrieval
\end{itemize}

\textbf{Model vs Algorithm}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Model}: Learned parameters (neural network weights)
    \item \textbf{Algorithm}: Step-by-step procedure (BFS, Dijkstra)
\end{itemize}

\textbf{Dense vs Sparse} (two different meanings):
\begin{itemize}[leftmargin=*]
    \item \textbf{In vectors}: Dense = most elements non-zero, Sparse = most elements zero
    \item \textbf{In retrieval}: Dense = learned embeddings (DPR), Sparse = keyword-based (BM25)
\end{itemize}

\textbf{Graph vs Network}:
\begin{itemize}[leftmargin=*]
    \item Generally interchangeable, but:
    \item \textbf{Graph}: Mathematical abstraction, formal structure
    \item \textbf{Network}: Often implies real-world system (social network, neural network)
\end{itemize}

\textbf{Node vs Vertex}:
\begin{itemize}[leftmargin=*]
    \item Completely interchangeable terms for the same concept
    \item \textbf{Node}: More common in CS/databases
    \item \textbf{Vertex}: More common in mathematics
\end{itemize}

\textbf{Edge vs Link vs Relationship}:
\begin{itemize}[leftmargin=*]
    \item All refer to connections between nodes
    \item \textbf{Edge}: Mathematical/graph theory term
    \item \textbf{Link}: Web/networking term
    \item \textbf{Relationship}: Knowledge graph/database term
\end{itemize}

\textbf{Chunk vs Passage vs Segment}:
\begin{itemize}[leftmargin=*]
    \item All refer to pieces of documents
    \item \textbf{Chunk}: General term, can be any size
    \item \textbf{Passage}: Usually paragraph-sized, coherent semantic unit
    \item \textbf{Segment}: Generic division of text
\end{itemize}

\textbf{Context vs Context Window}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Context}: The information provided to LLM (retrieved documents + query)
    \item \textbf{Context Window}: The maximum token limit the LLM can process
\end{itemize}

\textbf{Latency vs Throughput}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Latency}: How long one request takes (measured in milliseconds)
    \item \textbf{Throughput}: How many requests per second (measured in requests/sec or QPS)
\end{itemize}

\textbf{Precision vs Accuracy}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Precision}: Of retrieved items, what fraction are relevant?
    \item \textbf{Accuracy}: Of all items, what fraction are correctly classified?
    \item In RAG: Precision is more important than accuracy
\end{itemize}

\textbf{Training vs Inference}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Training}: Learning model parameters from data (done once, expensive)
    \item \textbf{Inference}: Using trained model to make predictions (done many times, needs to be fast)
\end{itemize}

\textbf{Embedding Model vs LLM}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Embedding Model}: Converts text to vectors (BERT, text-embedding-3)
    \item \textbf{LLM}: Generates text (GPT-4, Claude)
    \item Some models can do both (e.g., BERT can be used for embeddings or classification)
\end{itemize}

\begin{aside}
If you're confused about some of these distinctions, that's normal. Many of these terms won't click until you've used them in practice. The confusion is a feature, not a bug - it means you're paying attention to nuance.
\end{aside}

% End of Section 1
\clearpage

% ============================================================================
% SECTION 2: BEGINNER-FRIENDLY FOUNDATIONS
% ============================================================================

\chapter{BEGINNER-FRIENDLY FOUNDATIONS}

\begin{aside}
This section builds your mental model of how everything works under the hood. If you already know LLMs, embeddings, and graphs, you might be tempted to skip this. Don't. The nuances here matter for production systems, and we'll highlight failure modes that aren't obvious until you've shipped broken code to users.
\end{aside}

\section{LLM Basics (Large Language Models)}

\subsection{What is an LLM?}

\textbf{Simple Analogy}: Imagine a super-smart autocomplete that has read most of the internet. You give it a prompt, it predicts the most likely continuation.

\textbf{Technical Definition}: A neural network trained on vast text data to predict the next token (word/subword) given previous context.

\subsection{Core Concepts You Must Understand}

\subsubsection{Tokens \& Tokenization}

\textbf{What}: Text is broken into chunks (tokens) before processing.

\begin{lstlisting}[style=python]
# Example: How text becomes tokens
text = "RAG systems are powerful"

# GPT tokenization (simplified)
tokens = ["RAG", " systems", " are", " powerful"]
token_ids = [22060, 6067, 527, 8147]  # Numeric IDs

# Why it matters:
# - APIs charge per token
# - Models have token limits (8k, 32k, 128k)
# - 1 token $\approx$ 0.75 words on average
\end{lstlisting}

\textbf{Key Insight}: "Hello world" = 2 tokens, but "Supercalifragilisticexpialidocious" might be 5+ tokens.

\subsubsection{Embeddings}

\textbf{What}: Converting text into dense vector representations (arrays of numbers) that capture semantic meaning.

\textbf{Analogy}: Like coordinates on a map, but instead of (latitude, longitude), you have 1536 dimensions representing meaning.

\begin{lstlisting}[style=python]
from openai import OpenAI
client = OpenAI()

# Create an embedding
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Knowledge graphs organize information"
)

embedding = response.data[0].embedding
# Result: [0.023, -0.15, 0.087, ..., 0.032]  # 1536 numbers
# Length: 1536 dimensions

# Similar sentences have similar embeddings
embedding_2 = client.embeddings.create(
    model="text-embedding-3-small",
    input="Graphs structure knowledge"
).data[0].embedding

# Cosine similarity will be high (0.85+)
\end{lstlisting}

\textbf{Why Embeddings Matter for RAG}:
\begin{itemize}[leftmargin=*]
    \item They enable semantic search ("find similar meaning" not just keyword matching)
    \item Power vector databases
    \item Core of retrieval systems
\end{itemize}

\textbf{Visualization}:
\begin{lstlisting}
Text Space:              Embedding Space (simplified to 2D):
"Dog"                    (0.8, 0.6)  
"Cat"                    (0.75, 0.65)   <- Close to dog
"Knowledge Graph"        (-0.2, 0.9)        
"Graph Database"         (-0.15, 0.85)       <- Close to KG

Distance = Semantic Similarity
\end{lstlisting}

\subsubsection{Prompting Fundamentals}

\textbf{Zero-Shot Prompting}:
\begin{lstlisting}
Prompt: "Translate to French: Hello"
Response: "Bonjour"
\end{lstlisting}

\textbf{Few-Shot Prompting}:
\begin{lstlisting}
Prompt:
"
Translate to French:
Hello -> Bonjour
Goodbye -> Au revoir
Thank you -> ?
"
Response: "Merci"
\end{lstlisting}

\textbf{Structured Prompting} (Critical for RAG):
\begin{lstlisting}[style=python]
prompt = f"""
You are a helpful assistant that answers questions using provided context.

Context:
{retrieved_documents}

Question: {user_question}

Instructions:
- Only use information from the context
- If the answer isn't in the context, say "I don't know"
- Cite the source document

Answer:
"""
\end{lstlisting}

\subsubsection{Transformer Architecture Deep Dive}

\textbf{Why Transformers Matter for RAG}: Understanding transformer architecture helps you choose the right models, optimize inference, and debug issues in production RAG systems.

\begin{aside}
This subsection gets mathematical. That's unavoidable - transformers are the engine under the hood of everything you'll build. You don't need to memorize the equations, but you should understand what each component does and why. If your eyes glaze over during the attention mechanism explanation, that's normal. Come back to it later when you're debugging why your retrieval is slow.
\end{aside}

\paragraph{The Transformer Revolution}

\textbf{Before Transformers (Pre-2017)}:
\begin{itemize}[leftmargin=*]
    \item \textbf{RNNs/LSTMs}: Sequential processing, slow, can't parallelize
    \item \textbf{Limited context}: Struggled with long-range dependencies
    \item \textbf{No bidirectional context}: Hard to capture full semantic meaning
\end{itemize}

\textbf{After Transformers (2017-present)}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Parallel processing}: All tokens processed simultaneously
    \item \textbf{Self-attention}: Every token can attend to every other token
    \item \textbf{Scalability}: Can be trained on massive datasets efficiently
\end{itemize}

\paragraph{Core Components of a Transformer}

\textbf{1. Input Embedding Layer}

Text $\rightarrow$ Tokens $\rightarrow$ Embeddings:
\begin{lstlisting}
"The cat sat" -> [501, 2368, 3287] (token IDs)
              -> [[0.2, -0.5, ...], [0.1, 0.8, ...], [-0.3, 0.2, ...]]
              (d-dimensional vectors, typically d=768 or 1536)
\end{lstlisting}

\textbf{2. Positional Encoding}

\textbf{Problem}: Self-attention is position-invariant ("cat sat" = "sat cat")

\textbf{Solution}: Add positional information to embeddings

\begin{lstlisting}
PE(pos, 2i)   = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))

where:
- pos = position in sequence
- i = dimension index
- d = embedding dimension
\end{lstlisting}

\textbf{Why sine/cosine?}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Bounded}: Values stay in [-1, 1]
    \item \textbf{Unique}: Each position gets unique encoding
    \item \textbf{Relative positioning}: PE(pos+k) can be expressed as linear function of PE(pos)
    \item \textbf{Extrapolation}: Can handle sequences longer than seen in training
\end{itemize}

\textbf{Alternative}: Learned positional embeddings (used in BERT, GPT)

\textbf{3. Multi-Head Self-Attention}

\textbf{Single Attention Head}:
\begin{lstlisting}
Input: X $\in$ $\mathbb{R}$^(n$\times$d)  (n tokens, d dimensions each)

1. Project to Q, K, V:
   Q = XW_Q,  K = XW_K,  V = XW_V
   where W_Q, W_K, W_V $\in$ $\mathbb{R}$^(d$\times$d_k)

2. Compute attention scores:
   Attention(Q,K,V) = softmax(QK^T / $\sqrt{}$d_k) V

Step-by-step:
   - QK^T: n$\times$n matrix of all-pairs dot products (how relevant is each token to each other?)
   - / $\sqrt{}$d_k: Scale to prevent vanishing gradients
   - softmax: Normalize to probabilities (each row sums to 1)
   - $\times$ V: Weighted sum of value vectors
\end{lstlisting}

\textbf{Why $\sqrt{}$d\_k Scaling?}

Without scaling, for large $d_k$:
\begin{lstlisting}
If Q,K have zero mean and unit variance:
QK^T has variance d_k

For d_k = 512: dot products are in range [-30, 30]
softmax([-30, -5, 0, 5, 30]) $\approx$ [0, 0, 0, 0, 1]  <- All weight on one token!

With scaling by $\sqrt{}$d_k:
QK^T / $\sqrt{}$d_k has variance 1
softmax([-4.2, -0.7, 0, 0.7, 4.2]) $\approx$ [0.01, 0.12, 0.24, 0.48, 0.15]  <- Better distribution!
\end{lstlisting}

\begin{aside}
This scaling factor might seem like a minor detail. It's not. Without it, attention collapses to mostly zeros and ones, and your model learns nothing. This is one of those "the devil is in the details" moments that separates working code from broken code.
\end{aside}

\textbf{Multi-Head Attention}:

Instead of one attention, use h parallel heads:

\begin{lstlisting}
MultiHead(Q,K,V) = Concat(head$_1$, head$_2$, ..., head_h) W_O

where head_i = Attention(QW_Q$^i$, KW_K$^i$, VW_V$^i$)
\end{lstlisting}

\textbf{Why Multiple Heads?}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Different relationships}: Head 1 might capture syntax, Head 2 semantics, Head 3 coreference
    \item \textbf{Different subspaces}: Each head operates in different $d_k$-dimensional subspace
    \item \textbf{Ensemble effect}: Combining heads gives robust representation
\end{itemize}

\textbf{Example}:
\begin{lstlisting}
Sentence: "The cat sat on the mat"

Head 1 (Syntax):        Head 2 (Semantics):     Head 3 (Reference):
"cat" -> "sat" (0.8)     "cat" -> "mat" (0.6)     "cat" -> "The" (0.7)
(subject-verb)          (agent-location)         (noun-determiner)
\end{lstlisting}

\textbf{4. Feed-Forward Networks}

After attention, each position passes through identical FFN:

\begin{lstlisting}
FFN(x) = ReLU(xW$_1$ + b$_1$)W$_2$ + b$_2$

where:
- W$_1$ $\in$ $\mathbb{R}$^(d_model $\times$ d_ff), typically d_ff = 4 $\times$ d_model
- W$_2$ $\in$ $\mathbb{R}$^(d_ff $\times$ d_model)
\end{lstlisting}

\textbf{Why FFN?}:
\begin{itemize}[leftmargin=*]
    \item Attention captures relationships, FFN adds non-linearity and expressiveness
    \item Each position processed independently (no mixing across positions)
    \item Huge parameter count (most parameters are here!)
\end{itemize}

\textbf{5. Layer Normalization \& Residual Connections}

\begin{lstlisting}
# After attention
x = LayerNorm(x + MultiHeadAttention(x))

# After FFN
x = LayerNorm(x + FFN(x))
\end{lstlisting}

\textbf{LayerNorm}:
\begin{lstlisting}
LayerNorm(x) = $\gamma$ $\odot$ (x - $\mu$) / $\sigma$ + $\beta$

where:
- $\mu$ = mean(x)
- $\sigma$ = std(x)
- $\gamma$, $\beta$ = learned parameters
\end{lstlisting}

\textbf{Why This Matters}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Residual connections}: Prevent vanishing gradients in deep networks (GPT-3 has 96 layers!)
    \item \textbf{Layer norm}: Stabilizes training, allows higher learning rates
\end{itemize}

\paragraph{Encoder vs. Decoder Transformers}

\textbf{Encoder} (BERT):
\begin{lstlisting}
Input: Full sentence
Attention: Bidirectional (each token sees all tokens)
Output: Contextualized representation of each token
Use case: Understanding, classification, embedding
\end{lstlisting}

\textbf{Decoder} (GPT):
\begin{lstlisting}
Input: Prefix of sequence
Attention: Causal/Masked (token i can only see tokens $\leq$ i)
Output: Probability distribution for next token
Use case: Generation, completion
\end{lstlisting}

\textbf{Encoder-Decoder} (T5, BART):
\begin{lstlisting}
Encoder: Process input
Decoder: Generate output, attending to encoder
Use case: Translation, summarization
\end{lstlisting}

\textbf{Causal Masking in Decoders}:
\begin{lstlisting}
Attention matrix without mask:
     t1   t2   t3   t4
t1 [0.2  0.3  0.1  0.4]
t2 [0.1  0.4  0.2  0.3]
t3 [0.3  0.1  0.5  0.1]
t4 [0.2  0.2  0.2  0.4]

With causal mask (zero out future):
     t1   t2   t3   t4
t1 [1.0  0    0    0  ]
t2 [0.3  0.7  0    0  ]
t3 [0.2  0.1  0.7  0  ]
t4 [0.2  0.2  0.2  0.4]

This prevents t2 from "cheating" by looking at t3, t4
\end{lstlisting}

\paragraph{Complete Transformer Block}

\begin{lstlisting}
Input: Token embeddings + Positional encodings
  $\downarrow$
Multi-Head Attention
  $\downarrow$
Add & Norm (residual + layer norm)
  $\downarrow$
Feed-Forward Network
  $\downarrow$
Add & Norm
  $\downarrow$
Output: Contextualized representations

$\times$  N layers (N=12 for BERT-base, N=96 for GPT-3)
\end{lstlisting}

\paragraph{Key Parameters and Model Sizes}

\textbf{BERT-base}:
\begin{itemize}[leftmargin=*]
    \item Layers: 12
    \item Hidden size: 768
    \item Attention heads: 12
    \item Parameters: 110M
    \item Context window: 512 tokens
\end{itemize}

\textbf{BERT-large}:
\begin{itemize}[leftmargin=*]
    \item Layers: 24
    \item Hidden size: 1024
    \item Attention heads: 16
    \item Parameters: 340M
\end{itemize}

\textbf{GPT-3}:
\begin{itemize}[leftmargin=*]
    \item Layers: 96
    \item Hidden size: 12,288
    \item Attention heads: 96
    \item Parameters: 175B
    \item Context window: 2048 tokens
\end{itemize}

\textbf{GPT-4} (estimated):
\begin{itemize}[leftmargin=*]
    \item Parameters: ~1.76T (mixture of experts)
    \item Context window: 128K tokens
\end{itemize}

\paragraph{Computational Complexity}

\textbf{Self-Attention}: O(n$^2$ $\cdot$ d)
\begin{itemize}[leftmargin=*]
    \item n = sequence length
    \item d = embedding dimension
    \item Bottleneck for long sequences!
\end{itemize}

\textbf{Why This Matters for RAG}:
\begin{itemize}[leftmargin=*]
    \item Long documents $\rightarrow$ expensive to embed
    \item Chunking reduces n $\rightarrow$ manageable computation
    \item Context window limits affect retrieval design
\end{itemize}

\textbf{Approximations for Long Sequences}:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Sparse attention} (BigBird, Longformer): O(n $\cdot$ log n)
    \item \textbf{Linear attention}: O(n $\cdot$ d$^2$)
    \item \textbf{Chunking}: Process in windows (used in RAG!)
\end{enumerate}

\paragraph{Inference Optimization for RAG}

\begin{aside}
Most tutorials skip these optimizations. Then you deploy to production and wonder why your RAG system costs \$10,000/month and takes 5 seconds per query. Read this section carefully. Your AWS bill will thank you.
\end{aside}

\textbf{KV Caching}:

When generating tokens autoregressively:
\begin{lstlisting}
# Without KV cache:
t1: compute attention for "The"
t2: recompute attention for "The", compute for "cat"
t3: recompute for "The", "cat", compute for "sat"
-> O(n$^2$) redundant computation!

# With KV cache:
t1: compute K,V for "The", cache them
t2: reuse K,V for "The", compute only for "cat"
t3: reuse K,V for "The", "cat", compute only for "sat"
-> O(n) computation, huge speedup!
\end{lstlisting}

\textbf{Batch Processing}:
\begin{lstlisting}[style=python]
# Inefficient: one at a time
for doc in documents:
    embedding = model.encode(doc)  # Separate forward pass

# Efficient: batched
embeddings = model.encode(documents, batch_size=32)  # One forward pass
# 10-100x faster!
\end{lstlisting}

\section{Retrieval Basics}

\subsection{What is Retrieval?}

\textbf{Goal}: Given a query, find the most relevant documents from a large collection.

\textbf{Real-World Analogy}:
\begin{itemize}[leftmargin=*]
    \item Google Search = Retrieval system
    \item Library catalog = Retrieval system
    \item Your brain searching memories = Retrieval system
\end{itemize}

\subsection{Types of Retrieval}

\subsubsection{Keyword Search (BM25)}

\textbf{How it works}: Count matching words, adjust for document length and term rarity.

\begin{lstlisting}[style=python]
from rank_bm25 import BM25Okapi

documents = [
    "Knowledge graphs represent structured information",
    "RAG combines retrieval with generation",
    "Vector databases enable semantic search"
]

# Tokenize
tokenized_docs = [doc.split() for doc in documents]

# Build BM25 index
bm25 = BM25Okapi(tokenized_docs)

# Query
query = "semantic search"
scores = bm25.get_scores(query.split())
# [0.2, 0.1, 0.9] <- Document 3 wins
\end{lstlisting}

\textbf{Strengths}: Fast, works with exact matches, no ML needed\\
\textbf{Weaknesses}: Misses semantic similarity ("car" vs "automobile")

\subsubsection{Semantic Search (Dense Retrieval)}

\begin{aside}
You might be thinking "BM25 is old-school, I'll just use embeddings for everything." Please don't. BM25 beats semantic search for exact phrase matching, rare technical terms, and names. This is why we combine them in hybrid retrieval - and why ignoring BM25 will haunt you when users search for product SKUs or error codes.
\end{aside}

\textbf{How it works}: Convert query and documents to embeddings, find nearest neighbors.

\begin{lstlisting}[style=python]
import numpy as np
from openai import OpenAI

client = OpenAI()

def get_embedding(text):
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

# Embed documents
docs = [
    "Knowledge graphs represent structured information",
    "RAG combines retrieval with generation",
    "Vector databases enable semantic search"
]
doc_embeddings = [get_embedding(doc) for doc in docs]

# Embed query
query = "what is semantic search?"
query_embedding = get_embedding(query)

# Calculate cosine similarity
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

scores = [cosine_similarity(query_embedding, doc_emb)
          for doc_emb in doc_embeddings]
# [0.65, 0.58, 0.91] <- Document 3 wins (semantic match!)
\end{lstlisting}

\textbf{Strengths}: Understands meaning, handles synonyms\\
\textbf{Weaknesses}: Slower, requires embeddings, can miss exact matches

\subsubsection{Hybrid Retrieval (Best of Both)}

\begin{lstlisting}[style=python]
# Combine BM25 + Semantic
bm25_scores = normalize(bm25.get_scores(query))
semantic_scores = normalize(cosine_similarities)

# Weighted combination
final_scores = 0.3 * bm25_scores + 0.7 * semantic_scores
\end{lstlisting}

\subsection{Vector Databases}

\textbf{What}: Specialized databases optimized for storing and searching embeddings.

\textbf{Key Operations}:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Insert}: Store vectors with metadata
    \item \textbf{Search}: Find k-nearest neighbors (kNN)
    \item \textbf{Filter}: Combine vector search with metadata filters
\end{enumerate}

\begin{lstlisting}[style=python]
import chromadb

# Initialize
client = chromadb.Client()
collection = client.create_collection("my_docs")

# Add documents
collection.add(
    documents=["RAG is powerful", "KG structures knowledge"],
    ids=["doc1", "doc2"],
    metadatas=[{"source": "paper1"}, {"source": "paper2"}]
)

# Query
results = collection.query(
    query_texts=["what is RAG?"],
    n_results=2
)
# Returns: most similar documents
\end{lstlisting}

\textbf{Popular Vector DBs}:
\begin{itemize}[leftmargin=*]
    \item \textbf{FAISS}: Fast, local, Facebook's library
    \item \textbf{ChromaDB}: Simple, embedded, great for prototyping
    \item \textbf{Pinecone}: Managed, production-grade, scales automatically
    \item \textbf{Weaviate}: Open-source, full-featured
\end{itemize}

\section{Knowledge Graph Fundamentals}

\subsection{What is a Knowledge Graph?}

\textbf{Definition}: A graph-structured database where knowledge is stored as entities (nodes) and relationships (edges).

\textbf{Real-World Analogy}:
\begin{itemize}[leftmargin=*]
    \item Social network (Facebook): People = nodes, Friendships = edges
    \item Map: Cities = nodes, Roads = edges
    \item Knowledge: Concepts = nodes, Relationships = edges
\end{itemize}

\subsection{Core Components}

\subsubsection{Nodes (Entities)}
Things that exist: Person, Company, Product, Concept

\subsubsection{Edges (Relationships)}
How things connect: WORKS\_FOR, OWNS, IS\_PART\_OF

\subsubsection{Properties}
Attributes of nodes/edges: name, age, date, weight

\subsection{Graph Representation}

\textbf{Visual}:
\begin{lstlisting}
(Person:Alice {age: 30})
       |
       | -[WORKS_FOR {since: 2020}]->
       |
       v
(Company:Acme {industry: "Tech"})
\end{lstlisting}

\textbf{Triple Format} (Subject-Predicate-Object):
\begin{lstlisting}
Alice WORKS_FOR Acme
Alice AGE 30
Acme INDUSTRY "Tech"
\end{lstlisting}

\textbf{Why Graphs Beat Tables}:

\begin{aside}
This is the most common question: "Why not just use PostgreSQL?" Fair question. Short answer: for simple lookups, you should. But try expressing "find friends-of-friends who work at competitors of companies in my portfolio" in SQL. You'll end up with 5 self-joins and a query planner that gives up. Graphs shine for traversals and multi-hop queries. Everything else, use the tool you already know.
\end{aside}

\textbf{Relational Database (Tables)}:
\begin{lstlisting}
Employees Table:
| ID | Name  | Company | Age |
|----|-------|---------|-----|
| 1  | Alice | Acme    | 30  |

Companies Table:
| Name | Industry |
|------|----------|
| Acme | Tech     |

# To find "Who works in Tech?":
# Need JOIN operation - slow for complex queries
\end{lstlisting}

\textbf{Knowledge Graph}:
\begin{lstlisting}[style=cypher]
MATCH (p:Person)-[:WORKS_FOR]->(c:Company {industry: "Tech"})
RETURN p.name

# Direct traversal - fast even with millions of nodes
\end{lstlisting}

\subsection{Graph Theory Intuition}

\subsubsection{Paths}
Sequence of connected nodes:
\begin{lstlisting}
Alice -> WORKS_FOR -> Acme -> LOCATED_IN -> San Francisco
\end{lstlisting}

\subsubsection{Multi-Hop Queries}
Follow multiple relationships:
\begin{lstlisting}
"Find friends of friends who work at tech companies"
(Me)-[:FRIEND]->(Friend)-[:FRIEND]->(FoF)-[:WORKS_FOR]->(Company {industry: "Tech"})
\end{lstlisting}

\subsubsection{Neighborhoods}
All nodes within N steps:
\begin{lstlisting}
# 1-hop neighborhood of Alice
Alice -> Acme, Bob, Project_X

# 2-hop neighborhood
Alice -> Acme -> [All employees], Bob -> [Bob's friends], ...
\end{lstlisting}

\section{Cypher \& SPARQL Basics}

\subsection{Cypher (Neo4j Query Language)}

\textbf{ASCII Art Syntax} - Intuitive and visual!

\begin{lstlisting}[style=cypher]
// Create nodes
CREATE (a:Person {name: "Alice", age: 30})
CREATE (c:Company {name: "Acme"})

// Create relationship
CREATE (a)-[:WORKS_FOR {since: 2020}]->(c)

// Query: Find all people working at Acme
MATCH (p:Person)-[:WORKS_FOR]->(c:Company {name: "Acme"})
RETURN p.name, p.age

// Multi-hop: Friends of Alice who work in Tech
MATCH (alice:Person {name: "Alice"})-[:FRIEND]-(friend)-[:WORKS_FOR]->(c:Company {industry: "Tech"})
RETURN friend.name, c.name

// Aggregation: Count employees per company
MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
RETURN c.name, COUNT(p) AS employee_count
ORDER BY employee_count DESC
\end{lstlisting}

\subsection{SPARQL (RDF Query Language)}

\textbf{Used for}: Semantic web, ontologies, Wikidata

\begin{lstlisting}
# Find all companies Alice works for
SELECT ?company WHERE {
    :Alice :worksFor ?company .
}

# Multi-hop
SELECT ?friendCompany WHERE {
    :Alice :friend ?friend .
    ?friend :worksFor ?friendCompany .
}
\end{lstlisting}

\textbf{For this course}: We'll focus on \textbf{Cypher} (more popular in industry).

% End of Section 2 basic content
\clearpage

% ============================================================================
% SECTION 2A: THEORETICAL FOUNDATIONS (DEEP DIVE)
% ============================================================================

\chapter{THEORETICAL FOUNDATIONS (DEEP DIVE)}

\begin{quotation}
\textit{"Theory without practice is sterile, practice without theory is blind."} - Immanuel Kant
\end{quotation}

This section provides the mathematical and conceptual foundations that power RAG and Knowledge Graph systems. Understanding these principles deeply will transform you from a code copier to an AI systems architect.

\begin{aside}
Fair warning: This section is dense. We're going to cover vector space theory, information retrieval theory, graph theory, and semantic similarity - all with actual math. If you're here to copy-paste code and move on, skip this section and come back when something breaks in production and you need to understand why. If you want to be the person who designs the system rather than just using it, buckle up.
\end{aside}

\section{Vector Space Theory \& Embeddings (The Mathematics of Meaning)}

\subsection{The Core Idea: Meaning as Geometry}

\textbf{Fundamental Insight}: If we can represent words, sentences, or documents as points in a high-dimensional space, then similar meanings should be close together geometrically.

\textbf{Historical Context}: This idea dates back to distributional semantics (1950s): \textit{"You shall know a word by the company it keeps"} - J.R. Firth. Modern embeddings (Word2Vec 2013, BERT 2018) are the mathematical realization of this principle.

\subsection{Vector Spaces: A Primer}

A \textbf{vector space} is a mathematical structure where:
\begin{itemize}[leftmargin=*]
    \item Each point is represented by coordinates: \textbf{v} = [v$_1$, v$_2$, ..., v$_n$]
    \item You can add vectors and multiply by scalars
    \item Distance and angle have meaning
\end{itemize}

\textbf{Example in 2D}:
\begin{lstlisting}
Word "king" = [0.5, 0.8]
Word "queen" = [0.4, 0.7]
Word "man" = [0.3, 0.1]
Word "woman" = [0.2, 0.0]

Geometry shows: king - man + woman $\approx$ queen
\end{lstlisting}

This is the famous word analogy property!

\subsection{Why High Dimensions?}

\textbf{Real embeddings use 768-4096 dimensions}. Why so many?

\begin{aside}
This is one of those questions that seems simple but has a deep answer. The short version: we need enough dimensions to keep millions of different meanings separate. The long version involves manifold theory and will make your head hurt. We'll give you both.
\end{aside}

\textbf{Curse of Dimensionality Paradox}: In high dimensions:
\begin{itemize}[leftmargin=*]
    \item Most points are far from each other (good for distinguishing meanings)
    \item But angles become more meaningful than distances
    \item More capacity to encode subtle semantic distinctions
\end{itemize}

\textbf{Information Content}: Language has ~10$^5$ common words $\times$ multiple senses = need high dimensions to keep them separated.

\subsubsection{The Mathematical Justification for High Dimensionality}

The necessity of high-dimensional embeddings can be rigorously understood through the \textbf{Johnson-Lindenstrauss Lemma}, which states that a set of points in high-dimensional space can be embedded into a lower-dimensional space while approximately preserving pairwise distances.

\textbf{Formal Statement}: For any 0 < $\varepsilon$ < 1, a set of n points in $\mathbb{R}^D$ can be embedded into $\mathbb{R}^k$ where k = O(log(n)/$\varepsilon$$^2$), such that all pairwise distances are preserved within a factor of (1 $\pm$ $\varepsilon$).

\textbf{Implications for Embeddings}:
\begin{itemize}[leftmargin=*]
    \item With 100,000 words and $\varepsilon$ = 0.1 (10\% error tolerance), we need k $\approx$ 115,000 dimensions theoretically
    \item However, semantic structure has redundancy and lower intrinsic dimensionality
    \item Modern embeddings (768-1536 dims) represent a practical compromise between:
    \begin{itemize}
        \item \textbf{Expressiveness}: Enough dimensions to separate distinct meanings
        \item \textbf{Computational efficiency}: Small enough for fast similarity computation
        \item \textbf{Statistical efficiency}: Not so high that we need enormous training data
    \end{itemize}
\end{itemize}

\subsubsection{Intrinsic Dimensionality of Language}

Research suggests that while embeddings use 768+ dimensions, the \textbf{intrinsic dimensionality} of semantic space is much lower (estimated 50-200 dimensions). This means:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Manifold Hypothesis}: Semantic meanings lie on a lower-dimensional manifold embedded in high-dimensional space
    \item \textbf{Redundancy}: Many dimensions encode similar information (entangled representations)
    \item \textbf{Optimization}: High dimensions make training easier (less local minima) even if not all are strictly necessary
\end{enumerate}

\textbf{Empirical Evidence}:
\begin{lstlisting}[style=python]
# Principal Component Analysis on word2vec embeddings
# Typically shows that 95% of variance captured in ~100 principal components
# Yet we use 300 dims because:
# - Easier to train
# - Better generalization
# - Captures rare semantic distinctions
\end{lstlisting}

\subsubsection{The Geometry of Meaning: A Deeper Dive}

\textbf{Vector Space Axioms Applied to Semantics}:

A vector space V over a field F (typically $\mathbb{R}$ for embeddings) satisfies:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Closure under addition}: v + w $\in$ V for all v, w $\in$ V
    \begin{itemize}
        \item Semantic meaning: Combining concepts creates new concepts
        \item Example: "king" + "crown" = "monarchy"
    \end{itemize}

    \item \textbf{Associativity}: (u + v) + w = u + (v + w)
    \begin{itemize}
        \item Meaning composition is consistent regardless of grouping
    \end{itemize}

    \item \textbf{Existence of zero vector}: $\exists$ 0 $\in$ V such that v + 0 = v
    \begin{itemize}
        \item The "null meaning" or "no information" vector
    \end{itemize}

    \item \textbf{Existence of additive inverse}: For each v, $\exists$ -v such that v + (-v) = 0
    \begin{itemize}
        \item Semantic opposites: "hot" + "cold" $\approx$ neutral
    \end{itemize}

    \item \textbf{Scalar multiplication}: $\alpha$$\cdot$v $\in$ V for all $\alpha$ $\in$ $\mathbb{R}$, v $\in$ V
    \begin{itemize}
        \item Intensity or magnitude of meaning
        \item Example: 2$\cdot$"happy" = "very happy", 0.5$\cdot$"run" = "jog"
    \end{itemize}
\end{enumerate}

\textbf{Why This Mathematical Structure Matters}:

The vector space structure enables \textbf{algebraic reasoning about meaning}:
\begin{itemize}[leftmargin=*]
    \item We can "solve" for unknown concepts: "king" - "man" + "woman" = ?
    \item We can interpolate: 0.7$\cdot$"walk" + 0.3$\cdot$"run" $\approx$ "jog"
    \item We can find orthogonal (unrelated) concepts using the nullspace
\end{itemize}

\subsection{The Mathematics of Embeddings}

\subsubsection{Cosine Similarity (The Core Metric)}

\textbf{Why cosine, not Euclidean distance?}

\begin{aside}
This question comes up constantly. "Why not just use normal distance?" Because normal distance is fooled by vector magnitude. A 10-page essay and a 1-sentence summary might have identical meaning but very different vector magnitudes. Cosine similarity only cares about direction, not length. This makes it scale-invariant, which is exactly what you want for semantic similarity.
\end{aside}

Given two vectors \textbf{u} and \textbf{v}:

\begin{lstlisting}
Euclidean Distance: d(u,v) = ||u - v|| = $\sqrt{}$($\Sigma$(u$_i$ - v$_i$)$^2$)
Cosine Similarity:  cos($\theta$) = (u$\cdot$v)/(||u|| ||v||) = $\Sigma$(u$_i$v$_i$)/$\sqrt{}$($\Sigma$u$_i$$^2$)$\sqrt{}$($\Sigma$v$_i$$^2$)
\end{lstlisting}

\textbf{Cosine similarity ranges from -1 to 1}:
\begin{itemize}[leftmargin=*]
    \item 1 = same direction (identical meaning)
    \item 0 = orthogonal (unrelated)
    \item -1 = opposite direction (antonyms)
\end{itemize}

\textbf{Why cosine wins}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Scale invariant}: "good movie" and "really really good movie" should be similar despite different vector magnitudes
    \item \textbf{Normalized}: Always in [-1, 1], easy to interpret
    \item \textbf{Angular}: Captures semantic relationship independent of frequency
\end{itemize}

\textbf{Geometric Intuition}:
\begin{lstlisting}
         u  /
           /$\theta$
          /_____ v

cos($\theta$) = how aligned the vectors are
Small $\theta$ -> cos($\theta$) $\approx$ 1 -> very similar
Large $\theta$ -> cos($\theta$) $\approx$ 0 -> unrelated
\end{lstlisting}

\paragraph{Mathematical Properties of Cosine Similarity}

\textbf{1. Relationship to Dot Product}:

For normalized vectors ($||u|| = ||v|| = 1$), cosine similarity reduces to the dot product:
\begin{lstlisting}
cos($\theta$) = u $\cdot$ v = $\Sigma$$_i$ u$_i$v$_i$
\end{lstlisting}

This is why many vector databases normalize embeddings and use dot product for speed!

\textbf{2. Metric Properties (or lack thereof)}:

Cosine similarity is NOT a metric because it violates the triangle inequality. However, we can convert it to a metric:

\begin{lstlisting}
d_cos(u,v) = 1 - cos(u,v)  (cosine distance)
\end{lstlisting}

Or for a proper metric:
\begin{lstlisting}
d_angular(u,v) = arccos(cos(u,v)) / $\pi$
\end{lstlisting}

This gives values in [0,1] and satisfies triangle inequality.

\textbf{3. Computational Optimization}:

For large-scale retrieval:
\begin{lstlisting}[style=python]
# Naive: O(nd) for n documents, d dimensions
similarities = [cosine(query, doc) for doc in documents]

# Optimized with normalization + matrix multiplication: O(nd) but much faster
# Normalize once
docs_normalized = docs / np.linalg.norm(docs, axis=1, keepdims=True)
query_normalized = query / np.linalg.norm(query)

# Single matrix multiplication
similarities = np.dot(docs_normalized, query_normalized)
\end{lstlisting}

\textbf{4. Why Cosine for Text?}:

Theoretical justification from \textbf{distributional semantics}:

\begin{itemize}[leftmargin=*]
    \item Document vectors represent word co-occurrence statistics
    \item Longer documents have larger magnitude but same semantic content
    \item Cosine normalizes away document length, focusing on \textbf{word distribution}
\end{itemize}

\textbf{Example}:
\begin{lstlisting}
Doc 1: "dog cat dog cat dog cat" -> [3, 3]
Doc 2: "dog cat"                  -> [1, 1]

Euclidean distance: ||[3,3] - [1,1]|| = 2.83 (seems different!)
Cosine similarity:  [3,3]$\cdot$[1,1] / (|[3,3]||[1,1]|) = 6/(4.24*1.41) = 1.0 (identical!)
\end{lstlisting}

Both documents have the same semantic content (50\% dog, 50\% cat), and cosine correctly identifies this.

\paragraph{Alternative Similarity Measures and When to Use Them}

\textbf{Euclidean Distance (L2)}:
\begin{lstlisting}
d(u,v) = ||u - v|| = $\sqrt{}$($\Sigma$(u$_i$ - v$_i$)$^2$)
\end{lstlisting}
\begin{itemize}[leftmargin=*]
    \item \textbf{Use when}: Magnitude matters (e.g., embedding dense entities with varying importance)
    \item \textbf{RAG application}: Less common, but useful for hierarchical embeddings
\end{itemize}

\textbf{Manhattan Distance (L1)}:
\begin{lstlisting}
d(u,v) = $\Sigma$|u$_i$ - v$_i$|
\end{lstlisting}
\begin{itemize}[leftmargin=*]
    \item \textbf{Use when}: Sparse vectors, interpretable dimensions
    \item \textbf{RAG application}: TF-IDF vectors, bag-of-words
\end{itemize}

\textbf{Dot Product (Inner Product)}:
\begin{lstlisting}
u $\cdot$ v = $\Sigma$ u$_i$v$_i$
\end{lstlisting}
\begin{itemize}[leftmargin=*]
    \item \textbf{Use when}: Vectors are normalized OR magnitude encodes importance
    \item \textbf{RAG application}: Fast approximate nearest neighbor search (FAISS uses this)
\end{itemize}

\textbf{Comparison Table}:
\begin{lstlisting}
Similarity Measure | Normalized? | Metric? | Speed    | Best For
-------------------|-------------|---------|----------|------------------
Cosine             | Yes         | No*     | Fast     | Text embeddings
Euclidean          | No          | Yes     | Fast     | Dense vectors
Dot Product        | No          | No      | Fastest  | Pre-normalized
Manhattan          | No          | Yes     | Fast     | Sparse vectors

*Cosine distance (1-cos) forms a pseudo-metric
\end{lstlisting}

\subsubsection{How Embeddings Are Learned}

\paragraph{Skip-gram (Word2Vec) - The Original Insight}

\textbf{Objective}: Predict context words from center word

Given sentence: "The quick brown fox jumps"
\begin{itemize}[leftmargin=*]
    \item Center: "brown"
    \item Context: ["quick", "fox"]
\end{itemize}

\textbf{Neural Network}:
\begin{lstlisting}
Input: one-hot vector for "brown" [0,0,1,0,0,...]
       $\downarrow$
Hidden Layer (embedding): [0.2, -0.5, 0.8, ...] <- This is the embedding!
       $\downarrow$
Output: probability distribution over context words
\end{lstlisting}

\textbf{Training}: Adjust embeddings so that words appearing in similar contexts get similar vectors.

\textbf{Mathematical Formulation}:

Maximize: $\Sigma$ log P(context | word)

Where: P(context | word) = exp(u\_context $\cdot$ v\_word) / $\Sigma$ exp(u\_i $\cdot$ v\_word)

This is \textbf{softmax} - converts dot products into probabilities.

\paragraph{Skip-gram vs. CBOW (Continuous Bag of Words)}

\textbf{CBOW}: Inverse of skip-gram - predict center word from context

\begin{lstlisting}
Skip-gram:  center -> context words
CBOW:       context words -> center

Training Signal:
Skip-gram:  "The quick [brown] fox jumps" -> predict "quick", "fox"
CBOW:       "The quick [?] fox jumps" -> predict "brown" from context
\end{lstlisting}

\textbf{When to use each}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Skip-gram}: Better for small datasets, rare words, captures more nuanced semantics
    \item \textbf{CBOW}: Faster training, better for frequent words, smoother embeddings
\end{itemize}

\textbf{Mathematical Difference}:
\begin{lstlisting}
Skip-gram: P(context | center) = $\Pi$$_i$ P(w$_i$ | w_center)
CBOW:      P(center | context) = P(w_center | avg(context_words))
\end{lstlisting}

\subsubsection{Properties of Good Embeddings}

\textbf{Linearity}: Semantic relationships are linear transformations
\begin{lstlisting}
king - man + woman $\approx$ queen
Paris - France + Italy $\approx$ Rome
\end{lstlisting}

\textbf{Clustering}: Similar concepts cluster together
\begin{lstlisting}
Fruits: [apple, orange, banana] are close in space
Animals: [dog, cat, lion] form another cluster
\end{lstlisting}

\textbf{Dimensionality}: Each dimension captures a semantic feature
\begin{itemize}[leftmargin=*]
    \item Dimension 42 might encode "royalty"
    \item Dimension 108 might encode "gender"
    \item (Though in practice, dimensions are entangled)
\end{itemize}

\subsection{Practical Implications for RAG}

\textbf{Why this matters for your RAG system}:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Chunk Size}: Larger chunks $\rightarrow$ more diverse content $\rightarrow$ lower quality embeddings
    \begin{itemize}
        \item Sweet spot: 200-1000 tokens per chunk
        \item Each chunk should have coherent semantic content
    \end{itemize}

    \item \textbf{Query Expansion}: If query is short, expand it before embedding
    \begin{itemize}
        \item Short query: "RAG" $\rightarrow$ poor embedding
        \item Expanded: "What is retrieval-augmented generation?" $\rightarrow$ better
    \end{itemize}

    \item \textbf{Embedding Model Choice}:
    \begin{itemize}
        \item General models (OpenAI): Good for diverse content
        \item Domain-specific: Train on your corpus for 10-20\% improvement
    \end{itemize}

    \item \textbf{Similarity Threshold}: Not all cosine scores are created equal
    \begin{itemize}
        \item 0.9+ : Very similar (same topic, same phrasing)
        \item 0.7-0.9 : Similar (same topic, different phrasing)
        \item 0.5-0.7 : Related (adjacent topics)
        \item <0.5 : Probably not relevant
    \end{itemize}
\end{enumerate}

\section{Information Retrieval Theory (The Science of Finding)}

\subsection{What is Information Retrieval?}

\textbf{Formal Definition}: Given a query Q and document collection D, find documents D' $\subset$ D that are relevant to Q.

\textbf{Core Challenge}: "Relevance" is subjective and context-dependent.

\subsection{Classic IR: TF-IDF (Term Frequency-Inverse Document Frequency)}

\textbf{The Intuition}:
\begin{itemize}[leftmargin=*]
    \item Words that appear often in a document are important for that document (TF)
    \item But words that appear in all documents are less discriminative (IDF)
\end{itemize}

\textbf{Mathematics}:

\textbf{Term Frequency} (how often term t appears in document d):
\begin{lstlisting}
TF(t,d) = count(t,d) / |d|
\end{lstlisting}

\textbf{Inverse Document Frequency} (how rare is term t):
\begin{lstlisting}
IDF(t) = log(N / df(t))

where:
- N = total documents
- df(t) = documents containing term t
\end{lstlisting}

\textbf{TF-IDF Score}:
\begin{lstlisting}
TF-IDF(t,d) = TF(t,d) $\times$ IDF(t)
\end{lstlisting}

\textbf{Example}:

Document: "The cat sat on the mat"
Query: "cat"

\begin{lstlisting}
TF("cat") = 1/6 = 0.167
IDF("cat") = log(1000/50) = 2.996  (if 50 docs out of 1000 mention "cat")
TF-IDF = 0.167 $\times$ 2.996 = 0.500

vs.

TF("the") = 2/6 = 0.333
IDF("the") = log(1000/999) = 0.001  ("the" is in almost every document)
TF-IDF = 0.333 $\times$ 0.001 = 0.0003
\end{lstlisting}

\textbf{Insight}: Common words get downweighted automatically!

\subsection{BM25: The King of Lexical Retrieval}

\textbf{BM25} (Best Matching 25) improves on TF-IDF with \textbf{diminishing returns} and \textbf{length normalization}.

\textbf{The Formula} (don't memorize, understand the components):

\begin{lstlisting}
BM25(Q,D) = $\Sigma$ IDF(q$_i$) $\cdot$ (f(q$_i$,D) $\cdot$ (k$_1$ + 1)) / (f(q$_i$,D) + k$_1$ $\cdot$ (1 - b + b $\cdot$ |D|/avgdl))
            q$_i$$\in$Q

where:
- f(q$_i$,D) = frequency of term q$_i$ in document D
- |D| = length of document D
- avgdl = average document length
- k$_1$ = term frequency saturation (usually 1.2-2.0)
- b = length normalization (usually 0.75)
\end{lstlisting}

\textbf{What each part does}:

\begin{enumerate}[leftmargin=*]
    \item \textbf{IDF(q$_i$)}: Rare terms are more important (like TF-IDF)

    \item \textbf{Saturation}: `f/(f + k$_1$)` approaches 1 as f increases
    \begin{itemize}
        \item Mentioning "cat" 100 times doesn't make doc 100$\times$ more relevant
        \item Diminishing returns built in!
    \end{itemize}

    \item \textbf{Length Normalization}: `(1 - b + b $\cdot$ |D|/avgdl)`
    \begin{itemize}
        \item Longer documents naturally have higher term frequencies
        \item This penalty prevents bias toward long docs
    \end{itemize}
\end{enumerate}

\textbf{Visual Intuition}:
\begin{lstlisting}
TF-IDF: Score grows linearly with term frequency
        |        /
Score   |      /
        |    /
        |  /
        |/___________
          Term Freq

BM25: Score saturates (diminishing returns)
        |     ____
Score   |   /
        | /
        |/___________
          Term Freq
\end{lstlisting}

\textbf{Why BM25 is still used in 2025}:

Despite neural retrieval, BM25 excels at:
\begin{itemize}[leftmargin=*]
    \item \textbf{Exact matches}: "invoice \#12345"
    \item \textbf{Rare terms}: Technical jargon, product IDs
    \item \textbf{Speed}: No GPU needed
    \item \textbf{Interpretability}: You can see which terms matched
\end{itemize}

\subsection{Hybrid Retrieval: Best of Both Worlds}

\textbf{The Insight}: BM25 and neural retrieval are complementary.

\textbf{Reciprocal Rank Fusion (RRF)}: Simple but effective

\begin{lstlisting}
Given two ranked lists: BM25 results and Dense results

Score(doc) = 1/(k + rank_BM25(doc)) + 1/(k + rank_dense(doc))

where k = 60 (empirically chosen constant)
\end{lstlisting}

\textbf{Example}:
\begin{lstlisting}
BM25 ranks:    [doc1, doc3, doc2, doc5]
Dense ranks:   [doc2, doc1, doc4, doc3]

RRF scores:
doc1: 1/61 + 1/62 = 0.0328
doc2: 1/63 + 1/61 = 0.0322
doc3: 1/62 + 1/64 = 0.0318
...

Final ranking: [doc1, doc2, doc3, ...]
\end{lstlisting}

\subsection{Relevance and Precision-Recall}

\textbf{Fundamental Tradeoff}: You can't have perfect precision and perfect recall simultaneously.

\textbf{Definitions}:
\begin{lstlisting}
Precision = Relevant Retrieved / Total Retrieved
Recall = Relevant Retrieved / Total Relevant

F1 Score = 2 $\cdot$ (Precision $\cdot$ Recall) / (Precision + Recall)
\end{lstlisting}

\textbf{Practical Implications for RAG}:

\begin{itemize}[leftmargin=*]
    \item \textbf{Top-k = 5}: High precision, might miss relevant info
    \item \textbf{Top-k = 50}: Better recall, but more noise for LLM
    \item \textbf{Sweet spot}: 10-20 documents for most use cases
\end{itemize}

\textbf{Retrieval @ k}: Metric for RAG systems
\begin{lstlisting}
Recall@5 = "What fraction of relevant docs are in top 5?"
\end{lstlisting}

\section{Graph Theory Fundamentals (The Mathematics of Relationships)}

\subsection{What is a Graph? (Formally)}

\textbf{Definition}: A graph G = (V, E) consists of:
\begin{itemize}[leftmargin=*]
    \item \textbf{V}: Set of vertices (nodes)
    \item \textbf{E}: Set of edges (relationships)
\end{itemize}

\textbf{Types of Graphs}:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Directed Graph} (Digraph):
    \begin{itemize}
        \item Edges have direction: A $\rightarrow$ B $\neq$ B $\rightarrow$ A
        \item Example: "Alice follows Bob" (Twitter)
    \end{itemize}

    \item \textbf{Undirected Graph}:
    \begin{itemize}
        \item Edges are bidirectional: A --- B = B --- A
        \item Example: "Alice is friends with Bob" (Facebook)
    \end{itemize}

    \item \textbf{Weighted Graph}:
    \begin{itemize}
        \item Edges have weights: A -(5)$\rightarrow$ B
        \item Example: Road network (weights = distance)
    \end{itemize}

    \item \textbf{Property Graph} (Knowledge Graphs):
    \begin{itemize}
        \item Nodes and edges have properties
        \item Example: (Person \{name:"Alice"\}) -[KNOWS \{since:2020\}]$\rightarrow$ (Person \{name:"Bob"\})
    \end{itemize}
\end{enumerate}

\subsection{Graph Representation}

\subsubsection{Adjacency Matrix}

For graph with n nodes:
\begin{lstlisting}
     A  B  C  D
A  [ 0  1  1  0 ]
B  [ 0  0  1  1 ]
C  [ 0  0  0  1 ]
D  [ 0  0  0  0 ]

1 = edge exists, 0 = no edge
\end{lstlisting}

\textbf{Space}: O(n$^2$)\\
\textbf{Edge lookup}: O(1)\\
\textbf{Best for}: Dense graphs (many edges)

\subsubsection{Adjacency List}

\begin{lstlisting}
A -> [B, C]
B -> [C, D]
C -> [D]
D -> []
\end{lstlisting}

\textbf{Space}: O(n + e) where e = number of edges\\
\textbf{Edge lookup}: O(degree)\\
\textbf{Best for}: Sparse graphs (few edges) $\leftarrow$ Most real graphs!

\begin{aside}
If you're implementing a knowledge graph from scratch and considering an adjacency matrix because "O(1) lookup is faster," stop. Most real graphs are sparse - a typical person knows hundreds of people, not millions. An adjacency matrix for 1M nodes takes 1TB of RAM just to store zeros. Use adjacency lists. This is one of those textbook-vs-reality moments where the "slower" algorithm is actually faster in practice.
\end{aside}

\subsection{Graph Properties}

\subsubsection{Degree}

\textbf{In-degree}: Number of incoming edges\\
\textbf{Out-degree}: Number of outgoing edges

\textbf{Example}: Twitter
\begin{itemize}[leftmargin=*]
    \item High in-degree = celebrity (many followers)
    \item High out-degree = active user (follows many)
\end{itemize}

\subsubsection{Path}

\textbf{Path}: Sequence of vertices connected by edges
\begin{itemize}[leftmargin=*]
    \item A $\rightarrow$ B $\rightarrow$ C is a path of length 2
\end{itemize}

\textbf{Shortest Path}: Minimum number of edges between two nodes
\begin{itemize}[leftmargin=*]
    \item Dijkstra's algorithm (weighted)
    \item BFS (unweighted)
\end{itemize}

\textbf{Why this matters for KG}:
\begin{itemize}[leftmargin=*]
    \item "How is Alice connected to Machine Learning?"
    \item Find shortest path: Alice $\rightarrow$ WORKS\_ON $\rightarrow$ Project X $\rightarrow$ REQUIRES $\rightarrow$ Machine Learning
\end{itemize}

\subsubsection{Connectedness}

\textbf{Connected Graph}: Path exists between any two nodes

\textbf{Components}: Maximal connected subgraphs
\begin{lstlisting}
Graph:    AB    CDE
                
          F      G

Components: {A,B,F}, {C,D,E,G}
\end{lstlisting}

\textbf{In KG}: Disconnected components might indicate:
\begin{itemize}[leftmargin=*]
    \item Different knowledge domains
    \item Data quality issues (missing links)
\end{itemize}

\subsubsection{Cycles}

\textbf{Cycle}: Path that starts and ends at same node
\begin{lstlisting}
A -> B -> C -> A  (cycle of length 3)
\end{lstlisting}

\textbf{DAG} (Directed Acyclic Graph): No cycles
\begin{itemize}[leftmargin=*]
    \item Used for: Ontologies, dependency graphs
    \item Example: File $\rightarrow$ Directory $\rightarrow$ Filesystem (no circular dependencies)
\end{itemize}

\textbf{Cyclic Graphs}: Allow cycles
\begin{itemize}[leftmargin=*]
    \item Used for: Social networks, knowledge graphs
    \item Example: A knows B, B knows C, C knows A
\end{itemize}

\subsection{Graph Algorithms for Knowledge Graphs}

\subsubsection{Breadth-First Search (BFS)}

\textbf{Use Case}: Find shortest path, neighborhood exploration

\textbf{Algorithm}:
\begin{lstlisting}
BFS(start_node):
    queue = [start_node]
    visited = {start_node}

    while queue not empty:
        node = queue.pop()
        for neighbor in node.neighbors:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)
\end{lstlisting}

\textbf{Complexity}: O(V + E)

\textbf{KG Application}: "Find all skills within 2 hops of Alice"

\subsubsection{PageRank (The Google Algorithm)}

\textbf{Intuition}: A node is important if important nodes point to it.

\textbf{Mathematical Formulation}:
\begin{lstlisting}
PR(A) = (1-d)/N + d $\cdot$ $\Sigma$ PR(T$_i$)/C(T$_i$)
                      i

where:
- d = damping factor (usually 0.85)
- N = total nodes
- T$_i$ = nodes pointing to A
- C(T$_i$) = out-degree of T$_i$
\end{lstlisting}

\textbf{Iterative Computation}:
\begin{lstlisting}
Initialize: PR(node) = 1/N for all nodes
Repeat until convergence:
    For each node A:
        PR_new(A) = (1-d)/N + d $\cdot$ $\Sigma$ PR_old(T$_i$)/C(T$_i$)
\end{lstlisting}

\textbf{KG Application}: "Find the most influential people in the organization"

\textbf{Example}:
\begin{lstlisting}
Graph:   A <- B
         $\downarrow$   $\downarrow$
         C <- D

After convergence:
PR(C) > PR(D) > PR(A) > PR(B)

C is most important (receives links from important nodes A and D)
\end{lstlisting}

% End of Section 2A
\clearpage

% ============================================================================
% SECTION 3: RAG ENGINEERING MODULE
% ============================================================================

\chapter{RAG ENGINEERING MODULE}

\begin{aside}
Welcome to the practical section. Everything before this was foundation. Everything here is production-grade engineering. The examples look simple - they're not. Each design decision has failure modes that won't appear until you hit production traffic. We'll point them out as we go.
\end{aside}

\section{What is RAG (Retrieval-Augmented Generation)?}

\textbf{Problem RAG Solves}:
\begin{itemize}[leftmargin=*]
    \item LLMs have knowledge cutoff dates (trained on old data)
    \item LLMs hallucinate (make up facts confidently)
    \item LLMs can't access private/proprietary data
    \item LLMs have token limits (can't process entire databases)
\end{itemize}

\textbf{Solution}: Retrieve relevant information $\rightarrow$ Feed to LLM $\rightarrow$ Generate grounded answers

\subsection{RAG Pipeline (Basic)}

\begin{lstlisting}
User Query
    $\downarrow$
[1] Query Processing (rewrite, expand)
    $\downarrow$
[2] Retrieval (search documents)
    $\downarrow$
[3] Context Construction (format retrieved docs)
    $\downarrow$
[4] LLM Generation (answer with context)
    $\downarrow$
Answer
\end{lstlisting}

\subsection{Concrete Example}

\textbf{Without RAG}:
\begin{lstlisting}
User: "What was our Q4 2024 revenue?"
LLM: "I don't have access to real-time data..."
\end{lstlisting}

\textbf{With RAG}:
\begin{lstlisting}
User: "What was our Q4 2024 revenue?"
    $\downarrow$
Retrieval: Find "Q4_2024_earnings.pdf"
    $\downarrow$
Context: "Q4 2024 revenue: $5.2M, up 23% YoY..."
    $\downarrow$
LLM: "According to the Q4 2024 earnings report, revenue was $5.2M,
     representing a 23% increase year-over-year."
\end{lstlisting}

\section{Chunking Strategies}

\subsection{Why Chunking Matters}

\textbf{Problem}: Documents are too long for:
\begin{itemize}[leftmargin=*]
    \item Embedding models (token limits: 512-8192)
    \item LLM context windows (need concise relevant chunks, not entire PDFs)
    \item Retrieval accuracy (large chunks = mixed topics = poor similarity scores)
\end{itemize}

\begin{aside}
Chunking is the most underestimated part of RAG. Everyone obsesses over embeddings and rerankers, but bad chunking will tank your system no matter how sophisticated everything else is. A chunk that cuts off mid-sentence? The LLM gets confused. A chunk that spans three different topics? Your similarity scores are garbage. Get this right first, optimize everything else later.
\end{aside}

\subsection{Chunking Methods}

\subsubsection{Fixed-Size Chunking}

\textbf{Method}: Split every N characters/tokens with overlap.

\begin{lstlisting}[style=python]
def fixed_size_chunking(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap  # Overlap prevents cutting sentences
    return chunks

document = "Long document text..." * 1000
chunks = fixed_size_chunking(document, chunk_size=500, overlap=50)
\end{lstlisting}

\textbf{Pros}: Simple, predictable size\\
\textbf{Cons}: Breaks mid-sentence, ignores document structure

\subsubsection{Sentence-Based Chunking}

\begin{lstlisting}[style=python]
import nltk
nltk.download('punkt')

def sentence_chunking(text, sentences_per_chunk=5):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    for i in range(0, len(sentences), sentences_per_chunk):
        chunk = " ".join(sentences[i:i+sentences_per_chunk])
        chunks.append(chunk)
    return chunks
\end{lstlisting}

\textbf{Pros}: Preserves sentence boundaries\\
\textbf{Cons}: Variable chunk sizes

\subsubsection{Semantic Chunking (Advanced)}

\begin{lstlisting}[style=python]
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Splits on paragraph, then sentence, then word boundaries
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)

chunks = splitter.split_text(document)
\end{lstlisting}

\textbf{Pros}: Respects document structure\\
\textbf{Cons}: More complex

\subsubsection{Structural Chunking (Best for Many Use Cases)}

\textbf{Method}: Split by document structure (headers, sections, paragraphs).

\begin{lstlisting}[style=python]
def structural_chunking(markdown_text):
    chunks = []
    current_chunk = ""
    current_header = ""

    for line in markdown_text.split('\n'):
        if line.startswith('#'):  # Header
            if current_chunk:
                chunks.append({
                    'content': current_chunk,
                    'header': current_header
                })
            current_header = line
            current_chunk = line + '\n'
        else:
            current_chunk += line + '\n'

    if current_chunk:
        chunks.append({'content': current_chunk, 'header': current_header})

    return chunks
\end{lstlisting}

\textbf{Pros}: Maintains semantic coherence\\
\textbf{Cons}: Requires structured documents

\begin{aside}
At this point you're probably thinking "which chunking method should I use?" Here's the truth: for 80\% of use cases, fixed-size with overlap (500-1000 tokens, 10-20\% overlap) works fine. Don't overcomplicate it. Try the simple thing first. The advanced strategies below are for when the simple thing fails - and you'll know it has failed because your retrieval quality will be obviously bad.
\end{aside}

\subsubsection{Advanced Chunking Strategies}

\paragraph{Semantic Similarity-Based Chunking}

Instead of fixed boundaries, split based on semantic coherence:

\begin{lstlisting}[style=python]
from sentence_transformers import SentenceTransformer
import numpy as np

def semantic_chunking(text, similarity_threshold=0.7):
    """
    Split text when semantic similarity between consecutive sentences drops
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')
    sentences = nltk.sent_tokenize(text)

    # Embed all sentences
    embeddings = model.encode(sentences)

    # Compute consecutive similarities
    chunks = []
    current_chunk = [sentences[0]]

    for i in range(1, len(sentences)):
        similarity = cosine_similarity(
            embeddings[i-1].reshape(1, -1),
            embeddings[i].reshape(1, -1)
        )[0][0]

        if similarity < similarity_threshold:
            # Topic changed, start new chunk
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentences[i]]
        else:
            current_chunk.append(sentences[i])

    # Add final chunk
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks
\end{lstlisting}

\textbf{Why This Works}:
\begin{itemize}[leftmargin=*]
    \item Automatically detects topic boundaries
    \item No manual tuning of chunk size
    \item Preserves semantic coherence
\end{itemize}

\textbf{Trade-offs}:
\begin{itemize}[leftmargin=*]
    \item Computationally expensive (need to embed every sentence)
    \item Variable chunk sizes can be problematic for some systems
    \item Best for: Long documents with clear topic transitions (e.g., research papers, reports)
\end{itemize}

\paragraph{Sliding Window Chunking with Context}

\begin{lstlisting}[style=python]
def sliding_window_with_context(text, window_size=500, stride=400, context_size=100):
    """
    Create overlapping chunks where each chunk includes context from previous/next
    """
    chunks = []
    metadata = []

    start = 0
    while start < len(text):
        # Main content
        end = min(start + window_size, len(text))
        chunk_text = text[start:end]

        # Add context from before
        context_before = text[max(0, start - context_size):start]

        # Add context after
        context_after = text[end:min(end + context_size, len(text))]

        # Store main chunk with metadata about context
        chunks.append({
            'main_content': chunk_text,
            'context_before': context_before,
            'context_after': context_after,
            'full_chunk': context_before + chunk_text + context_after,
            'position': (start, end)
        })

        start += stride

    return chunks

# Usage for RAG
chunks = sliding_window_with_context(document)
# Embed 'full_chunk' for better context understanding
# But retrieve 'main_content' to avoid duplication
\end{lstlisting}

\textbf{Benefits}:
\begin{itemize}[leftmargin=*]
    \item Prevents information loss at boundaries
    \item Each chunk has context for better embedding quality
    \item Answers questions that span chunk boundaries
\end{itemize}

\paragraph{Hierarchical Chunking (Multi-Level)}

\begin{lstlisting}[style=python]
def hierarchical_chunking(text):
    """
    Create chunks at multiple granularities: document -> section -> paragraph -> sentence
    """
    # Level 1: Full document summary
    doc_summary = {
        'level': 'document',
        'content': text[:1000],  # First 1000 chars as summary
        'metadata': {'type': 'overview'}
    }

    # Level 2: Sections (by headers)
    sections = text.split('\n\n')  # Simplified
    section_chunks = []

    for i, section in enumerate(sections):
        if len(section) > 100:  # Skip tiny sections
            section_chunks.append({
                'level': 'section',
                'content': section,
                'metadata': {
                    'section_id': i,
                    'parent': 'document'
                }
            })

            # Level 3: Paragraphs within section
            paragraphs = section.split('\n')
            for j, para in enumerate(paragraphs):
                if len(para) > 50:
                    section_chunks.append({
                        'level': 'paragraph',
                        'content': para,
                        'metadata': {
                            'paragraph_id': j,
                            'parent_section': i,
                            'parent': 'section'
                        }
                    })

    return [doc_summary] + section_chunks
\end{lstlisting}

\textbf{Retrieval Strategy}:
\begin{lstlisting}[style=python]
# Query routing based on question type
if is_broad_question(query):
    # Retrieve from section-level chunks
    results = retrieve(query, level='section')
elif is_specific_question(query):
    # Retrieve from paragraph-level chunks
    results = retrieve(query, level='paragraph')
else:
    # Hybrid: retrieve from multiple levels
    results = retrieve(query, level='all')
\end{lstlisting}

\textbf{When to Use}:
\begin{itemize}[leftmargin=*]
    \item Complex documents with clear hierarchical structure (research papers, legal docs, manuals)
    \item Need to answer both broad and specific questions
    \item Want to provide varying levels of detail
\end{itemize}

\paragraph{Entity-Aware Chunking}

\begin{lstlisting}[style=python]
import spacy

def entity_aware_chunking(text, max_chunk_size=500):
    """
    Never split named entities across chunks
    """
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)

    chunks = []
    current_chunk = []
    current_size = 0

    for sent in doc.sents:
        sent_text = sent.text
        sent_size = len(sent_text)

        # Check if adding this sentence would exceed limit
        if current_size + sent_size > max_chunk_size and current_chunk:
            # Check if we're in the middle of an entity
            last_token = list(doc.sents)[len(current_chunk)-1][-1]

            if last_token.ent_type_:  # In middle of entity
                # Continue chunk to include complete entity
                current_chunk.append(sent_text)
                current_size += sent_size
            else:
                # Safe to split
                chunks.append(" ".join(current_chunk))
                current_chunk = [sent_text]
                current_size = sent_size
        else:
            current_chunk.append(sent_text)
            current_size += sent_size

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks
\end{lstlisting}

\textbf{Why This Matters}:
\begin{itemize}[leftmargin=*]
    \item Prevents splitting "New York City" across chunks
    \item Maintains entity context for better retrieval
    \item Improves answer quality for entity-centric questions
\end{itemize}

\subsection{Chunking Best Practices}

\begin{longtable}{p{0.2\textwidth} p{0.25\textwidth} p{0.15\textwidth} p{0.3\textwidth}}
\toprule
\textbf{Document Type} & \textbf{Strategy} & \textbf{Chunk Size} & \textbf{Considerations} \\
\midrule
\endfirsthead
\toprule
\textbf{Document Type} & \textbf{Strategy} & \textbf{Chunk Size} & \textbf{Considerations} \\
\midrule
\endhead
Technical docs & Structural (by headers) & 500-1000 & Preserve code blocks, tables \\
Legal contracts & Sentence + Entity-aware & 300-500 & Never split entities, clauses \\
News articles & Paragraph/Semantic & 200-400 & Preserve quotes, citations \\
Code docs & Function/class based & Varies & Keep signatures with bodies \\
Chat logs & Fixed with overlap & 100-200 & Maintain conversation context \\
Research papers & Hierarchical & 800-1200 & Preserve sections, citations \\
E-commerce & Per-product entity & 100-300 & One product per chunk \\
Financial reports & Table-aware + Structural & 400-800 & Keep tables intact \\
\bottomrule
\end{longtable}

\subsection{The Science of Chunk Size Selection}

\textbf{Too Small} (< 100 tokens):
\begin{itemize}[leftmargin=*]
    \item  Lacks sufficient context
    \item  Poor embedding quality (not enough signal)
    \item  High retrieval cost (need more chunks to cover topic)
    \item  Fragments coherent ideas
\end{itemize}

\textbf{Too Large} (> 1500 tokens):
\begin{itemize}[leftmargin=*]
    \item  Mixed topics $\rightarrow$ poor similarity scores
    \item  Exceeds embedding model limits
    \item  LLM gets irrelevant information
    \item  Slower processing
\end{itemize}

\textbf{Sweet Spot} (200-800 tokens):
\begin{itemize}[leftmargin=*]
    \item  Complete semantic units
    \item  Good embedding quality
    \item  Manageable for LLM context
    \item  Efficient retrieval
\end{itemize}

\begin{aside}
Yes, there's actual math here. No, you don't need to optimize to the last token. Start with 500 tokens and 10\% overlap. If your retrieval sucks, adjust. If it works, stop optimizing and ship your product.
\end{aside}

\textbf{Mathematical Analysis}:

\begin{lstlisting}
Optimal chunk size C* minimizes:
L(C) = $\alpha$$\cdot$Fragmentation(C) + $\beta$$\cdot$Noise(C) + $\gamma$$\cdot$Cost(C)

where:
- Fragmentation(C) = E[incomplete_concepts | chunk_size=C]
- Noise(C) = E[irrelevant_content | chunk_size=C]
- Cost(C) = computational_cost(C)

Empirically:
C* $\approx$ 500 tokens for most domains
\end{lstlisting}

\section{Embeddings Selection}

\begin{aside}
The embedding model you choose matters less than you think. A decent embedding model with good chunking beats a perfect embedding model with bad chunking every time. Don't spend weeks benchmarking models. Pick a reasonable one and focus on your data quality.
\end{aside}

\subsection{Embedding Model Comparison}

\begin{longtable}{p{0.25\textwidth} p{0.15\textwidth} p{0.15\textwidth} p{0.1\textwidth} p{0.25\textwidth}}
\toprule
\textbf{Model} & \textbf{Dimensions} & \textbf{Max Tokens} & \textbf{Speed} & \textbf{Use Case} \\
\midrule
\endfirsthead
\toprule
\textbf{Model} & \textbf{Dimensions} & \textbf{Max Tokens} & \textbf{Speed} & \textbf{Use Case} \\
\midrule
\endhead
text-embedding-3-small & 1536 & 8191 & Fast & General purpose, cost-effective \\
text-embedding-3-large & 3072 & 8191 & Medium & High accuracy needed \\
text-embedding-ada-002 & 1536 & 8191 & Fast & Legacy, still good \\
BGE-large & 1024 & 512 & Fast & Open-source, self-hosted \\
E5-mistral & 4096 & 512 & Slow & Highest quality \\
\bottomrule
\end{longtable}

\subsection{Choosing the Right Model}

\begin{lstlisting}[style=python]
from openai import OpenAI

client = OpenAI()

# For most cases: text-embedding-3-small
def embed_text(text):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# For domain-specific (legal, medical): Fine-tune or use specialized models
# Example with sentence-transformers (open-source)
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-large-en-v1.5')
embedding = model.encode("Your text here")
\end{lstlisting}

\subsection{Embedding Best Practices}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Consistency}: Use same model for indexing and querying
    \item \textbf{Normalization}: Normalize embeddings for cosine similarity
    \item \textbf{Metadata}: Store model version with embeddings
    \item \textbf{Batch Processing}: Embed in batches for efficiency
\end{enumerate}

\begin{lstlisting}[style=python]
# Efficient batch embedding
def batch_embed(texts, batch_size=100):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        embeddings.extend([item.embedding for item in response.data])
    return embeddings
\end{lstlisting}

\section{Indexing \& Vector Stores}

\subsection{FAISS (Facebook AI Similarity Search)}

\textbf{Use Case}: Local development, millions of vectors, no server needed

\begin{lstlisting}[style=python]
import faiss
import numpy as np

# Create index
dimension = 1536  # text-embedding-3-small dimension
index = faiss.IndexFlatL2(dimension)  # L2 distance

# Add embeddings
embeddings_array = np.array(embeddings).astype('float32')
index.add(embeddings_array)

# Search
query_embedding = np.array([get_embedding(query)]).astype('float32')
k = 5  # Top 5 results
distances, indices = index.search(query_embedding, k)

# indices[0] contains IDs of top 5 similar chunks
\end{lstlisting}

\textbf{Advanced FAISS} (for scale):
\begin{lstlisting}[style=python]
# IVF index for faster search (100M+ vectors)
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, 100)  # 100 clusters

# Train index (required for IVF)
index.train(embeddings_array)
index.add(embeddings_array)
\end{lstlisting}

\subsection{ChromaDB}

\textbf{Use Case}: Simple prototyping, built-in embedding, metadata filtering

\begin{lstlisting}[style=python]
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.create_collection(
    name="my_documents",
    metadata={"description": "Company knowledge base"}
)

# Add documents (auto-embedding)
collection.add(
    documents=["RAG is powerful", "Knowledge graphs structure data"],
    metadatas=[{"source": "blog", "date": "2024-01"},
               {"source": "paper", "date": "2024-02"}],
    ids=["doc1", "doc2"]
)

# Query with metadata filter
results = collection.query(
    query_texts=["what is RAG?"],
    n_results=5,
    where={"source": "blog"}  # Metadata filter
)
\end{lstlisting}

\subsection{Pinecone (Production)}

\textbf{Use Case}: Production deployment, auto-scaling, managed service

\begin{lstlisting}[style=python]
import pinecone

# Initialize
pinecone.init(api_key="your-api-key", environment="us-west1-gcp")
index = pinecone.Index("knowledge-base")

# Upsert vectors
index.upsert(vectors=[
    ("doc1", embedding1, {"text": "...", "source": "..."}),
    ("doc2", embedding2, {"text": "...", "source": "..."})
])

# Query
results = index.query(
    vector=query_embedding,
    top_k=5,
    include_metadata=True,
    filter={"source": {"$eq": "blog"}}
)
\end{lstlisting}

\section{Retrievers (BM25, Hybrid, Dense)}

\subsection{BM25 Retriever}

\begin{lstlisting}[style=python]
from rank_bm25 import BM25Okapi

class BM25Retriever:
    def __init__(self, documents):
        self.documents = documents
        tokenized = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)

    def retrieve(self, query, top_k=5):
        scores = self.bm25.get_scores(query.split())
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return [self.documents[i] for i in top_indices]
\end{lstlisting}

\subsection{Dense Retriever (Semantic)}

\begin{lstlisting}[style=python]
class DenseRetriever:
    def __init__(self, documents, embeddings):
        self.documents = documents
        self.index = faiss.IndexFlatL2(len(embeddings[0]))
        self.index.add(np.array(embeddings).astype('float32'))

    def retrieve(self, query, top_k=5):
        query_emb = get_embedding(query)
        distances, indices = self.index.search(
            np.array([query_emb]).astype('float32'), top_k
        )
        return [self.documents[i] for i in indices[0]]
\end{lstlisting}

\subsection{Hybrid Retriever (Best of Both)}

\begin{lstlisting}[style=python]
class HybridRetriever:
    def __init__(self, documents, embeddings):
        self.bm25 = BM25Retriever(documents)
        self.dense = DenseRetriever(documents, embeddings)
        self.documents = documents

    def retrieve(self, query, top_k=5, alpha=0.7):
        # Get candidates from both
        bm25_docs = self.bm25.retrieve(query, top_k=20)
        dense_docs = self.dense.retrieve(query, top_k=20)

        # Score combination (simplified)
        scores = {}
        for doc in bm25_docs:
            scores[doc] = scores.get(doc, 0) + (1 - alpha)
        for doc in dense_docs:
            scores[doc] = scores.get(doc, 0) + alpha

        # Return top-k
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [doc for doc, score in sorted_docs[:top_k]]
\end{lstlisting}

\textbf{Why Hybrid?}
\begin{itemize}
    \item BM25 catches exact term matches ("Product ID: ABC123")
    \item Dense catches semantic similarity ("car" $\rightarrow$ "automobile")
    \item Together: Best recall
\end{itemize}

\section{3.6 Rerankers (Cross-Encoders)}

\subsection{Why Reranking?}

\textbf{Problem}: Initial retrieval optimizes for speed (approximate search). Reranking adds precision.

\begin{aside}
Reranking feels like overkill when you're prototyping. It's not. Your initial retrieval will return garbage in the top 5 about 30\% of the time. Reranking fixes that. Skip it if you want, but don't be surprised when your users complain that the chatbot gives them irrelevant answers.
\end{aside}

\textbf{Pipeline}:
\begin{verbatim}
Query $\rightarrow$ Retrieve 100 candidates $\rightarrow$ Rerank to top 5 $\rightarrow$ Pass to LLM
\end{verbatim}

\subsection{Cross-Encoder Reranking}

\begin{lstlisting}[style=python]
from sentence_transformers import CrossEncoder

# Load reranker model
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank(query, documents, top_k=5):
    # Create query-document pairs
    pairs = [[query, doc] for doc in documents]

    # Score all pairs
    scores = reranker.predict(pairs)

    # Sort and return top-k
    sorted_indices = np.argsort(scores)[-top_k:][::-1]
    return [documents[i] for i in sorted_indices]

# Usage
initial_results = retriever.retrieve(query, top_k=100)
final_results = rerank(query, initial_results, top_k=5)
\end{lstlisting}

\subsection{Before/After Reranking}

\textbf{Before (Just Dense Retrieval)}:
\begin{verbatim}
Query: "How do I reset my password?"
Results:
1. "Password security best practices..." (semantic match, but wrong)
2. "Creating strong passwords..." (semantic match, but wrong)
3. "To reset your password, click..." (correct, but ranked 3rd)
\end{verbatim}

\textbf{After (With Reranker)}:
\begin{verbatim}
Results:
1. "To reset your password, click..." 
2. "Password reset troubleshooting..." 
3. "Password security best practices..."
\end{verbatim}

\section{3.7 Query Rewriting \& Decomposition}

\subsection{Query Rewriting}

\textbf{Goal}: Transform user query into better retrieval queries.

\begin{lstlisting}[style=python]
def rewrite_query(user_query):
    prompt = f"""
    Rewrite this user query to be more effective for document retrieval.
    Make it clearer and add important keywords.

    Original query: {user_query}

    Rewritten query:
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return response.choices[0].message.content

# Example
user_query = "How do I fix it?"
rewritten = rewrite_query(user_query)
# Result: "How to troubleshoot and fix common system errors"
\end{lstlisting}

\subsection{Query Decomposition (Multi-Step Queries)}

\textbf{Use Case}: Complex questions requiring multiple retrievals

\begin{lstlisting}[style=python]
def decompose_query(complex_query):
    prompt = f"""
    Break this complex question into simpler sub-questions:

    Question: {complex_query}

    Sub-questions (as JSON list):
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return eval(response.choices[0].message.content)

# Example
query = "Compare the performance of GPT-4 and Claude on coding tasks"
sub_queries = decompose_query(query)
# Result: [
#   "What is the performance of GPT-4 on coding tasks?",
#   "What is the performance of Claude on coding tasks?",
#   "How do GPT-4 and Claude compare overall?"
# ]

# Retrieve for each sub-query
all_docs = []
for sq in sub_queries:
    docs = retriever.retrieve(sq)
    all_docs.extend(docs)
\end{lstlisting}

\section{3.8 Context Window Optimization}

\subsection{Context Construction}

\begin{lstlisting}[style=python]
def build_context(retrieved_docs, max_tokens=4000):
    context = ""
    token_count = 0

    for i, doc in enumerate(retrieved_docs):
        doc_tokens = len(doc.split()) * 1.3  # Rough estimate

        if token_count + doc_tokens > max_tokens:
            break

        context += f"\n[Document {i+1}]\n{doc}\n"
        token_count += doc_tokens

    return context

# Usage
prompt = f"""
Answer the question using only the context below.

Context:
{build_context(retrieved_docs)}

Question: {user_query}

Answer:
"""
\end{lstlisting}

\subsection{Sliding Window Retrieval}

For very long documents:

\begin{lstlisting}[style=python]
def sliding_window_retrieval(long_document, query, window_size=500, stride=250):
    chunks = []
    for i in range(0, len(long_document), stride):
        chunk = long_document[i:i+window_size]
        chunks.append(chunk)

    # Embed and retrieve as normal
    chunk_embeddings = [get_embedding(c) for c in chunks]
    # ... retrieve top chunks
\end{lstlisting}

\section{3.9 Cited Answers \& Hallucination Control}

\begin{aside}
This is the difference between a demo and a product. Demos can hallucinate and nobody cares. Products that hallucinate get you sued, fired, or worse. Force citations. Always. If the LLM can't cite a source, it shouldn't make the claim. This is not negotiable for production systems.
\end{aside}

\subsection{Citation Pattern}

\begin{lstlisting}[style=python]
def rag_with_citations(query, retrieved_docs):
    # Build context with source IDs
    context = ""
    for i, doc in enumerate(retrieved_docs):
        context += f"\n[Source {i+1}]: {doc['text']}\n"

    prompt = f"""
    Answer the question using the provided sources.
    Cite sources using [Source X] notation.

    Context:
    {context}

    Question: {query}

    Answer (with citations):
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return response.choices[0].message.content

# Example output:
# "The company's Q4 revenue was $5.2M [Source 1], representing
#  a 23% increase from Q3 [Source 2]."
\end{lstlisting}

\subsection{Hallucination Detection}

\begin{lstlisting}[style=python]
def detect_hallucination(answer, context):
    prompt = f"""
    Check if the answer is fully supported by the context.

    Context: {context}
    Answer: {answer}

    Is the answer grounded in the context? (Yes/No)
    If No, list the unsupported claims.

    Response:
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content
\end{lstlisting}

\subsection{"I Don't Know" Pattern}

\begin{lstlisting}[style=python]
prompt = f"""
Answer the question using only the context below.

IMPORTANT:
- If the answer is not in the context, respond with "I don't have enough information to answer this question."
- Do not make up or infer information not explicitly stated.

Context:
{context}

Question: {query}

Answer:
"""
\end{lstlisting}

\begin{aside}
You now know how to build a production RAG system. Not a toy, not a demo - a real system that handles actual user queries without hallucinating nonsense. The next section covers knowledge graphs, which will let you add structured reasoning on top of your unstructured retrieval. This is where things get interesting.
\end{aside}

\clearpage

% ============================================================================
% SECTION 4: KNOWLEDGE GRAPH ENGINEERING MODULE
% ============================================================================

\part{KNOWLEDGE GRAPH ENGINEERING MODULE}

\begin{aside}
Knowledge graphs are where structured reasoning lives. RAG gives you semantic search, but knowledge graphs give you logical traversal - "find the manager's manager's direct reports who work on ML projects." You can't do that with vector similarity alone. The challenge isn't the tech, it's figuring out what your schema should look like before you've loaded a million nodes.
\end{aside}

\section{4.1 Graph Schema Design}

\subsection{What is a Schema?}

\textbf{Definition}: The blueprint of your graph - what types of nodes and relationships exist, and what properties they have.

\textbf{Analogy}: Like a database schema, but for graphs.

\begin{aside}
Unlike SQL schemas, graph schemas are flexible - you can add new node types and relationships without migrations. This is both a blessing and a curse. Blessing: easy to evolve. Curse: people abuse this flexibility and end up with an unmaintainable mess of ad-hoc relationships. Design your schema properly from the start.
\end{aside}

\subsection{Schema Design Process}

\subsubsection{Step 1: Identify Entities (Nodes)}

\textbf{Example Domain}: Company Knowledge Base

Entities:
\begin{itemize}
    \item Person (employees, customers)
    \item Company
    \item Product
    \item Project
    \item Document
\end{itemize}

\subsubsection{Step 2: Identify Relationships (Edges)}

Relationships:
\begin{itemize}
    \item Person WORKS\_FOR Company
    \item Person MANAGES Person
    \item Person AUTHORED Document
    \item Company PRODUCES Product
    \item Project USES Product
\end{itemize}

\subsubsection{Step 3: Define Properties}

\begin{lstlisting}[style=cypher]
// Node properties
Person: {name, email, role, hire_date}
Company: {name, industry, founded_year}
Product: {name, version, release_date}
Document: {title, content, created_date}

// Relationship properties
WORKS_FOR: {since, position}
MANAGES: {since}
AUTHORED: {date, contribution_type}
\end{lstlisting}

\subsection{Complete Schema Example}

\begin{lstlisting}[style=cypher]
// Create constraints (ensures data quality)
CREATE CONSTRAINT person_email IF NOT EXISTS
FOR (p:Person) REQUIRE p.email IS UNIQUE;

CREATE CONSTRAINT company_name IF NOT EXISTS
FOR (c:Company) REQUIRE c.name IS UNIQUE;

// Example data following schema
CREATE (alice:Person {
    name: "Alice Smith",
    email: "alice@example.com",
    role: "Engineer",
    hire_date: date("2020-01-15")
})

CREATE (acme:Company {
    name: "Acme Corp",
    industry: "Technology",
    founded_year: 2010
})

CREATE (alice)-[:WORKS_FOR {
    since: date("2020-01-15"),
    position: "Senior Engineer"
}]->(acme)
\end{lstlisting}

\subsection{Schema Best Practices}

\begin{enumerate}
    \item \textbf{Use Clear Labels}: \texttt{Person} not \texttt{P}, \texttt{WORKS\_FOR} not \texttt{W4}
    \item \textbf{Normalize Data}: Store shared properties once
    \item \textbf{Plan for Queries}: Design schema around your query patterns
    \item \textbf{Use Constraints}: Enforce uniqueness and data integrity
\end{enumerate}

\section{4.2 Triple Extraction from Text}

\subsection{What is Triple Extraction?}

\textbf{Goal}: Convert unstructured text into (Subject, Predicate, Object) triples.

\begin{aside}
This is harder than it looks. The examples below make it seem easy - extract some nouns and verbs, done. Real text is messy. Pronouns, implied relationships, context-dependent meaning, ambiguous references. Rule-based extraction gets you 60\% accuracy at best. LLM-based extraction gets you 85-90\% but costs money and is slow. Pick your tradeoff based on your quality requirements.
\end{aside}

\textbf{Example}:
\begin{verbatim}
Text: "Alice works at Acme Corp as a senior engineer."

Triples:
(Alice, WORKS_AT, Acme Corp)
(Alice, HAS_ROLE, Senior Engineer)
(Acme Corp, TYPE, Company)
\end{verbatim}

\subsection{Method 1: Rule-Based Extraction}

\begin{lstlisting}[style=python]
import spacy

nlp = spacy.load("en_core_web_sm")

def extract_triples_basic(text):
    doc = nlp(text)
    triples = []

    for sent in doc.sents:
        subject = None
        relation = None
        object_ = None

        for token in sent:
            # Find subject (noun)
            if token.dep_ in ("nsubj", "nsubjpass") and not subject:
                subject = token.text

            # Find relation (verb)
            if token.pos_ == "VERB" and not relation:
                relation = token.lemma_

            # Find object
            if token.dep_ in ("dobj", "pobj") and not object_:
                object_ = token.text

        if subject and relation and object_:
            triples.append((subject, relation.upper(), object_))

    return triples

# Example
text = "Alice works at Acme Corp. Bob manages the engineering team."
triples = extract_triples_basic(text)
# [("Alice", "WORK", "Acme Corp"), ("Bob", "MANAGE", "team")]
\end{lstlisting}

\subsection{Method 2: LLM-Based Extraction (Better Quality)}

\begin{lstlisting}[style=python]
def extract_triples_llm(text):
    prompt = f"""
    Extract knowledge graph triples from the text below.
    Format: (Subject, Relationship, Object)

    Text: {text}

    Triples (as JSON list):
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    # Parse response
    triples = eval(response.choices[0].message.content)
    return triples

# Example
text = """
Alice Smith is a senior engineer at Acme Corp.
She has been working there since 2020.
Acme Corp is a technology company founded in 2010.
"""

triples = extract_triples_llm(text)
# [
#   ("Alice Smith", "IS_A", "Senior Engineer"),
#   ("Alice Smith", "WORKS_AT", "Acme Corp"),
#   ("Alice Smith", "WORKS_SINCE", "2020"),
#   ("Acme Corp", "IS_A", "Technology Company"),
#   ("Acme Corp", "FOUNDED_IN", "2010")
# ]
\end{lstlisting}

\subsection{Method 3: Production-Grade Extraction}

\begin{lstlisting}[style=python]
from typing import List, Tuple
import json

class KnowledgeExtractor:
    def __init__(self, client):
        self.client = client

    def extract_triples(self, text: str) -> List[Tuple]:
        prompt = f"""
        Extract structured knowledge from the text.

        For each entity and relationship:
        1. Identify entities (people, companies, products, concepts)
        2. Identify relationships between entities
        3. Extract properties of entities

        Format output as JSON:
        {{
            "entities": [
                {{"id": "e1", "type": "Person", "name": "Alice Smith", "properties": {{}}}},
                ...
            ],
            "relationships": [
                {{"subject": "e1", "predicate": "WORKS_AT", "object": "e2", "properties": {{}}}},
                ...
            ]
        }}

        Text:
        {text}

        JSON:
        """

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            response_format={"type": "json_object"}
        )

        return json.loads(response.choices[0].message.content)

# Usage
extractor = KnowledgeExtractor(client)
result = extractor.extract_triples(text)
\end{lstlisting}

\section{4.3 Entity Linking}

\subsection{What is Entity Linking?}

\textbf{Problem}: Different text mentions refer to the same entity.

\begin{verbatim}
Text 1: "Alice works at Acme"
Text 2: "Alice Smith is an engineer"
Text 3: "A. Smith wrote the report"

Question: Are these the same Alice?
\end{verbatim}

\textbf{Solution}: Entity linking resolves mentions to canonical entities.

\subsection{Simple Entity Linking}

\begin{lstlisting}[style=python]
class EntityLinker:
    def __init__(self):
        self.entities = {}  # Canonical entities
        self.aliases = {}   # Alias $\rightarrow$ Canonical mapping

    def add_entity(self, canonical_name, aliases=None):
        self.entities[canonical_name] = {"name": canonical_name}
        if aliases:
            for alias in aliases:
                self.aliases[alias.lower()] = canonical_name

    def link(self, mention):
        mention_lower = mention.lower()
        return self.aliases.get(mention_lower, mention)

# Usage
linker = EntityLinker()
linker.add_entity("Alice Smith", aliases=["Alice", "A. Smith", "alice"])

print(linker.link("Alice"))      # "Alice Smith"
print(linker.link("A. Smith"))   # "Alice Smith"
print(linker.link("alice"))      # "Alice Smith"
\end{lstlisting}

\subsection{LLM-Based Entity Linking}

\begin{lstlisting}[style=python]
def link_entities_llm(mentions, known_entities):
    prompt = f"""
    Match each mention to a known entity, or mark as NEW.

    Known entities:
    {json.dumps(known_entities, indent=2)}

    Mentions:
    {json.dumps(mentions, indent=2)}

    Output format (JSON):
    [
        {{"mention": "Alice", "linked_to": "Alice Smith"}},
        {{"mention": "Bob", "linked_to": "NEW"}}
    ]

    JSON:
    """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)
\end{lstlisting}

\clearpage

\section{4.4 Building KGs Using Neo4j}

\begin{aside}
Neo4j is the most popular graph database for good reason - it's mature, fast, and has excellent tooling. The Docker setup below takes 30 seconds. The hard part isn't installation, it's designing your schema and remembering to create indexes before you load a million nodes and wonder why queries take 10 seconds.
\end{aside}

\subsection{Setting Up Neo4j}

\begin{lstlisting}[style=bash]
# Using Docker
docker run \
    --name neo4j \
    -p 7474:7474 -p 7687:7687 \
    -e NEO4J_AUTH=neo4j/password \
    neo4j:latest
\end{lstlisting}

\subsection{Connecting from Python}

\begin{lstlisting}[style=python]
from neo4j import GraphDatabase

class Neo4jConnection:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        with self.driver.session() as session:
            result = session.run(query, parameters)
            return [record.data() for record in result]

# Connect
conn = Neo4jConnection(
    uri="bolt://localhost:7687",
    user="neo4j",
    password="password"
)
\end{lstlisting}

\subsection{Building a KG from Triples}

\begin{lstlisting}[style=python]
class KnowledgeGraphBuilder:
    def __init__(self, neo4j_conn):
        self.conn = neo4j_conn

    def add_triple(self, subject, predicate, object_, properties=None):
        query = """
        MERGE (s:Entity {name: $subject})
        MERGE (o:Entity {name: $object})
        MERGE (s)-[r:RELATION {type: $predicate}]->(o)
        """
        if properties:
            query += "\nSET r += $properties"

        self.conn.query(query, {
            "subject": subject,
            "predicate": predicate,
            "object": object_,
            "properties": properties or {}
        })

    def build_from_text(self, text):
        # Extract triples
        triples = extract_triples_llm(text)

        # Add to graph
        for subject, predicate, object_ in triples:
            self.add_triple(subject, predicate, object_)

# Usage
builder = KnowledgeGraphBuilder(conn)
builder.build_from_text("""
    Alice Smith is a senior engineer at Acme Corp.
    She manages the data team.
    Acme Corp was founded in 2010.
""")
\end{lstlisting}

\subsection{Production KG Builder}

\begin{lstlisting}[style=python]
class ProductionKGBuilder:
    def __init__(self, neo4j_conn):
        self.conn = neo4j_conn
        self.create_indexes()

    def create_indexes(self):
        """Create indexes for performance"""
        self.conn.query("""
            CREATE INDEX entity_name IF NOT EXISTS
            FOR (e:Entity) ON (e.name)
        """)

    def add_entity(self, entity_type, name, properties):
        query = f"""
        MERGE (e:{entity_type} {{name: $name}})
        SET e += $properties
        RETURN e
        """
        return self.conn.query(query, {
            "name": name,
            "properties": properties
        })

    def add_relationship(self, from_entity, rel_type, to_entity, properties=None):
        query = """
        MATCH (a {name: $from})
        MATCH (b {name: $to})
        MERGE (a)-[r:REL {type: $rel_type}]->(b)
        SET r += $properties
        RETURN r
        """
        return self.conn.query(query, {
            "from": from_entity,
            "to": to_entity,
            "rel_type": rel_type,
            "properties": properties or {}
        })

    def bulk_import(self, triples):
        """Efficient bulk import"""
        for subject, predicate, object_ in triples:
            self.add_entity("Entity", subject, {})
            self.add_entity("Entity", object_, {})
            self.add_relationship(subject, predicate, object_)
\end{lstlisting}

\clearpage

\section{4.5 Querying with Cypher}

\begin{aside}
Cypher looks weird if you're coming from SQL. The ASCII-art syntax (\texttt{()-[]->()} feels gimmicky at first. It's not. It's actually brilliant - you can read queries visually as graph patterns. Give it a week and you'll prefer it to SQL JOINs for graph traversals.
\end{aside}

\subsection{Basic Queries}

\begin{lstlisting}[style=cypher]
// Find all people
MATCH (p:Person)
RETURN p.name, p.email

// Find who works where
MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
RETURN p.name, c.name

// Find with filters
MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
WHERE c.industry = "Technology"
RETURN p.name, p.role, c.name
\end{lstlisting}

\subsection{Multi-Hop Queries}

\begin{lstlisting}[style=cypher]
// Friends of friends
MATCH (me:Person {name: "Alice"})-[:FRIEND]->(friend)-[:FRIEND]->(fof)
RETURN DISTINCT fof.name

// People who work at same company as Alice's friends
MATCH (alice:Person {name: "Alice"})-[:FRIEND]->(friend)-[:WORKS_FOR]->(c:Company)
MATCH (colleague:Person)-[:WORKS_FOR]->(c)
WHERE colleague <> alice AND colleague <> friend
RETURN colleague.name, c.name

// Path finding: How is Alice connected to Bob?
MATCH path = shortestPath((alice:Person {name: "Alice"})-[*]-(bob:Person {name: "Bob"}))
RETURN path
\end{lstlisting}

\subsection{Aggregation Queries}

\begin{lstlisting}[style=cypher]
// Count employees per company
MATCH (p:Person)-[:WORKS_FOR]->(c:Company)
RETURN c.name, COUNT(p) AS employee_count
ORDER BY employee_count DESC

// Average team size
MATCH (manager:Person)-[:MANAGES]->(employee:Person)
RETURN manager.name, COUNT(employee) AS team_size
ORDER BY team_size DESC
\end{lstlisting}

\subsection{Advanced Pattern Matching}

\begin{lstlisting}[style=cypher]
// Find triangles (A knows B, B knows C, C knows A)
MATCH (a:Person)-[:KNOWS]->(b:Person)-[:KNOWS]->(c:Person)-[:KNOWS]->(a)
RETURN a.name, b.name, c.name

// Find influential people (many incoming connections)
MATCH (p:Person)
WITH p, size((p)<-[:REPORTS_TO]-()) AS subordinates
WHERE subordinates > 5
RETURN p.name, subordinates
ORDER BY subordinates DESC

// Recommendation: Products used by similar people
MATCH (me:Person {name: "Alice"})-[:USES]->(p:Product)
MATCH (similar:Person)-[:USES]->(p)
MATCH (similar)-[:USES]->(rec:Product)
WHERE NOT (me)-[:USES]->(rec)
RETURN rec.name, COUNT(similar) AS score
ORDER BY score DESC
LIMIT 5
\end{lstlisting}

\clearpage

\section{4.6 Graph Traversal Reasoning}

\subsection{Neighborhood Expansion}

\begin{lstlisting}[style=python]
def get_neighborhood(entity_name, max_depth=2):
    query = f"""
    MATCH path = (e:Entity {{name: $name}})-[*1..{max_depth}]-(neighbor)
    RETURN DISTINCT neighbor.name, length(path) AS distance
    ORDER BY distance
    """
    return conn.query(query, {"name": entity_name})

# Example
neighbors = get_neighborhood("Alice Smith", max_depth=2)
# Returns all entities within 2 hops of Alice
\end{lstlisting}

\subsection{Path-Based Reasoning}

\begin{lstlisting}[style=cypher]
// Find all paths between two entities
MATCH path = (a:Person {name: "Alice"})-[*..5]-(b:Person {name: "Bob"})
RETURN path
LIMIT 10

// Find shortest path
MATCH path = shortestPath((a:Person {name: "Alice"})-[*]-(b:Person {name: "Bob"}))
RETURN [node in nodes(path) | node.name] AS path_nodes,
       [rel in relationships(path) | type(rel)] AS path_relationships
\end{lstlisting}

\subsection{Complex Multi-Hop Reasoning}

\begin{lstlisting}[style=python]
def find_expertise_path(person, skill):
    """
    Find how a person is connected to a skill
    (e.g., through projects, colleagues, training)
    """
    query = """
    MATCH path = (p:Person {name: $person})-[*..4]-(s:Skill {name: $skill})
    WITH path,
         [node in nodes(path) | labels(node)[0] + ': ' + node.name] AS path_desc,
         length(path) AS dist
    ORDER BY dist
    LIMIT 5
    RETURN path_desc, dist
    """
    return conn.query(query, {"person": person, "skill": skill})

# Example: How does Alice connect to "Machine Learning"?
paths = find_expertise_path("Alice Smith", "Machine Learning")
# Might return:
# [Person: Alice] -> [WORKS_ON] -> [Project: ML Pipeline] -> [REQUIRES] -> [Skill: Machine Learning]
\end{lstlisting}

\subsection{Graph Algorithms}

\begin{lstlisting}[style=cypher]
// PageRank (find influential nodes)
CALL gds.pageRank.stream('my-graph')
YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC
LIMIT 10

// Community Detection
CALL gds.louvain.stream('my-graph')
YIELD nodeId, communityId
RETURN communityId, collect(gds.util.asNode(nodeId).name) AS members

// Shortest paths with weights
MATCH (start:Person {name: "Alice"}), (end:Person {name: "Bob"})
CALL gds.shortestPath.dijkstra.stream('my-graph', {
    sourceNode: start,
    targetNode: end,
    relationshipWeightProperty: 'weight'
})
YIELD path
RETURN path
\end{lstlisting}

\clearpage

\section{4.7 Micro-Projects}

\subsection{Project 4A: Build a Movie Knowledge Graph}

\textbf{Goal}: Create a KG from movie data (actors, directors, genres).

\textbf{Dataset}: Use IMDB CSV or API

\textbf{Tasks}:
\begin{enumerate}
    \item Design schema (Movie, Person, Genre nodes)
    \item Extract triples from movie descriptions
    \item Load into Neo4j
    \item Query: "Find actors who worked with Christopher Nolan"
\end{enumerate}

\begin{lstlisting}[style=cypher]
// Schema
CREATE (m:Movie {title: "Inception", year: 2010})
CREATE (p:Person {name: "Leonardo DiCaprio"})
CREATE (d:Person {name: "Christopher Nolan"})
CREATE (g:Genre {name: "Sci-Fi"})

CREATE (p)-[:ACTED_IN {role: "Cobb"}]->(m)
CREATE (d)-[:DIRECTED]->(m)
CREATE (m)-[:HAS_GENRE]->(g)

// Query
MATCH (actor:Person)-[:ACTED_IN]->(m:Movie)<-[:DIRECTED]-(director:Person {name: "Christopher Nolan"})
RETURN DISTINCT actor.name
\end{lstlisting}

\subsection{Project 4B: Academic Citation Graph}

\textbf{Goal}: Build citation network from research papers.

\textbf{Tasks}:
\begin{enumerate}
    \item Extract (Paper, CITES, Paper) relationships
    \item Find most influential papers (PageRank)
    \item Find papers in same research cluster
\end{enumerate}

\begin{lstlisting}[style=cypher]
// Build graph
CREATE (p1:Paper {title: "Attention Is All You Need", year: 2017})
CREATE (p2:Paper {title: "BERT", year: 2018})
CREATE (p2)-[:CITES]->(p1)

// Most cited papers
MATCH (p:Paper)
WITH p, size((p)<-[:CITES]-()) AS citations
WHERE citations > 10
RETURN p.title, citations
ORDER BY citations DESC
\end{lstlisting}

\subsection{Project 4C: Company Org Chart}

\textbf{Goal}: Model organizational hierarchy.

\textbf{Queries to Implement}:
\begin{itemize}
    \item Who reports to whom?
    \item What is the management chain for person X?
    \item Which teams are largest?
\end{itemize}

\begin{lstlisting}[style=cypher]
// Build org structure
CREATE (ceo:Person {name: "Jane Doe", title: "CEO"})
CREATE (cto:Person {name: "John Smith", title: "CTO"})
CREATE (eng:Person {name: "Alice", title: "Engineer"})

CREATE (eng)-[:REPORTS_TO]->(cto)
CREATE (cto)-[:REPORTS_TO]->(ceo)

// Find management chain
MATCH path = (emp:Person {name: "Alice"})-[:REPORTS_TO*]->(top)
WHERE NOT (top)-[:REPORTS_TO]->()
RETURN [person in nodes(path) | person.name] AS chain
\end{lstlisting}

\begin{aside}
You now have both pieces: RAG for semantic search over unstructured text, and knowledge graphs for structured traversal over relationships. Separately, they're useful. Together, they're transformative. Section 5 shows you how to combine them - and why most attempts to do this fail.
\end{aside}

\clearpage

% ============================================================================
% SECTION 5: HYBRID RAG + KG SYSTEMS
% ============================================================================

\part{HYBRID RAG + KG SYSTEMS (MAIN FOCUS)}

\begin{aside}
This is it. This is why you're here. RAG alone is useful but limited. Knowledge graphs alone are powerful but rigid. Combined correctly, you get a system that can answer questions neither approach could handle alone. Combined incorrectly, you get twice the complexity and worse results than either system alone. Pay attention to the failure modes in this section - they're drawn from real production systems that had to be rebuilt.
\end{aside}

\section{5.1 Why Combine RAG and Knowledge Graphs?}

\subsection{Limitations of Pure RAG}

 \textbf{Struggles with multi-hop reasoning}
\begin{verbatim}
Question: "What technology does Alice's manager's company use?"
Pure RAG: Retrieves documents mentioning Alice, managers, companies, technology separately
$\rightarrow$ No coherent answer
\end{verbatim}

 \textbf{Misses structured relationships}
\begin{verbatim}
Question: "Who reports to the CTO?"
Pure RAG: Finds documents with "CTO" and "reports"
$\rightarrow$ May miss implicit reporting structures
\end{verbatim}

 \textbf{No entity disambiguation}
\begin{verbatim}
Question: "What does Apple produce?"
Pure RAG: Returns info about fruit OR company
$\rightarrow$ No context to disambiguate
\end{verbatim}

\subsection{Limitations of Pure KG}

 \textbf{Can't handle unstructured knowledge}
\begin{verbatim}
Question: "What are best practices for API design?"
Pure KG: No nodes for "best practices" concept
$\rightarrow$ Can't answer without structured triples
\end{verbatim}

 \textbf{Limited by schema}
\begin{verbatim}
Question: "What did the CEO say in the Q4 earnings call?"
Pure KG: Doesn't store full transcript text
$\rightarrow$ Only has structured metadata
\end{verbatim}

\subsection{Power of Hybrid RAG + KG}

\begin{itemize}
    \item  \textbf{Multi-hop reasoning} (from KG) + \textbf{Rich context} (from RAG)
    \item  \textbf{Structured queries} (KG) + \textbf{Semantic search} (RAG)
    \item  \textbf{Entity disambiguation} (KG) + \textbf{Document retrieval} (RAG)
    \item  \textbf{Explainable paths} (KG) + \textbf{Cited answers} (RAG)
\end{itemize}

\clearpage

\section{5.2 Graph-RAG Architecture}

\subsection{Architecture Overview}

\begin{verbatim}
User Query
    $\downarrow$
[1] Query Understanding
    $\rightarrow$ Extract entities
    $\rightarrow$ Classify intent
    $\rightarrow$ Identify query type
    $\downarrow$
[2] Hybrid Retrieval
    $\rightarrow$ KG: Graph traversal for structured knowledge
    $\rightarrow$ RAG: Vector search for unstructured text
    $\rightarrow$ Merge results
    $\downarrow$
[3] Context Enhancement
    $\rightarrow$ Expand KG neighborhoods
    $\rightarrow$ Retrieve related documents
    $\rightarrow$ Rank and filter
    $\downarrow$
[4] LLM Generation
    $\rightarrow$ Combine graph paths + documents
    $\rightarrow$ Generate answer
    $\rightarrow$ Add citations + reasoning traces
    $\downarrow$
Answer with explanation
\end{verbatim}

\subsection{System Components}

\begin{lstlisting}[style=python]
class HybridRAGKGSystem:
    def __init__(self):
        self.vector_db = ChromaDB()        # For RAG
        self.graph_db = Neo4jConnection()  # For KG
        self.llm = OpenAI()               # For generation
        self.entity_linker = EntityLinker()

    def query(self, user_question):
        # Step 1: Understand query
        entities = self.extract_entities(user_question)
        query_type = self.classify_query(user_question)

        # Step 2: Retrieve from both sources
        if query_type == "structured":
            # KG-heavy retrieval
            graph_results = self.query_graph(entities)
            doc_results = self.query_documents(user_question, top_k=3)
        elif query_type == "unstructured":
            # RAG-heavy retrieval
            doc_results = self.query_documents(user_question, top_k=10)
            graph_results = self.query_graph(entities, max_depth=1)
        else:
            # Hybrid retrieval
            graph_results = self.query_graph(entities)
            doc_results = self.query_documents(user_question, top_k=5)

        # Step 3: Enhance context
        enhanced_context = self.enhance_context(
            graph_results, doc_results, entities
        )

        # Step 4: Generate answer
        answer = self.generate_answer(
            user_question, enhanced_context
        )

        return answer
\end{lstlisting}

\section{5.3 KG-Augmented Retrieval}

\subsection{Pattern 1: Entity-Centric Retrieval}

\textbf{Use Case}: Query mentions specific entities

\begin{lstlisting}[style=python]
def entity_centric_retrieval(query, entities):
    """
    1. Find entities in KG
    2. Get their neighborhoods
    3. Retrieve documents mentioning those neighbors
    """
    # Step 1: Find entity in KG
    kg_query = """
    MATCH (e:Entity {name: $entity})-[*1..2]-(neighbor)
    RETURN DISTINCT neighbor.name, labels(neighbor)[0] AS type
    """
    neighbors = graph_db.query(kg_query, {"entity": entities[0]})

    # Step 2: Build expanded query
    expanded_query = query + " " + " ".join([n['name'] for n in neighbors])

    # Step 3: Retrieve documents
    documents = vector_db.query(expanded_query, top_k=10)

    return {
        "graph_context": neighbors,
        "documents": documents
    }

# Example
query = "What projects is Alice working on?"
entities = ["Alice"]
results = entity_centric_retrieval(query, entities)

# Results include:
# - KG: Alice $\rightarrow$ WORKS_ON $\rightarrow$ Project X, Project Y
# - Docs: Project descriptions, meeting notes about those projects
\end{lstlisting}

\subsection{Pattern 2: Relationship-Aware Retrieval}

\begin{lstlisting}[style=python]
def relationship_aware_retrieval(subject, relation, object_=None):
    """
    Query like: "What does Alice manage?"
    $\rightarrow$ Find relationship in KG
    $\rightarrow$ Retrieve supporting documents
    """
    # Build Cypher query
    if object_:
        kg_query = """
        MATCH (s:Entity {name: $subject})-[r:RELATION {type: $relation}]->(o:Entity {name: $object})
        RETURN s, r, o
        """
        params = {"subject": subject, "relation": relation, "object": object_}
    else:
        kg_query = """
        MATCH (s:Entity {name: $subject})-[r:RELATION {type: $relation}]->(o)
        RETURN s, r, o
        LIMIT 10
        """
        params = {"subject": subject, "relation": relation}

    # Execute graph query
    graph_results = graph_db.query(kg_query, params)

    # For each result, find supporting documents
    all_docs = []
    for result in graph_results:
        doc_query = f"{result['s']['name']} {relation} {result['o']['name']}"
        docs = vector_db.query(doc_query, top_k=3)
        all_docs.extend(docs)

    return {
        "graph_facts": graph_results,
        "supporting_docs": all_docs
    }
\end{lstlisting}

\subsection{Pattern 3: Multi-Hop Graph $\rightarrow$ RAG}

\begin{lstlisting}[style=python]
def multi_hop_retrieval(start_entity, path_pattern, max_hops=3):
    """
    Follow graph paths, then retrieve documents for each node
    """
    # Find paths in graph
    kg_query = f"""
    MATCH path = (start:Entity {{name: $start}})-[*1..{max_hops}]-(end)
    WHERE {path_pattern}
    RETURN path, end
    LIMIT 20
    """

    paths = graph_db.query(kg_query, {"start": start_entity})

    # For each path, retrieve documents
    context = {
        "paths": [],
        "documents": {}
    }

    for path_result in paths:
        path = path_result['path']
        nodes = [node['name'] for node in path]

        context['paths'].append(nodes)

        # Retrieve docs for each node
        for node in nodes:
            if node not in context['documents']:
                docs = vector_db.query(node, top_k=2)
                context['documents'][node] = docs

    return context

# Example: "How is Alice connected to Machine Learning?"
context = multi_hop_retrieval(
    start_entity="Alice",
    path_pattern="end:Skill AND end.name = 'Machine Learning'"
)
# Returns:
# - Paths: [Alice $\rightarrow$ WORKS_ON $\rightarrow$ ML Project $\rightarrow$ REQUIRES $\rightarrow$ Machine Learning]
# - Documents for each: {Alice: [...], ML Project: [...], Machine Learning: [...]}
\end{lstlisting}

\clearpage

\section{5.4 KG-Guided Query Routing}

\subsection{Query Classification}

\begin{lstlisting}[style=python]
class QueryRouter:
    def __init__(self, llm):
        self.llm = llm

    def classify_query(self, query):
        """
        Classify query type to route to appropriate retrieval strategy
        """
        prompt = f"""
        Classify this query into one category:

        1. FACTUAL: Simple fact retrieval (who, what, when, where)
           Example: "Who is the CEO?"

        2. RELATIONAL: About relationships between entities
           Example: "Who reports to Alice?"

        3. MULTI_HOP: Requires following multiple relationships
           Example: "What skills do Alice's teammates have?"

        4. ANALYTICAL: Requires deep understanding or summarization
           Example: "What are the main challenges in our Q4 report?"

        5. HYBRID: Combines structured and unstructured knowledge
           Example: "How does our product compare to competitors based on customer feedback?"

        Query: {query}

        Classification (JSON):
        {{"type": "...", "confidence": 0.0-1.0, "reasoning": "..."}}
        """

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            response_format={"type": "json_object"}
        )

        return json.loads(response.choices[0].message.content)
\end{lstlisting}

\subsection{Routing Logic}

\begin{lstlisting}[style=python]
def route_query(query, classification):
    """
    Route to appropriate retrieval strategy based on query type
    """
    query_type = classification['type']

    if query_type == "FACTUAL":
        # KG-first: Direct lookup
        return kg_factual_lookup(query)

    elif query_type == "RELATIONAL":
        # KG-only: Traverse relationships
        return kg_relationship_query(query)

    elif query_type == "MULTI_HOP":
        # KG traversal + RAG for context
        return multi_hop_retrieval(query)

    elif query_type == "ANALYTICAL":
        # RAG-heavy: Retrieve many docs, minimal KG
        return rag_heavy_retrieval(query)

    elif query_type == "HYBRID":
        # Full hybrid: Both systems equally
        return full_hybrid_retrieval(query)

def kg_factual_lookup(query):
    """Simple KG lookup for factual queries"""
    entities = extract_entities(query)
    if entities:
        result = graph_db.query("""
            MATCH (e:Entity {name: $name})
            RETURN e
        """, {"name": entities[0]})
        return {"source": "KG", "result": result}

def rag_heavy_retrieval(query):
    """RAG-focused for analytical queries"""
    docs = vector_db.query(query, top_k=20)
    reranked = rerank(query, docs, top_k=10)
    return {"source": "RAG", "documents": reranked}
\end{lstlisting}

\clearpage

\section{5.5 Combining Structured + Unstructured Knowledge}

\subsection{Context Fusion Strategy}

\begin{aside}
Context fusion is where most hybrid systems fail. You have graph facts ("Alice reports to Bob") and document snippets ("Bob recently announced new product priorities"). How do you combine them without confusing the LLM? The naive approach is to dump both into the prompt and hope for the best. That fails about 40\% of the time - the LLM either ignores one source or hallucinates connections between them. The approach below actually works.
\end{aside}

\begin{lstlisting}[style=python]
class ContextFusion:
    def fuse_contexts(self, graph_results, doc_results, query):
        """
        Combine graph paths and documents into unified context
        """
        # Build graph context
        graph_context = self.format_graph_context(graph_results)

        # Build document context
        doc_context = self.format_doc_context(doc_results)

        # Create fused prompt
        fused_context = f"""
        STRUCTURED KNOWLEDGE (from Knowledge Graph):
        {graph_context}

        UNSTRUCTURED KNOWLEDGE (from Documents):
        {doc_context}

        Instructions:
        - Use structured knowledge for facts, relationships, and entities
        - Use unstructured knowledge for details, explanations, and context
        - Cite sources: [Graph: ...] or [Doc: ...]
        - If structured and unstructured conflict, prefer structured for facts
        """

        return fused_context

    def format_graph_context(self, graph_results):
        """Format graph results as readable text"""
        formatted = []
        for result in graph_results:
            if 'path' in result:
                path_str = " $\rightarrow$ ".join([
                    f"{node['name']} ({node['type']})"
                    for node in result['path']
                ])
                formatted.append(f"- Path: {path_str}")
            elif 'entity' in result:
                entity = result['entity']
                formatted.append(
                    f"- Entity: {entity['name']} ({entity['type']}) "
                    f"Properties: {entity.get('properties', {})}"
                )
        return "\n".join(formatted)

    def format_doc_context(self, doc_results):
        """Format documents with source info"""
        formatted = []
        for i, doc in enumerate(doc_results):
            formatted.append(
                f"[Doc {i+1}] (Source: {doc.get('source', 'Unknown')})\n"
                f"{doc['content']}\n"
            )
        return "\n".join(formatted)
\end{lstlisting}

\subsection{Practical Example}

\begin{lstlisting}[style=python]
# Query: "What are Alice's manager's responsibilities?"

# Step 1: Extract entities
entities = ["Alice"]

# Step 2: Query KG
graph_results = graph_db.query("""
    MATCH (alice:Person {name: "Alice"})-[:REPORTS_TO]->(manager:Person)
    MATCH (manager)-[:RESPONSIBLE_FOR]->(responsibility)
    RETURN manager.name, collect(responsibility.name) AS responsibilities
""")
# Result: {"manager.name": "Bob", "responsibilities": ["Engineering", "Product"]}

# Step 3: Query RAG
doc_query = "Bob's responsibilities engineering product"
doc_results = vector_db.query(doc_query, top_k=5)
# Returns documents about Bob's role, engineering team, product roadmap

# Step 4: Fuse contexts
fusion = ContextFusion()
fused_context = fusion.fuse_contexts(graph_results, doc_results, query)

# Step 5: Generate answer
prompt = f"""
{fused_context}

Question: What are Alice's manager's responsibilities?

Answer with citations:
"""

answer = llm.generate(prompt)
# "Alice's manager is Bob [Graph]. His responsibilities include Engineering and Product [Graph].
#  According to the team charter, he oversees the development of core platform features [Doc 2]
#  and coordinates with the product team on roadmap priorities [Doc 3]."
\end{lstlisting}

\clearpage

\section{5.6 Using LLMs to Generate Cypher Queries}

\subsection{Text-to-Cypher}

\begin{lstlisting}[style=python]
class Text2Cypher:
    def __init__(self, llm, schema):
        self.llm = llm
        self.schema = schema  # Graph schema description

    def generate_cypher(self, natural_language_query):
        """Convert natural language to Cypher query"""
        prompt = f"""
        Convert the natural language question to a Cypher query.

        GRAPH SCHEMA:
        {self.schema}

        RULES:
        - Use MATCH for patterns
        - Use WHERE for filters
        - Use RETURN for results
        - Limit results to 10 unless specified

        EXAMPLES:
        Q: "Who works at Acme?"
        A: MATCH (p:Person)-[:WORKS_FOR]->(c:Company {{name: "Acme"}})
           RETURN p.name

        Q: "Who does Alice report to?"
        A: MATCH (alice:Person {{name: "Alice"}})-[:REPORTS_TO]->(manager)
           RETURN manager.name

        QUESTION: {natural_language_query}

        CYPHER QUERY:
        """

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        cypher_query = response.choices[0].message.content.strip()
        return cypher_query

# Usage
schema = """
Nodes:
- Person (properties: name, email, role)
- Company (properties: name, industry)
- Project (properties: name, status)

Relationships:
- (Person)-[:WORKS_FOR]->(Company)
- (Person)-[:REPORTS_TO]->(Person)
- (Person)-[:WORKS_ON]->(Project)
"""

text2cypher = Text2Cypher(llm, schema)
query = "What projects is Alice working on?"
cypher = text2cypher.generate_cypher(query)
# MATCH (p:Person {name: "Alice"})-[:WORKS_ON]->(proj:Project)
# RETURN proj.name, proj.status
\end{lstlisting}

\subsection{Query Validation}

\begin{lstlisting}[style=python]
def validate_and_execute_cypher(cypher_query, graph_db):
    """
    Validate Cypher query before execution
    """
    # Basic validation
    if "DELETE" in cypher_query.upper() or "REMOVE" in cypher_query.upper():
        raise ValueError("Destructive queries not allowed")

    # Dry run (explain query)
    try:
        explain_query = f"EXPLAIN {cypher_query}"
        graph_db.query(explain_query)
    except Exception as e:
        # Query has syntax error
        return {
            "success": False,
            "error": str(e),
            "suggestion": "Check Cypher syntax"
        }

    # Execute
    try:
        results = graph_db.query(cypher_query)
        return {
            "success": True,
            "results": results
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }
\end{lstlisting}

\subsection{Self-Correcting Cypher Generation}

\begin{lstlisting}[style=python]
def self_correcting_text2cypher(nl_query, max_attempts=3):
    """
    Generate Cypher with self-correction
    """
    for attempt in range(max_attempts):
        # Generate Cypher
        cypher = text2cypher.generate_cypher(nl_query)

        # Try to execute
        result = validate_and_execute_cypher(cypher, graph_db)

        if result['success']:
            return result['results']

        # If failed, try to fix
        if attempt < max_attempts - 1:
            fix_prompt = f"""
            This Cypher query failed:
            {cypher}

            Error: {result['error']}

            Generate a corrected version:
            """
            # Continue loop with correction
            nl_query = fix_prompt

    return {"error": "Failed to generate valid Cypher after retries"}
\end{lstlisting}

\clearpage

\section{5.7 KG Reasoning + RAG Context for Perfect Answers}

\subsection{The Perfect Answer Pattern}

\begin{lstlisting}[style=python]
class PerfectAnswerSystem:
    def answer_question(self, question):
        """
        Combine KG reasoning + RAG context for comprehensive answers
        """
        # Step 1: Extract structured components
        entities = self.extract_entities(question)
        intent = self.classify_intent(question)

        # Step 2: KG reasoning (find facts and paths)
        kg_facts = self.kg_reasoning(entities, intent)

        # Step 3: RAG context (find supporting details)
        rag_context = self.rag_retrieval(question, entities)

        # Step 4: Verify facts (cross-check KG with RAG)
        verified_facts = self.verify_facts(kg_facts, rag_context)

        # Step 5: Generate comprehensive answer
        answer = self.generate_with_reasoning(
            question,
            verified_facts,
            rag_context,
            kg_facts
        )

        return answer

    def kg_reasoning(self, entities, intent):
        """Extract facts and relationships from KG"""
        # Generate Cypher based on intent
        if intent == "relationship":
            cypher = f"""
            MATCH (e1)-[r]-(e2)
            WHERE e1.name IN {entities}
            RETURN e1, type(r) AS relationship, e2
            LIMIT 20
            """
        elif intent == "property":
            cypher = f"""
            MATCH (e)
            WHERE e.name IN {entities}
            RETURN e, properties(e) AS props
            """
        else:
            cypher = f"""
            MATCH path = (e1)-[*1..3]-(e2)
            WHERE e1.name IN {entities}
            RETURN path
            LIMIT 10
            """

        return graph_db.query(cypher, {"entities": entities})

    def verify_facts(self, kg_facts, rag_context):
        """Cross-verify KG facts with RAG documents"""
        verified = []

        for fact in kg_facts:
            # Check if any document supports this fact
            fact_str = self.format_fact(fact)
            supporting_docs = [
                doc for doc in rag_context
                if self.supports_fact(doc, fact_str)
            ]

            verified.append({
                "fact": fact,
                "confidence": "high" if supporting_docs else "medium",
                "supporting_docs": supporting_docs
            })

        return verified

    def generate_with_reasoning(self, question, facts, context, kg_facts):
        """Generate answer with reasoning trace"""
        prompt = f"""
        Answer the question using the provided information.

        QUESTION: {question}

        VERIFIED FACTS (from Knowledge Graph):
        {json.dumps(facts, indent=2)}

        SUPPORTING CONTEXT (from Documents):
        {self.format_docs(context)}

        Instructions:
        1. Answer the question directly
        2. Explain your reasoning
        3. Cite all sources
        4. Show the logical path from question to answer

        Format:
        ANSWER: [Direct answer]

        REASONING:
        - [Step-by-step logical reasoning]

        EVIDENCE:
        - [Graph facts cited]
        - [Documents cited]

        Response:
        """

        response = llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        return response.choices[0].message.content
\end{lstlisting}

\clearpage

\section{5.8 Trustworthiness and Explainability Patterns}

\subsection{Pattern 1: Provenance Tracking}

\begin{lstlisting}[style=python]
class ProvenanceTracker:
    def track_answer_sources(self, answer, kg_results, rag_results):
        """
        Track where each claim in the answer comes from
        """
        # Parse answer into claims
        claims = self.extract_claims(answer)

        provenance = []
        for claim in claims:
            sources = {
                "claim": claim,
                "kg_support": self.find_kg_support(claim, kg_results),
                "rag_support": self.find_rag_support(claim, rag_results),
                "confidence": self.calculate_confidence(claim, kg_results, rag_results)
            }
            provenance.append(sources)

        return provenance

    def calculate_confidence(self, claim, kg_results, rag_results):
        """Calculate confidence based on source agreement"""
        kg_support = len(self.find_kg_support(claim, kg_results))
        rag_support = len(self.find_rag_support(claim, rag_results))

        if kg_support > 0 and rag_support > 0:
            return "HIGH"  # Both sources agree
        elif kg_support > 0 or rag_support > 0:
            return "MEDIUM"  # One source
        else:
            return "LOW"  # No clear source
\end{lstlisting}

\subsection{Pattern 2: Reasoning Chains}

\begin{lstlisting}[style=python]
def generate_with_reasoning_chain(question, kg_context, rag_context):
    """
    Generate answer with explicit reasoning chain
    """
    prompt = f"""
    Answer the question step-by-step, showing your reasoning.

    KG Context: {kg_context}
    RAG Context: {rag_context}

    Question: {question}

    Format your response as:

    THOUGHT 1: [What I need to find out first]
    ACTION 1: [Which knowledge source to use: KG or RAG]
    OBSERVATION 1: [What I found]

    THOUGHT 2: [Next step in reasoning]
    ACTION 2: [...]
    OBSERVATION 2: [...]

    FINAL ANSWER: [Complete answer with citations]
    """

    response = llm.generate(prompt)
    return response
\end{lstlisting}

\subsection{Pattern 3: Confidence Scores}

\begin{lstlisting}[style=python]
class ConfidenceScorer:
    def score_answer(self, answer, question, sources):
        """
        Score answer confidence based on multiple factors
        """
        scores = {
            "source_agreement": self.check_source_agreement(sources),
            "coverage": self.check_question_coverage(question, answer),
            "specificity": self.check_specificity(answer),
            "citation_quality": self.check_citations(answer, sources)
        }

        # Weighted average
        total_score = (
            scores["source_agreement"] * 0.4 +
            scores["coverage"] * 0.3 +
            scores["specificity"] * 0.2 +
            scores["citation_quality"] * 0.1
        )

        return {
            "overall_confidence": total_score,
            "breakdown": scores,
            "recommendation": self.get_recommendation(total_score)
        }

    def get_recommendation(self, score):
        if score > 0.8:
            return "HIGH CONFIDENCE: Answer is well-supported"
        elif score > 0.5:
            return "MEDIUM CONFIDENCE: Answer is partially supported"
        else:
            return "LOW CONFIDENCE: Answer may be unreliable"
\end{lstlisting}

\clearpage

\section{5.9 Architecture Diagrams}

\subsection{Diagram 1: Basic Hybrid Flow}

\begin{verbatim}

                    USER QUERY                           
          "What projects is Alice's manager working on?" 

                     
                     $\lor$
         
           Entity Extraction    
           Entities: [Alice]    
         
                     
         
                                
         $\lor$                       $\lor$
      
  KNOWLEDGE             VECTOR DB     
  GRAPH (Neo4j)         (ChromaDB)    
      
                                
          Cypher:                Similarity search:
          Alice$\rightarrow$Manager          "manager projects"
          Manager$\rightarrow$Projects      
                                
         $\lor$                       $\lor$
      
 Graph Results:        Documents:     
 - Alice               - Project      
   $\rightarrow$Bob                  descriptions 
 - Bob                 - Meeting      
   $\rightarrow$Project X            notes        
   $\rightarrow$Project Y          - Status       
      
                                
         
                     
                     $\lor$
         
            Context Fusion      
            Combine KG + RAG    
         
                     
                     $\lor$
         
            LLM Generation      
            (GPT-4)             
         
                     
                     $\lor$

                   FINAL ANSWER                          
 "Alice's manager is Bob [Graph]. Bob is currently       
  working on Project X and Project Y [Graph]. Project X  
  focuses on API redesign [Doc 1], while Project Y is    
  the mobile app refresh [Doc 2]."                       

\end{verbatim}

\subsection{Diagram 2: Query Routing Decision Tree}

\begin{verbatim}
                    [User Query]
                         
                         $\lor$
              
                Query Classifier    
              
                         
         
                                       
         $\lor$               $\lor$               $\lor$
    [Factual]      [Relational]    [Analytical]
                                       
         $\lor$               $\lor$               $\lor$
    KG Direct      KG Traversal      RAG Heavy
    Lookup         + RAG Lite        + KG Lite
                                       
         
                         
                         $\lor$
                   [Generate Answer]
\end{verbatim}

\clearpage

\section{5.10 Comparison: Plain RAG vs Hybrid RAG+KG}

\begin{aside}
This table answers the question everyone asks: "Is the extra complexity worth it?" Short answer: it depends. If your queries are simple lookups over documents, stick with RAG. If you need multi-hop reasoning, entity disambiguation, or explainable answers over structured data, the hybrid approach is worth the complexity. Don't build it because it's cool - build it because your use case demands it.
\end{aside}

\begin{longtable}{|p{4cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Aspect} & \textbf{Plain RAG} & \textbf{Hybrid RAG + KG} \\
\hline
\textbf{Multi-hop questions} &  Struggles, needs many retrievals &  Direct graph traversal \\
\hline
\textbf{Entity disambiguation} &  No context &  KG provides entity types \\
\hline
\textbf{Relationship queries} &  Keyword-based, imprecise &  Structured relationships \\
\hline
\textbf{Unstructured knowledge} &  Excellent &  Same via RAG \\
\hline
\textbf{Explainability} &  Citations only &  Citations + reasoning paths \\
\hline
\textbf{Setup complexity} & Low & High \\
\hline
\textbf{Query latency} & Fast (100-300ms) & Medium (300-800ms) \\
\hline
\textbf{Accuracy (structured)} & Medium (70-80\%) & High (85-95\%) \\
\hline
\textbf{Accuracy (unstructured)} & High (85-90\%) & High (85-95\%) \\
\hline
\end{longtable}

\subsection{Example Comparison}

\textbf{Query}: "What technology does Alice's manager's company use?"

\textbf{Plain RAG}:
\begin{verbatim}
Retrieved docs:
- "Alice is an engineer..."
- "The company uses Python and AWS..."
- "Manager Bob oversees engineering..."

Answer: "The company uses Python and AWS" 
Problem: Doesn't identify manager or verify company connection
\end{verbatim}

\textbf{Hybrid RAG + KG}:
\begin{verbatim}
KG traversal:
Alice $\rightarrow$[REPORTS_TO]$\rightarrow$ Bob $\rightarrow$[WORKS_FOR]$\rightarrow$ Acme Corp

KG query for Acme's tech:
Acme Corp $\rightarrow$[USES_TECHNOLOGY]$\rightarrow$ [Python, AWS, PostgreSQL]

RAG retrieval:
"Acme Corp's tech stack includes..."

Answer: "Alice's manager is Bob, who works at Acme Corp [Graph].
Acme Corp uses Python, AWS, and PostgreSQL [Graph + Doc 3]." 
\end{verbatim}

\begin{aside}
You now understand hybrid RAG + KG architectures conceptually. The next section is about making them work in production - deployment, monitoring, evaluation, and all the messy details tutorials skip. This is where theory meets reality, and where most systems break in ways you didn't anticipate.
\end{aside}

\clearpage

% ============================================================================
% SECTION 6: PRACTICAL ENGINEERING SKILLS
% ============================================================================

\part{PRACTICAL ENGINEERING SKILLS}

\begin{aside}
This section is about making your system actually work in production. Everything before this assumed clean data, perfect uptime, and users who ask well-formed questions. None of that is true. Real documents are messy PDFs with broken encoding. Real users ask ambiguous questions. Real systems crash at 3am. The code below handles these realities.
\end{aside}

\section{6.1 Document Processing Pipeline}

\subsection{End-to-End Pipeline}

\begin{lstlisting}[style=python]
class DocumentProcessor:
    def __init__(self):
        self.supported_formats = ['.pdf', '.docx', '.txt', '.md', '.html']

    def process_document(self, file_path):
        """
        Complete document processing pipeline
        """
        # Step 1: Extract text
        raw_text = self.extract_text(file_path)

        # Step 2: Clean text
        cleaned_text = self.clean_text(raw_text)

        # Step 3: Extract metadata
        metadata = self.extract_metadata(file_path, cleaned_text)

        # Step 4: Chunk text
        chunks = self.chunk_text(cleaned_text)

        # Step 5: Generate embeddings
        chunk_objects = self.create_chunk_objects(chunks, metadata)

        return chunk_objects

    def extract_text(self, file_path):
        """Extract text from various formats"""
        ext = Path(file_path).suffix.lower()

        if ext == '.pdf':
            return self.extract_from_pdf(file_path)
        elif ext == '.docx':
            return self.extract_from_docx(file_path)
        elif ext in ['.txt', '.md']:
            return Path(file_path).read_text(encoding='utf-8')
        elif ext == '.html':
            return self.extract_from_html(file_path)

    def extract_from_pdf(self, file_path):
        """Extract text from PDF"""
        import PyPDF2
        text = ""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text()
        return text

    def clean_text(self, text):
        """Clean extracted text"""
        import re

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)

        # Remove page numbers (simple heuristic)
        text = re.sub(r'\n\d+\n', '\n', text)

        # Fix broken words (simple version)
        text = re.sub(r'(\w+)-\s+(\w+)', r'\1\2', text)

        return text.strip()

    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """Chunk text with overlap"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        chunks = splitter.split_text(text)
        return chunks

    def create_chunk_objects(self, chunks, metadata):
        """Create chunk objects with embeddings"""
        chunk_objects = []
        for i, chunk in enumerate(chunks):
            chunk_obj = {
                "id": f"{metadata['file_name']}_{i}",
                "content": chunk,
                "metadata": {
                    **metadata,
                    "chunk_index": i,
                    "char_count": len(chunk)
                },
                "embedding": get_embedding(chunk)
            }
            chunk_objects.append(chunk_obj)

        return chunk_objects
\end{lstlisting}

\clearpage

\section{6.2 Metadata Extraction}

\subsection{Extracting Rich Metadata}

\begin{lstlisting}[style=python]
class MetadataExtractor:
    def extract_metadata(self, file_path, content):
        """Extract comprehensive metadata"""
        metadata = {
            # File metadata
            "file_name": Path(file_path).name,
            "file_type": Path(file_path).suffix,
            "file_size": Path(file_path).stat().st_size,
            "created_date": datetime.fromtimestamp(
                Path(file_path).stat().st_ctime
            ).isoformat(),

            # Content metadata
            "char_count": len(content),
            "word_count": len(content.split()),

            # Extracted metadata
            "title": self.extract_title(content),
            "author": self.extract_author(content),
            "summary": self.extract_summary(content),
            "keywords": self.extract_keywords(content),
            "entities": self.extract_entities(content),
        }

        return metadata

    def extract_entities(self, content):
        """Extract named entities"""
        import spacy
        nlp = spacy.load("en_core_web_sm")

        # Limit content for performance
        doc = nlp(content[:5000])

        entities = {}
        for ent in doc.ents:
            if ent.label_ not in entities:
                entities[ent.label_] = []
            entities[ent.label_].append(ent.text)

        # Deduplicate
        entities = {k: list(set(v)) for k, v in entities.items()}

        return entities

    def extract_summary(self, content):
        """Generate summary using LLM"""
        prompt = f"""
        Summarize this document in 2-3 sentences:

        {content[:2000]}

        Summary:
        """

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150
        )

        return response.choices[0].message.content
\end{lstlisting}

\clearpage

\section{6.3 Evaluation Frameworks}

\begin{aside}
Evaluation is where most RAG projects fail. You ship a system that seems to work, users complain it's wrong 30\% of the time, and you have no systematic way to measure or fix it. The frameworks below give you actual numbers. Yes, setting up evaluation is tedious. No, you can't skip it and expect to improve your system. If you're not measuring, you're guessing.
\end{aside}

\subsection{RAGAS (RAG Assessment)}

\begin{lstlisting}[style=python]
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

def evaluate_rag_system(test_questions, answers, contexts, ground_truths):
    """
    Evaluate RAG system using RAGAS metrics
    """
    from datasets import Dataset

    # Prepare data
    data = {
        "question": test_questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }

    dataset = Dataset.from_dict(data)

    # Evaluate
    result = evaluate(
        dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_recall,
            context_precision,
        ],
    )

    return result

# Example
test_questions = ["Who is the CEO?", "What is our Q4 revenue?"]
answers = ["The CEO is Alice", "Q4 revenue was $5.2M"]
contexts = [
    [["Alice Smith was appointed CEO in 2020"]],
    [["Our Q4 2024 revenue reached $5.2 million"]]
]
ground_truths = ["Alice Smith is the CEO", "Q4 2024 revenue was $5.2M"]

scores = evaluate_rag_system(test_questions, answers, contexts, ground_truths)
print(scores)
# {
#   'faithfulness': 0.95,
#   'answer_relevancy': 0.92,
#   'context_recall': 0.88,
#   'context_precision': 0.90
# }
\end{lstlisting}

\subsection{Custom Evaluation Metrics}

\begin{lstlisting}[style=python]
class RAGEvaluator:
    def __init__(self, llm):
        self.llm = llm

    def evaluate_answer_quality(self, question, answer, retrieved_docs, ground_truth=None):
        """
        Comprehensive answer evaluation
        """
        metrics = {
            "relevance": self.score_relevance(question, answer),
            "groundedness": self.score_groundedness(answer, retrieved_docs),
            "completeness": self.score_completeness(question, answer),
            "citation_quality": self.score_citations(answer, retrieved_docs),
        }

        if ground_truth:
            metrics["accuracy"] = self.score_accuracy(answer, ground_truth)

        metrics["overall"] = sum(metrics.values()) / len(metrics)

        return metrics

    def score_relevance(self, question, answer):
        """Does the answer address the question?"""
        prompt = f"""
        Rate how well this answer addresses the question (0.0 to 1.0):

        Question: {question}
        Answer: {answer}

        Score (just the number):
        """

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        try:
            return float(response.choices[0].message.content.strip())
        except:
            return 0.5

    def score_groundedness(self, answer, retrieved_docs):
        """Is the answer supported by retrieved documents?"""
        docs_text = "\n".join([d['content'] for d in retrieved_docs])

        prompt = f"""
        Rate how well this answer is grounded in the provided documents (0.0 to 1.0):

        Documents:
        {docs_text}

        Answer: {answer}

        Score (just the number):
        """

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        try:
            return float(response.choices[0].message.content.strip())
        except:
            return 0.5
\end{lstlisting}

\clearpage

\section{6.4 Deployment Considerations}

\subsection{FastAPI Application}

\begin{lstlisting}[style=python]
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="RAG+KG API")

# Request/Response models
class QueryRequest(BaseModel):
    question: str
    top_k: int = 5
    use_kg: bool = True

class QueryResponse(BaseModel):
    answer: str
    sources: list
    confidence: float
    reasoning: str = None

# Global system (initialize once)
rag_kg_system = HybridRAGKGSystem()

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    """
    Query the RAG+KG system
    """
    try:
        result = rag_kg_system.query(
            request.question,
            top_k=request.top_k,
            use_kg=request.use_kg
        )

        return QueryResponse(
            answer=result['answer'],
            sources=result['sources'],
            confidence=result.get('confidence', 0.0),
            reasoning=result.get('reasoning')
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ingest")
async def ingest_document(file_path: str):
    """
    Ingest a new document
    """
    try:
        processor = DocumentProcessor()
        chunks = processor.process_document(file_path)

        # Add to vector DB
        for chunk in chunks:
            vector_db.add(chunk)

        # Extract entities and relationships for KG
        extractor = KnowledgeExtractor(client)
        triples = extractor.extract_triples(chunks[0]['content'])

        # Add to KG
        kg_builder = KnowledgeGraphBuilder(graph_db)
        for triple in triples:
            kg_builder.add_triple(*triple)

        return {"status": "success", "chunks_added": len(chunks)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Health check
@app.get("/health")
async def health_check():
    return {"status": "healthy"}
\end{lstlisting}

\subsection{Docker Deployment}

\begin{lstlisting}[language=Docker]
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download spaCy model
RUN python -m spacy download en_core_web_sm

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
\end{lstlisting}

\begin{lstlisting}[language=YAML]
# docker-compose.yml
version: '3.8'

services:
  neo4j:
    image: neo4j:latest
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/password
    volumes:
      - neo4j_data:/data

  rag-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
    depends_on:
      - neo4j

volumes:
  neo4j_data:
\end{lstlisting}

\clearpage

\section{6.5 Scaling Strategies}

\begin{aside}
Your prototype handles 10 queries per minute fine. Then someone puts it in production Slack and 1000 employees start using it simultaneously. Now you're paying \$500/day in OpenAI API costs and queries take 15 seconds. Caching and batching aren't optimizations - they're requirements for anything beyond a demo.
\end{aside}

\subsection{Caching Layer}

\begin{lstlisting}[style=python]
from functools import lru_cache
import hashlib
import redis

class CacheLayer:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)

    def cache_query(self, query, result, ttl=3600):
        """Cache query results"""
        key = self.get_cache_key(query)
        self.redis_client.setex(
            key,
            ttl,
            json.dumps(result)
        )

    def get_cached_result(self, query):
        """Get cached result"""
        key = self.get_cache_key(query)
        result = self.redis_client.get(key)
        if result:
            return json.loads(result)
        return None

    def get_cache_key(self, query):
        """Generate cache key"""
        return f"query:{hashlib.md5(query.encode()).hexdigest()}"

# Usage
cache = CacheLayer()

def query_with_cache(question):
    # Check cache
    cached = cache.get_cached_result(question)
    if cached:
        return cached

    # Query system
    result = rag_kg_system.query(question)

    # Cache result
    cache.cache_query(question, result)

    return result
\end{lstlisting}

\subsection{Batch Processing}

\begin{lstlisting}[style=python]
async def process_documents_batch(file_paths, batch_size=10):
    """
    Process multiple documents in batches
    """
    import asyncio

    processor = DocumentProcessor()
    results = []

    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i+batch_size]

        # Process batch in parallel
        tasks = [
            asyncio.to_thread(processor.process_document, fp)
            for fp in batch
        ]

        batch_results = await asyncio.gather(*tasks)
        results.extend(batch_results)

    return results
\end{lstlisting}

\subsection{Load Balancing Multiple Vector DBs}

\begin{lstlisting}[style=python]
class ShardedVectorDB:
    def __init__(self, num_shards=3):
        self.shards = [
            ChromaClient(f"shard_{i}")
            for i in range(num_shards)
        ]

    def get_shard(self, document_id):
        """Route document to shard based on ID"""
        shard_idx = hash(document_id) % len(self.shards)
        return self.shards[shard_idx]

    def add(self, document_id, embedding, metadata):
        """Add to appropriate shard"""
        shard = self.get_shard(document_id)
        shard.add(document_id, embedding, metadata)

    def query(self, query_embedding, top_k=5):
        """Query all shards and merge results"""
        all_results = []

        for shard in self.shards:
            results = shard.query(query_embedding, top_k=top_k)
            all_results.extend(results)

        # Re-rank and return top-k
        all_results.sort(key=lambda x: x['score'], reverse=True)
        return all_results[:top_k]
\end{lstlisting}

\clearpage

\section{6.6 Cost Optimization}

\begin{aside}
Cost optimization sounds boring until you get your first \$10,000 API bill. Embeddings are cheap per call but expensive at scale. LLM calls are expensive per call. Every retrieval spawns both. The optimizations below aren't premature - they're the difference between a sustainable product and bankruptcy. Implement them before you launch, not after.
\end{aside}

\subsection{Embedding Cost Optimization}

\begin{lstlisting}[style=python]
class EmbeddingOptimizer:
    def __init__(self):
        self.embedding_cache = {}

    def get_embedding_with_cache(self, text):
        """Cache embeddings to avoid redundant API calls"""
        text_hash = hashlib.md5(text.encode()).hexdigest()

        if text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]

        # Generate embedding
        embedding = get_embedding(text)

        # Cache
        self.embedding_cache[text_hash] = embedding

        return embedding

    def batch_embed(self, texts, batch_size=100):
        """Batch embeddings for cost efficiency"""
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            response = client.embeddings.create(
                model="text-embedding-3-small",  # Cheaper model
                input=batch
            )

            embeddings.extend([item.embedding for item in response.data])

        return embeddings
\end{lstlisting}

\subsection{LLM Cost Optimization}

\begin{lstlisting}[style=python]
def optimize_llm_usage(question, contexts):
    """
    Reduce LLM costs by:
    1. Using smaller models when possible
    2. Reducing context size
    3. Caching common queries
    """
    # Use cheaper model for simple queries
    if is_simple_query(question):
        model = "gpt-3.5-turbo"
    else:
        model = "gpt-4"

    # Compress contexts
    compressed_context = compress_context(contexts, max_tokens=2000)

    # Generate answer
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"Context: {compressed_context}\n\nQuestion: {question}"}
        ],
        max_tokens=300  # Limit output tokens
    )

    return response.choices[0].message.content

def compress_context(contexts, max_tokens=2000):
    """
    Compress contexts to fit token limit
    """
    # Estimate tokens (rough: 1 token $\approx$ 4 chars)
    total_text = "\n".join(contexts)
    estimated_tokens = len(total_text) / 4

    if estimated_tokens <= max_tokens:
        return total_text

    # Truncate
    char_limit = max_tokens * 4
    return total_text[:char_limit] + "..."
\end{lstlisting}

\subsection{Monitoring Costs}

\begin{lstlisting}[style=python]
class CostTracker:
    def __init__(self):
        self.costs = {
            "embeddings": 0.0,
            "llm_calls": 0.0,
            "total": 0.0
        }

        # Pricing (as of 2025)
        self.pricing = {
            "text-embedding-3-small": 0.00002 / 1000,  # per token
            "gpt-3.5-turbo": 0.0015 / 1000,  # per token
            "gpt-4": 0.03 / 1000,  # per token
        }

    def track_embedding_cost(self, num_tokens, model="text-embedding-3-small"):
        cost = num_tokens * self.pricing[model]
        self.costs["embeddings"] += cost
        self.costs["total"] += cost

    def track_llm_cost(self, input_tokens, output_tokens, model="gpt-4"):
        cost = (input_tokens + output_tokens) * self.pricing[model]
        self.costs["llm_calls"] += cost
        self.costs["total"] += cost

    def get_report(self):
        return {
            "embeddings": f"${self.costs['embeddings']:.4f}",
            "llm_calls": f"${self.costs['llm_calls']:.4f}",
            "total": f"${self.costs['total']:.4f}"
        }
\end{lstlisting}

\clearpage

% ============================================================================
% SECTION 7: 10 HANDS-ON PROJECTS
% ============================================================================

\part{10 HANDS-ON PROJECTS}

\begin{aside}
These projects are where learning happens. Reading about RAG is easy. Building a system that actually works is hard. Each project will break in ways you didn't expect - PDFs with weird encoding, queries that return garbage, graphs that are too slow to query. That's the point. Fix the breakage and you'll understand the material. Skip the projects and you'll forget everything in a week.
\end{aside}

Each project builds your skills progressively, from simple RAG to complex hybrid systems.

\section{Project 1: Simple PDF RAG Chatbot}

\textbf{Goal}: Build a basic RAG system that answers questions about PDF documents.

\textbf{Skills Required}:
\begin{itemize}
    \item PDF text extraction
    \item Chunking
    \item Embeddings
    \item Vector search
    \item Basic prompting
\end{itemize}

\textbf{Architecture}:
\begin{verbatim}
PDF $\rightarrow$ Extract Text $\rightarrow$ Chunk $\rightarrow$ Embed $\rightarrow$ Store in ChromaDB
User Query $\rightarrow$ Embed $\rightarrow$ Retrieve Chunks $\rightarrow$ LLM $\rightarrow$ Answer
\end{verbatim}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Extract text from 3-5 PDF documents (use PyPDF2)
    \item Chunk text into 500-token segments with 50-token overlap
    \item Generate embeddings using \texttt{text-embedding-3-small}
    \item Store in ChromaDB with metadata (file\_name, page\_number)
    \item Implement query function: embed query $\rightarrow$ retrieve top 5 chunks $\rightarrow$ generate answer
    \item Add citation: include source file and page number
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Correctly extracts text from PDFs
    \item  Answers questions with relevant context
    \item  Includes citations (file + page)
    \item  Handles "I don't know" when answer not in docs
\end{itemize}

\textbf{Dataset}: Use 3-5 research papers from arXiv or your domain

\clearpage

\section{Project 2: Multi-Hop RAG System}

\textbf{Goal}: Handle complex questions requiring multiple retrieval steps.

\textbf{Skills Required}:
\begin{itemize}
    \item Query decomposition
    \item Multi-step retrieval
    \item Context aggregation
\end{itemize}

\textbf{Architecture}:
\begin{verbatim}
Complex Query $\rightarrow$ Decompose into Sub-Queries $\rightarrow$ Retrieve for Each $\rightarrow$ Aggregate $\rightarrow$ Answer
\end{verbatim}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Implement query decomposition using GPT-4
    \item For each sub-query, retrieve relevant chunks
    \item Aggregate all retrieved contexts
    \item Generate final answer synthesizing all sub-answers
    \item Test on multi-hop questions like:
    \begin{itemize}
        \item "Compare the methodologies of paper A and paper B"
        \item "What are the advantages and disadvantages of approach X?"
    \end{itemize}
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Successfully decomposes complex queries
    \item  Retrieves relevant context for each sub-query
    \item  Synthesizes coherent final answer
    \item  Handles at least 3-hop reasoning
\end{itemize}

\section{Project 3: Automatic KG Builder from Text}

\textbf{Goal}: Extract entities and relationships from text and build a knowledge graph.

\textbf{Skills Required}:
\begin{itemize}
    \item Named entity recognition
    \item Relationship extraction
    \item Triple generation
    \item Neo4j graph construction
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Use spaCy for entity extraction (Person, Org, Location, etc.)
    \item Implement relationship extraction using LLM
    \item Generate triples (Subject, Predicate, Object)
    \item Deduplicate entities (entity linking)
    \item Load triples into Neo4j
    \item Visualize graph in Neo4j Browser
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Extracts at least 50+ entities
    \item  Identifies at least 30+ relationships
    \item  Graph is queryable in Neo4j
    \item  Entity deduplication works (e.g., "Alice" = "Alice Smith")
\end{itemize}

\textbf{Dataset}: News articles, Wikipedia pages, or company documents

\clearpage

\section{Project 4: Entity/Relationship Extractor}

\textbf{Goal}: Build a production-grade entity and relationship extraction pipeline.

\textbf{Skills Required}:
\begin{itemize}
    \item Advanced NLP
    \item LLM-based extraction
    \item Schema validation
    \item Batch processing
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Define schema (entity types, relationship types)
    \item Implement entity extraction with confidence scores
    \item Implement relationship extraction between identified entities
    \item Add validation layer (ensure relationships make sense)
    \item Output structured JSON with entities + relationships
    \item Handle batch processing for multiple documents
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Precision > 80\% on entity extraction
    \item  Recall > 70\% on relationship extraction
    \item  Handles at least 100 documents
    \item  Output is valid against schema
\end{itemize}

\section{Project 5: Cypher Query Generator Using LLMs}

\textbf{Goal}: Convert natural language to Cypher queries.

\textbf{Skills Required}:
\begin{itemize}
    \item Text-to-Cypher prompting
    \item Query validation
    \item Error handling
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Create graph schema description (nodes, relationships, properties)
    \item Implement text-to-Cypher using GPT-4 with few-shot examples
    \item Add query validation (syntax check, no destructive operations)
    \item Implement self-correction (if query fails, retry with error message)
    \item Execute query on Neo4j and return results
    \item Format results in human-readable way
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Generates syntactically valid Cypher 90\% of the time
    \item  Correctly answers factual queries ("Who works at Company X?")
    \item  Handles relationship queries ("Who does Alice report to?")
    \item  Self-corrects failed queries
\end{itemize}

\clearpage

\section{Project 6: KG Search Engine}

\textbf{Goal}: Build a search engine powered by knowledge graph traversal.

\textbf{Skills Required}:
\begin{itemize}
    \item Graph algorithms (PageRank, shortest path)
    \item Cypher query optimization
    \item Result ranking
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Implement entity recognition in search queries
    \item Build query expansion using graph neighborhoods
    \item Implement PageRank to rank important nodes
    \item Find shortest paths between entities
    \item Build search result page showing:
    \begin{itemize}
        \item Direct matches
        \item Related entities
        \item Connection paths
    \end{itemize}
    \item Add filters (entity type, relationship type)
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Returns relevant results for entity searches
    \item  Shows relationship paths between entities
    \item  Ranks results by importance (PageRank)
    \item  Sub-second query performance
\end{itemize}

\section{Project 7: RAG with Reranker + Query Rewrite}

\textbf{Goal}: Build an advanced RAG system with reranking and query optimization.

\textbf{Skills Required}:
\begin{itemize}
    \item Hybrid retrieval (BM25 + semantic)
    \item Cross-encoder reranking
    \item Query rewriting
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Implement query rewriting using LLM
    \item Build hybrid retriever (BM25 + dense embeddings)
    \item Retrieve top 100 candidates
    \item Rerank using cross-encoder (e.g., \texttt{ms-marco-MiniLM})
    \item Take top 5 after reranking
    \item Generate answer with GPT-4
    \item Compare performance: plain RAG vs. this system
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Query rewriting improves retrieval recall by 15\%+
    \item  Reranking improves answer quality by 20\%+
    \item  Outperforms baseline RAG on test set
    \item  Handles ambiguous queries well
\end{itemize}

\clearpage

\section{Project 8: Graph-RAG with Neighborhood Expansion}

\textbf{Goal}: Implement Microsoft GraphRAG pattern - use KG to expand retrieval context.

\textbf{Skills Required}:
\begin{itemize}
    \item Graph traversal
    \item Context fusion
    \item Hybrid reasoning
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Extract entities from user query
    \item Find entities in knowledge graph
    \item Expand to 2-hop neighborhood (get related entities)
    \item Use neighborhood entities to expand RAG query
    \item Retrieve documents mentioning these entities
    \item Fuse KG facts + RAG documents
    \item Generate answer using both sources
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Successfully expands query context using KG
    \item  Answers multi-hop questions correctly
    \item  Outperforms plain RAG on relationship queries
    \item  Provides graph-based reasoning in answer
\end{itemize}

\section{Project 9: Hybrid RAG + KG Chatbot}

\textbf{Goal}: Build a full conversational chatbot with hybrid retrieval.

\textbf{Skills Required}:
\begin{itemize}
    \item Chat memory
    \item Context management
    \item Query routing
    \item Streaming responses
\end{itemize}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item Implement conversation memory (store last 5 turns)
    \item Build query router (classify query type)
    \item Route to appropriate retrieval strategy:
    \begin{itemize}
        \item Factual $\rightarrow$ KG
        \item Analytical $\rightarrow$ RAG
        \item Relationship $\rightarrow$ Hybrid
    \end{itemize}
    \item Maintain context across turns
    \item Stream responses for better UX
    \item Add conversation reset functionality
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Maintains context across conversation
    \item  Routes queries correctly 85\%+ of the time
    \item  Handles follow-up questions ("What about his manager?")
    \item  Streams responses smoothly
\end{itemize}

\clearpage

\section{Project 10: Production-Ready Enterprise Knowledge Assistant}

\textbf{Goal}: Build a complete, deployable enterprise knowledge system.

\textbf{Skills Required}:
\begin{itemize}
    \item Full-stack development
    \item API design
    \item Deployment
    \item Monitoring
    \item Testing
\end{itemize}

\textbf{Architecture}:
\begin{verbatim}
FastAPI Backend $\rightarrow$ Docker $\rightarrow$ Neo4j + ChromaDB $\rightarrow$ Frontend (React/Streamlit)
\end{verbatim}

\textbf{Step-by-Step Tasks}:
\begin{enumerate}
    \item \textbf{Backend (FastAPI)}:
    \begin{itemize}
        \item \texttt{/query} endpoint (POST)
        \item \texttt{/ingest} endpoint (upload docs)
        \item \texttt{/health} endpoint
        \item Authentication (API keys)
    \end{itemize}

    \item \textbf{Data Processing}:
    \begin{itemize}
        \item Async document processing
        \item Progress tracking
        \item Error handling
    \end{itemize}

    \item \textbf{Deployment}:
    \begin{itemize}
        \item Dockerize application
        \item Docker Compose for all services
        \item Environment variables for config
    \end{itemize}

    \item \textbf{Monitoring}:
    \begin{itemize}
        \item Query logging
        \item Performance metrics
        \item Cost tracking
    \end{itemize}

    \item \textbf{Testing}:
    \begin{itemize}
        \item Unit tests for core functions
        \item Integration tests for API
        \item Load testing (100 concurrent queries)
    \end{itemize}

    \item \textbf{Frontend}:
    \begin{itemize}
        \item Chat interface
        \item Document upload
        \item Admin panel (view metrics)
    \end{itemize}
\end{enumerate}

\textbf{Evaluation Criteria}:
\begin{itemize}
    \item  Handles 100+ concurrent users
    \item  99\% uptime over 1 week
    \item  Sub-second p95 latency
    \item  Full test coverage (>80\%)
    \item  Deployed and accessible via URL
\end{itemize}

\textbf{Deliverables}:
\begin{itemize}
    \item GitHub repository with README
    \item Deployed application (AWS/Vercel/etc.)
    \item API documentation
    \item Demo video (5 min)
\end{itemize}

\clearpage

% ============================================================================
% SECTION 8: CAPSTONE PROJECT
% ============================================================================

\part{CAPSTONE PROJECT}

\section{Enterprise "Company Brain" - The Ultimate RAG + KG System}

\subsection{Project Overview}

Build a complete enterprise knowledge management system that ingests company documents, builds a knowledge graph, and answers questions using hybrid RAG + KG retrieval.

\begin{aside}
This is the final boss. This project integrates everything you've learned - chunking, embeddings, graph construction, hybrid retrieval, deployment, monitoring, cost optimization. It will take weeks, not days. It will break in frustrating ways. You will question your life choices. When it finally works, you'll have a portfolio piece that actually demonstrates competence, not just "followed a tutorial." That's worth the pain.
\end{aside}

\subsection{System Requirements}

\textbf{Input Sources}:
\begin{itemize}
    \item PDF documents (reports, papers, manuals)
    \item Markdown files (wikis, docs)
    \item CSV data (employee directory, project list)
    \item Web pages (company blog, documentation)
\end{itemize}

\textbf{Capabilities}:
\begin{enumerate}
    \item \textbf{Document Ingestion}: Async pipeline processing all formats
    \item \textbf{Knowledge Graph}: Auto-build from all documents
    \item \textbf{Hybrid Search}: Combine structured + unstructured retrieval
    \item \textbf{Query Interface}: Natural language queries with explanations
    \item \textbf{Admin Dashboard}: Monitor usage, costs, data sources
\end{enumerate}

\subsection{Technical Specifications}

\textbf{Tech Stack}:
\begin{itemize}
    \item Backend: Python 3.11, FastAPI
    \item Vector DB: Pinecone or ChromaDB
    \item Graph DB: Neo4j
    \item LLM: GPT-4 (primary), GPT-3.5-turbo (fallback)
    \item Frontend: React or Streamlit
    \item Deployment: Docker + Docker Compose
    \item Monitoring: Prometheus + Grafana (bonus)
\end{itemize}

\textbf{Core Features} (Must-Have):
\begin{enumerate}
    \item Document upload (drag-and-drop)
    \item Automatic KG construction
    \item Natural language queries
    \item Cited answers with source links
    \item Reasoning explanation ("how I found this")
    \item Query routing (auto-select KG vs RAG vs Hybrid)
    \item Admin dashboard (stats, costs)
\end{enumerate}

\textbf{Advanced Features} (Nice-to-Have):
\begin{enumerate}
    \setcounter{enumi}{7}
    \item Multi-user support with authentication
    \item Document versioning
    \item Query history and analytics
    \item Custom entity types
    \item Graph visualization
    \item Export answers as reports
\end{enumerate}

\clearpage

\subsection{Implementation Steps}

\subsubsection{Phase 1: Data Ingestion (Week 1)}
\begin{enumerate}
    \item Build DocumentProcessor for all file types
    \item Implement async processing queue
    \item Add metadata extraction
    \item Test with 50+ documents
\end{enumerate}

\subsubsection{Phase 2: Knowledge Graph Construction (Week 2)}
\begin{enumerate}
    \item Entity and relationship extraction
    \item Entity linking and deduplication
    \item Load into Neo4j
    \item Build basic Cypher query interface
\end{enumerate}

\subsubsection{Phase 3: RAG System (Week 2-3)}
\begin{enumerate}
    \item Chunking strategy implementation
    \item Embedding generation and storage
    \item Hybrid retriever (BM25 + semantic)
    \item Reranker integration
\end{enumerate}

\subsubsection{Phase 4: Hybrid System (Week 3-4)}
\begin{enumerate}
    \item Query classification and routing
    \item KG-augmented retrieval
    \item Context fusion
    \item Answer generation with citations
\end{enumerate}

\subsubsection{Phase 5: API \& Frontend (Week 4-5)}
\begin{enumerate}
    \item FastAPI endpoints
    \item Frontend (chat interface)
    \item Admin dashboard
    \item Authentication
\end{enumerate}

\subsubsection{Phase 6: Testing \& Deployment (Week 5-6)}
\begin{enumerate}
    \item Unit tests (>80\% coverage)
    \item Integration tests
    \item Load testing
    \item Docker deployment
    \item Documentation
\end{enumerate}

\subsection{Evaluation Benchmarks}

\textbf{Quantitative Metrics}:
\begin{itemize}
    \item \textbf{Accuracy}: 85\%+ on 100-question test set
    \item \textbf{Latency}: p95 < 2 seconds
    \item \textbf{Throughput}: 50+ concurrent users
    \item \textbf{Cost}: < \$0.10 per query
    \item \textbf{Uptime}: 99.5\%+
\end{itemize}

\textbf{Qualitative Assessment}:
\begin{itemize}
    \item Answer quality (human evaluation)
    \item Citation accuracy (source verification)
    \item Reasoning clarity (explanation quality)
    \item User experience (UI/UX review)
\end{itemize}

\clearpage

\subsection{Evaluation Rubric}

\begin{longtable}{|p{4cm}|p{2cm}|p{8cm}|}
\hline
\textbf{Component} & \textbf{Weight} & \textbf{Criteria} \\
\hline
\textbf{Data Ingestion} & 15\% & Handles all file types, metadata extraction, async processing \\
\hline
\textbf{Knowledge Graph} & 20\% & Entity/relation extraction quality, graph completeness, Cypher queries work \\
\hline
\textbf{RAG System} & 20\% & Retrieval quality, chunking strategy, embedding optimization \\
\hline
\textbf{Hybrid Integration} & 25\% & Query routing, context fusion, answer quality \\
\hline
\textbf{Production Quality} & 20\% & API design, testing, deployment, documentation, monitoring \\
\hline
\textbf{Total} & 100\% & \\
\hline
\end{longtable}

\textbf{Grading Scale}:
\begin{itemize}
    \item \textbf{90-100}: Exceptional - Production-ready, innovative features
    \item \textbf{80-89}: Excellent - All core features working well
    \item \textbf{70-79}: Good - Core features present, some rough edges
    \item \textbf{60-69}: Adequate - Basic functionality works
    \item \textbf{<60}: Needs improvement
\end{itemize}

\subsection{What Your Portfolio Demo Should Show}

\textbf{5-Minute Video Covering}:
\begin{enumerate}
    \item \textbf{Intro} (30s): Problem statement and solution overview
    \item \textbf{Data Ingestion} (60s): Upload docs, show processing pipeline
    \item \textbf{Knowledge Graph} (60s): Visualize graph, run Cypher query
    \item \textbf{Query Demo} (90s):
    \begin{itemize}
        \item Factual query (KG-routed)
        \item Analytical query (RAG-routed)
        \item Multi-hop query (Hybrid)
    \end{itemize}
    \item \textbf{Advanced Features} (60s): Citations, reasoning, admin dashboard
    \item \textbf{Technical Deep-Dive} (30s): Architecture diagram, tech stack
\end{enumerate}

\subsection{How This Signals Hire-Readiness}

\textbf{What Employers See}:
\begin{itemize}
    \item  \textbf{Full-stack skills}: Backend + Frontend + DevOps
    \item  \textbf{AI/ML expertise}: LLMs, embeddings, vector DBs
    \item  \textbf{Data engineering}: Pipelines, async processing
    \item  \textbf{Production thinking}: Testing, monitoring, deployment
    \item  \textbf{Problem-solving}: Complex system design
    \item  \textbf{Communication}: Clear documentation and demo
\end{itemize}

\textbf{Conversation Starters in Interviews}:
\begin{itemize}
    \item "Tell me about your approach to query routing"
    \item "How did you optimize for cost and latency?"
    \item "What were the biggest challenges in building this?"
    \item "How would you scale this to 10M documents?"
\end{itemize}

\clearpage

% ============================================================================
% SECTION 9: ASSESSMENTS & QUIZZES
% ============================================================================

\part{ASSESSMENTS \& QUIZZES}

\begin{aside}
Quizzes test retention, not understanding. If you can pass these without looking up answers, great - you remember the material. If not, that's fine too. What matters is whether you can build working systems, not whether you memorized which embedding model is cheaper. Use these to identify gaps, then go back and reread those sections.
\end{aside}

\section{Module 1 Quiz: Foundations}

\begin{enumerate}
    \item \textbf{What is the main advantage of embeddings over keyword search?}
    \begin{itemize}
        \item a) Faster processing
        \item b) Captures semantic similarity
        \item c) Requires less storage
        \item d) No API needed
    \end{itemize}

    \item \textbf{In a knowledge graph, what represents the relationship between two entities?}
    \begin{itemize}
        \item a) Node
        \item b) Edge
        \item c) Property
        \item d) Label
    \end{itemize}

    \item \textbf{Which embedding model is most cost-effective for general use?}
    \begin{itemize}
        \item a) text-embedding-3-large
        \item b) text-embedding-ada-002
        \item c) text-embedding-3-small
        \item d) GPT-4
    \end{itemize}
\end{enumerate}

\textbf{Answer Key}: 1-b, 2-b, 3-c

\section{Module 2 Quiz: RAG Engineering}

\begin{enumerate}
    \item \textbf{Why is chunk overlap important?}
    \begin{itemize}
        \item a) Increases total chunks
        \item b) Prevents context loss at boundaries
        \item c) Improves embedding quality
        \item d) Reduces API costs
    \end{itemize}

    \item \textbf{What does a reranker do?}
    \begin{itemize}
        \item a) Re-sorts initial retrieval results for better precision
        \item b) Generates new embeddings
        \item c) Rewrites user queries
        \item d) Ranks LLM responses
    \end{itemize}

    \item \textbf{Which retrieval method combines exact matching with semantic search?}
    \begin{itemize}
        \item a) BM25 only
        \item b) Dense retrieval
        \item c) Hybrid retrieval
        \item d) Keyword search
    \end{itemize}
\end{enumerate}

\textbf{Answer Key}: 1-b, 2-a, 3-c

\clearpage

\section{Module 3 Quiz: Knowledge Graphs}

\begin{enumerate}
    \item \textbf{What Cypher query finds all people working at "Acme"?}
    \begin{itemize}
        \item a) \texttt{FIND (p)->(c) WHERE c.name = "Acme"}
        \item b) \texttt{MATCH (p:Person)-[:WORKS\_FOR]->(c:Company \{name: "Acme"\}) RETURN p}
        \item c) \texttt{SELECT * FROM Person WHERE company = "Acme"}
        \item d) \texttt{GET Person WITH Company = "Acme"}
    \end{itemize}

    \item \textbf{What is entity linking?}
    \begin{itemize}
        \item a) Creating relationships between entities
        \item b) Resolving different mentions to the same entity
        \item c) Extracting entities from text
        \item d) Storing entities in a database
    \end{itemize}

    \item \textbf{What does a 2-hop neighborhood query return?}
    \begin{itemize}
        \item a) Entities exactly 2 steps away
        \item b) Entities within 1-2 steps
        \item c) 2 random neighbors
        \item d) Second-degree connections only
    \end{itemize}
\end{enumerate}

\textbf{Answer Key}: 1-b, 2-b, 3-b

\section{Module 4 Quiz: Hybrid Systems}

\begin{enumerate}
    \item \textbf{When should you route a query to the knowledge graph?}
    \begin{itemize}
        \item a) Always
        \item b) For analytical questions requiring summarization
        \item c) For factual and relationship queries
        \item d) Never, always use RAG
    \end{itemize}

    \item \textbf{What is context fusion?}
    \begin{itemize}
        \item a) Merging multiple documents
        \item b) Combining KG facts with RAG documents into unified context
        \item c) Fusing query and answer
        \item d) Merging embeddings
    \end{itemize}

    \item \textbf{What advantage does hybrid RAG+KG have over plain RAG?}
    \begin{itemize}
        \item a) Faster query speed
        \item b) Multi-hop reasoning and structured relationships
        \item c) Lower cost
        \item d) Easier implementation
    \end{itemize}
\end{enumerate}

\textbf{Answer Key}: 1-c, 2-b, 3-b

\clearpage

\section{Coding Assignments}

\subsection{Assignment 1: Build a Basic RAG System}

\textbf{Task}: Implement a RAG system that answers questions about 3 PDF documents.

\textbf{Requirements}:
\begin{itemize}
    \item Extract and chunk PDFs
    \item Store in vector DB
    \item Implement query function
    \item Add citations
\end{itemize}

\textbf{Submission}: GitHub repo with code + README

\subsection{Assignment 2: Extract Knowledge Graph from News Articles}

\textbf{Task}: Extract entities and relationships from 20 news articles and build a Neo4j graph.

\textbf{Requirements}:
\begin{itemize}
    \item Use LLM for extraction
    \item Entity deduplication
    \item Load into Neo4j
    \item Run 5 example Cypher queries
\end{itemize}

\textbf{Submission}: Code + Graph visualization screenshot + Query examples

\subsection{Assignment 3: Build a Hybrid Query System}

\textbf{Task}: Implement query routing that decides between KG, RAG, or hybrid.

\textbf{Requirements}:
\begin{itemize}
    \item Query classifier (LLM-based)
    \item Three retrieval strategies
    \item Test on 20 varied questions
    \item Compare performance
\end{itemize}

\textbf{Submission}: Code + evaluation results (accuracy, latency)

\clearpage

\section{Final Interview-Style Questions}

\subsection{Technical Deep-Dive Questions}

\begin{enumerate}
    \item \textbf{"Explain your approach to chunking. Why did you choose that strategy?"}

    Expected: Discussion of document structure, semantic boundaries, trade-offs

    \item \textbf{"How would you handle entity disambiguation in a KG?"}

    Expected: Entity linking, alias management, confidence scores

    \item \textbf{"Walk me through your hybrid retrieval pipeline."}

    Expected: Query understanding $\rightarrow$ routing $\rightarrow$ KG/RAG $\rightarrow$ fusion $\rightarrow$ generation

    \item \textbf{"How do you ensure answers are grounded and not hallucinated?"}

    Expected: Citations, hallucination detection, "I don't know" pattern

    \item \textbf{"What metrics do you use to evaluate RAG system quality?"}

    Expected: Faithfulness, relevancy, precision/recall, latency, cost
\end{enumerate}

\subsection{System Design Questions}

\begin{enumerate}
    \item \textbf{"Design a RAG system for a company with 1M documents."}

    Expected: Sharding, caching, batch processing, cost optimization

    \item \textbf{"How would you scale a knowledge graph to billions of nodes?"}

    Expected: Graph partitioning, distributed systems, query optimization

    \item \textbf{"Design a monitoring system for a production RAG application."}

    Expected: Logging, metrics (latency, accuracy), cost tracking, alerts
\end{enumerate}

\subsection{Scenario-Based Questions}

\begin{enumerate}
    \item \textbf{"A user complains that the system keeps giving wrong answers. How do you debug?"}

    Expected: Check retrieval quality, inspect prompts, evaluate chunks, test queries

    \item \textbf{"Costs are too high. How do you optimize?"}

    Expected: Smaller models, caching, batch processing, context compression

    \item \textbf{"The system is slow. How do you improve latency?"}

    Expected: Caching, async processing, smaller models, index optimization
\end{enumerate}

\subsection{Decision Questions}

These questions test your ability to make architectural tradeoffs. There's no single correct answer - what matters is your reasoning about constraints and priorities.

\begin{enumerate}
    \item \textbf{You have a RAG system that performs well on factual queries but fails on multi-hop reasoning.}

    You can either:
    \begin{itemize}
        \item Increase chunk overlap
        \item Add reranking
        \item Introduce a knowledge graph
    \end{itemize}

    Which do you try first, and why?

    \textit{Consider}: Implementation complexity, existing infrastructure, data characteristics, performance requirements.

    \item \textbf{You need to choose a chunk size for a legal document corpus.}

    Option A: 512 tokens (faster retrieval, more granular)

    Option B: 2048 tokens (slower retrieval, more context)

    What factors determine your choice? What's the real tradeoff here?

    \textit{Consider}: Document structure, query types, LLM context limits, retrieval precision vs recall.

    \item \textbf{Your system currently uses text-embedding-ada-002. A new model offers 15\% better accuracy but 3x higher latency.}

    Do you switch? What questions do you ask before deciding?

    \textit{Consider}: User experience requirements, cost implications, accuracy vs speed tradeoff, production SLAs.

    \item \textbf{Your retrieval returns mediocre results. You can either:}

    A) Retrieve top-20 chunks instead of top-5

    B) Keep top-5 but add a reranking step

    Which approach is better, and under what conditions would you choose the opposite?

    \textit{Consider}: LLM context costs, reranking latency, quality vs cost, false positive handling.

    \item \textbf{You're building a RAG system for medical records (highly sensitive data).}

    Option A: Use GPT-4 with careful prompt engineering and auditing

    Option B: Use a local Llama model with lower quality but complete data privacy

    How do you make this decision? What if accuracy matters for patient safety?

    \textit{Consider}: Regulatory requirements, liability, performance requirements, deployment complexity.

    \item \textbf{Your knowledge graph needs a database. Neo4j is mature but expensive at scale. A custom solution would be cheaper but requires engineering time.}

    What factors drive this decision? When is "build vs buy" the wrong framing?

    \textit{Consider}: Team expertise, time to market, long-term maintenance, scale requirements, vendor lock-in.

    \item \textbf{Users complain about stale data. You can either:}

    A) Disable caching entirely (fresh data, high costs)

    B) Implement smart cache invalidation (complex, might miss edge cases)

    C) Accept staleness with a clear TTL policy (simple, documented tradeoff)

    Which do you choose and why? What questions determine this?

    \textit{Consider}: Data change frequency, user expectations, cost constraints, system complexity.
\end{enumerate}

\subsection{Failure Diagnosis Questions}

These test your ability to debug production systems. For each scenario, identify distinct failure modes and how you'd test for them.

\begin{enumerate}
    \item \textbf{Your system returns fluent but factually incorrect answers with high confidence.}

    List three distinct failure modes that could cause this and how you would test for each.

    \textit{Possible causes to consider}: Retrieval failures, prompt issues, hallucination, context contamination, outdated knowledge.

    \item \textbf{After scaling from 10k to 1M documents, query performance degraded from 200ms to 8 seconds.}

    Diagnose three different bottlenecks that could cause this. How would you isolate which one is the culprit?

    \textit{Possible causes to consider}: Index quality, memory issues, network latency, algorithmic complexity, database saturation.

    \item \textbf{Retrieval quality is inconsistent: excellent for some queries, terrible for others.}

    What are three different root causes? How would you systematically identify which applies?

    \textit{Possible causes to consider}: Query type mismatch, embedding model limitations, chunking boundary issues, sparse vs dense query characteristics.

    \item \textbf{Your knowledge graph traversal sometimes returns 100,000+ nodes and times out.}

    Identify three distinct failure modes. How would you prevent this without losing legitimate broad queries?

    \textit{Possible causes to consider}: Missing query limits, relationship explosion, cycle detection, query optimization, schema design.

    \item \textbf{Your system costs have tripled but answer quality hasn't improved.}

    List three different resource waste patterns and how you'd detect each.

    \textit{Possible causes to consider}: Over-retrieval, redundant API calls, inefficient caching, prompt bloat, unnecessary reranking.
\end{enumerate}

\subsection{"Bad Idea" Questions (Anti-Pattern Recognition)}

Mark each statement as True or False, then explain why. The explanation is what matters.

\begin{enumerate}
    \item \textbf{True or False}: "Increasing the embedding dimension from 768 to 1536 always improves retrieval quality."

    \textit{Why it matters}: Understand curse of dimensionality, overfitting, diminishing returns, computational cost.

    \item \textbf{True or False}: "Retrieving more chunks (top-20 instead of top-5) always leads to better answers."

    \textit{Why it matters}: Context pollution, cost implications, lost-in-the-middle problem, LLM attention limits.

    \item \textbf{True or False}: "A larger LLM context window (128k tokens) eliminates the need for good retrieval."

    \textit{Why it matters}: Cost scaling, attention degradation, retrieval as filtering, context vs reasoning.

    \item \textbf{True or False}: "Your knowledge graph should store every possible relationship you can extract."

    \textit{Why it matters}: Signal vs noise, query performance, maintenance burden, schema design principles.

    \item \textbf{True or False}: "Fine-tuning your embedding model on domain data is always worth the effort."

    \textit{Why it matters}: Cost-benefit analysis, data requirements, maintenance overhead, diminishing returns.

    \item \textbf{True or False}: "Caching should be disabled in production to ensure users always get the freshest data."

    \textit{Why it matters}: Cost optimization, latency requirements, staleness tolerance, cache invalidation strategies.

    \item \textbf{True or False}: "Knowledge graphs are always faster than vector similarity search for multi-hop queries."

    \textit{Why it matters}: Query complexity, graph size, indexing strategies, hybrid approaches.

    \item \textbf{True or False}: "In a hybrid RAG+KG system, you should always weight both retrieval methods equally in fusion."

    \textit{Why it matters}: Query-dependent routing, strengths/weaknesses of each approach, adaptive systems.
\end{enumerate}

\clearpage

% ============================================================================
% SECTION 10: COURSE RESOURCES & NEXT STEPS
% ============================================================================

\part{COURSE RESOURCES \& NEXT STEPS}

\begin{aside}
You've reached the end of the technical content. If you've built the projects, you know more about production RAG + KG systems than most engineers claiming to be "AI experts." The resources below help you go deeper. The career section shows you what's possible. But remember: the market doesn't care about courses completed - it cares about systems shipped. Build things, put them on GitHub, write about what you learned. That's how you get hired.
\end{aside}

\section{Recommended Reading}

\textbf{Books}:
\begin{itemize}
    \item "Speech and Language Processing" - Jurafsky \& Martin
    \item "Designing Data-Intensive Applications" - Martin Kleppmann
    \item "Graph Databases" - Ian Robinson, Jim Webber
    \item "Building LLM-Powered Applications" - Valentina Alto
\end{itemize}

\textbf{Papers}:
\begin{itemize}
    \item "Attention Is All You Need" (Transformers)
    \item "BERT: Pre-training of Deep Bidirectional Transformers"
    \item "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"
    \item "GraphRAG: Unlocking LLM discovery on narrative private data" (Microsoft)
\end{itemize}

\textbf{Online Resources}:
\begin{itemize}
    \item Neo4j Graph Academy (free courses)
    \item Pinecone Learning Center
    \item LangChain Documentation
    \item OpenAI Cookbook
\end{itemize}

\section{Community \& Support}

\begin{itemize}
    \item \textbf{Discord}: Join RAG/LLM engineering communities
    \item \textbf{Twitter}: Follow \#RAG, \#KnowledgeGraphs, \#LLMs
    \item \textbf{GitHub}: Explore open-source RAG projects
    \item \textbf{LinkedIn}: Connect with RAG engineers
\end{itemize}

\section{Career Pathways}

With RAG + KG skills, you're qualified for:

\begin{itemize}
    \item \textbf{RAG Engineer}: \$120k-\$180k
    \item \textbf{Knowledge Graph Engineer}: \$130k-\$190k
    \item \textbf{AI/ML Engineer (LLM focus)}: \$140k-\$220k
    \item \textbf{Senior/Staff positions}: \$200k-\$300k+
\end{itemize}

\textbf{Companies Hiring}:
\begin{itemize}
    \item Big Tech: Google, Microsoft, Meta, Amazon
    \item AI Startups: Anthropic, OpenAI, Cohere, Scale AI
    \item Enterprise: McKinsey, Deloitte, Accenture
    \item Specialized: Databricks, Snowflake, Elastic
\end{itemize}

\section{Certification Path (Self-Guided)}

\begin{enumerate}
    \item Complete all 10 projects
    \item Pass all quizzes (80\%+ score)
    \item Build and deploy capstone project
    \item Create portfolio (GitHub + demo video)
    \item Write 3 blog posts explaining concepts
    \item Contribute to open-source RAG project
\end{enumerate}

\section{What Comes After This Course?}

\textbf{Advanced Topics to Explore}:
\begin{itemize}
    \item Fine-tuning LLMs for domain-specific RAG
    \item Multi-modal RAG (images, audio, video)
    \item Reinforcement learning for retrieval optimization
    \item Distributed graph databases
    \item Real-time streaming RAG systems
    \item Privacy-preserving RAG (local LLMs)
\end{itemize}

\textbf{Keep Learning}:
\begin{itemize}
    \item Stay updated with latest LLM releases
    \item Experiment with new vector databases
    \item Try different embedding models
    \item Benchmark your systems
    \item Share your learnings
\end{itemize}

\clearpage

% ============================================================================
% SECTION 11: APPENDIX & QUICK REFERENCE
% ============================================================================

\part{APPENDIX: QUICK REFERENCE}

\begin{aside}
This appendix has the code snippets you'll actually copy-paste when building systems. Bookmark this page. You'll be back here constantly until this stuff becomes muscle memory.
\end{aside}

\section{Common Code Patterns}

\subsection{1. Basic RAG Query}

\begin{lstlisting}[style=python]
def rag_query(question, vector_db, llm):
    # Retrieve
    docs = vector_db.query(get_embedding(question), top_k=5)

    # Generate
    context = "\n".join([d['content'] for d in docs])
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    answer = llm.generate(prompt)

    return answer
\end{lstlisting}

\subsection{2. KG Entity Lookup}

\begin{lstlisting}[style=cypher]
MATCH (e:Entity {name: $entity_name})
RETURN e, properties(e)
\end{lstlisting}

\subsection{3. Hybrid Retrieval}

\begin{lstlisting}[style=python]
# Get from both sources
kg_facts = graph_db.query(cypher_query)
rag_docs = vector_db.query(embedding)

# Fuse
context = format_kg(kg_facts) + format_docs(rag_docs)
answer = llm.generate(context + question)
\end{lstlisting}

\clearpage

\section{Essential Tools Installation}

\begin{lstlisting}[style=bash]
# Python packages
pip install openai chromadb neo4j langchain sentence-transformers \
    faiss-cpu rank-bm25 spacy fastapi uvicorn pytest ragas

# Download spaCy model
python -m spacy download en_core_web_sm

# Neo4j (Docker)
docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j
\end{lstlisting}

\section{Glossary}

\begin{description}
    \item[RAG] Retrieval-Augmented Generation
    \item[KG] Knowledge Graph
    \item[Embedding] Dense vector representation of text
    \item[Chunking] Splitting documents into smaller pieces
    \item[Triple] (Subject, Predicate, Object) relationship
    \item[Cypher] Neo4j query language
    \item[BM25] Keyword-based ranking algorithm
    \item[Reranker] Model that re-scores retrieval results
    \item[Context Fusion] Combining KG and RAG contexts
\end{description}

\clearpage

\section{Conclusion}

You've reached the end. If you read through everything without building the projects, you've wasted your time - reading about systems isn't the same as building them. Go back and actually code something.

If you built the projects and struggled through the debugging, congratulations. You now have more practical knowledge about production RAG + KG systems than most people talking about "AI engineering" on LinkedIn. You can:

\begin{itemize}
    \item Build production-grade RAG systems (not demos)
    \item Design and query knowledge graphs (not toy examples)
    \item Create hybrid RAG + KG architectures (and know when not to)
    \item Deploy enterprise-ready AI applications (with proper monitoring and cost controls)
    \item Evaluate and optimize retrieval systems (with actual metrics)
\end{itemize}

\subsection{What Happens Next}

The material here doesn't expire next week. RAG and knowledge graphs will still be relevant in 5 years, even as specific models and tools change. The fundamentals don't change.

If you want to get hired, build something real and put it on GitHub. Write about what you learned and what broke. Nobody cares about course completion certificates - they care about shipped code and lessons learned.

The field evolves fast. New models, new databases, new techniques. That's not an excuse to wait - it's a reason to start now and iterate.

\textbf{Now go build something.}

\vspace{2cm}

\noindent\rule{\textwidth}{0.5pt}

\textbf{Course Version}: 1.0 (2025)

\textbf{Last Updated}: December 2025

\textbf{License}: Educational Use

\noindent\rule{\textwidth}{0.5pt}

\end{document}
